{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import copy\n",
    "import logging\n",
    "#import torch\n",
    "\n",
    "#from tqdm.autonotebook import tqdm\n",
    "\n",
    "from data.data_loader import Dataset\n",
    "from data.germeval2017 import germeval2017_dataset\n",
    "\n",
    "from misc.preferences import PREFERENCES\n",
    "#from misc.visualizer import *\n",
    "from misc.run_configuration import get_default_params, randomize_params\n",
    "from misc import utils\n",
    "\n",
    "from optimizer import get_default_optimizer\n",
    "from criterion import NllLoss, LossCombiner\n",
    "\n",
    "from models.transformer.encoder import TransformerEncoder\n",
    "from models.jointAspectTagger import JointAspectTagger\n",
    "from models.transformer.train import Trainer\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREFERENCES.defaults(\n",
    "    data_root='./data/germeval2017',\n",
    "    data_train='train_v1.4.tsv',    \n",
    "    data_validation='dev_v1.4.tsv',\n",
    "    data_test='test_TIMESTAMP1.tsv',\n",
    "    early_stopping='highest_5_F1'\n",
    ")\n",
    "def load(hp, logger):\n",
    "    dataset = Dataset(\n",
    "        'germeval',\n",
    "        logger,\n",
    "        hp,\n",
    "        source_index=0,\n",
    "        target_vocab_index=2,\n",
    "        data_path=PREFERENCES.data_root,\n",
    "        train_file=PREFERENCES.data_train,\n",
    "        valid_file=PREFERENCES.data_validation,\n",
    "        test_file=PREFERENCES.data_test,\n",
    "        file_format='.tsv',\n",
    "        init_token=None,\n",
    "        eos_token=None\n",
    "    )\n",
    "    dataset.load_data(germeval2017_dataset, verbose=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(dataset, hp, experiment_name):\n",
    "    loss = LossCombiner(4, dataset.class_weights, NllLoss)\n",
    "    transformer = TransformerEncoder(dataset.source_embedding,\n",
    "                                     hyperparameters=hp)\n",
    "    model = JointAspectTagger(transformer, hp.model_size, 4, 20, dataset.target_names)\n",
    "    optimizer = get_default_optimizer(model, hp)\n",
    "    trainer = Trainer(\n",
    "                        model,\n",
    "                        loss,\n",
    "                        optimizer,\n",
    "                        hp,\n",
    "                        dataset,\n",
    "                        experiment_name,\n",
    "                        enable_tensorboard=False,\n",
    "                        verbose=False)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_name = 'HyperParameterTest'\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\HyperParameterTest\\20190223\\9\n"
     ]
    }
   ],
   "source": [
    "# get general logger just for search\n",
    "experiment_name = utils.create_loggers(experiment_name=experiment_name)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('Run hyper parameter random grid search for experiment with name ' + experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utils.get_current_git_commit()\n",
    "logger.info('Current commit: ' + utils.get_current_git_commit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_optim_iterations = 5\n",
    "logger.info('num_optim_iterations: ' + str(num_optim_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_grid_search_ranges = {\n",
    "    'batch_size': (1, 80),\n",
    "    'num_encoder_blocks': (1, 10),\n",
    "    'pointwise_layer_size': (32, 4000),\n",
    "    'clip_comments_to': (12, 500),\n",
    "    'learning_rate': (0, 1e-2),\n",
    "    'learning_rate_factor': (1e-3, 4),\n",
    "    'learning_rate_warmup': (1000, 10000),\n",
    "    'optim_adam_beta1': (0.5, 0.99),\n",
    "    'optim_adam_beta2': (0.5, 0.99),\n",
    "    'dropout_rate': (0, 0.8),\n",
    "    'transformer_config': {\n",
    "        'transformer_heads': [1, 2, 3, 4, 5, 6, 10, 12, 15, 20]\n",
    "    }\n",
    "}\n",
    "logger.info(pprint.pformat(random_grid_search_ranges, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|          Hyperparameters           |\n",
      "+-------------------------+----------+\n",
      "|        Parameter        |  Value   |\n",
      "+-------------------------+----------+\n",
      "|        batch_size       |    12    |\n",
      "|        model_size       |   300    |\n",
      "|    learning_rate_type   |   noam   |\n",
      "|      learning_rate      |    0     |\n",
      "|   learning_rate_warmup  |   4800   |\n",
      "|   learning_rate_factor  |    2     |\n",
      "|     optim_adam_beta1    |   0.9    |\n",
      "|     optim_adam_beta2    |   0.98   |\n",
      "|      early_stopping     |    5     |\n",
      "|         use_cuda        |   True   |\n",
      "|       n_enc_blocks      |    3     |\n",
      "|         n_heads         |    6     |\n",
      "|           d_k           |    50    |\n",
      "|           d_v           |    50    |\n",
      "|       dropout_rate      |   0.1    |\n",
      "|   pointwise_layer_size  |   2048   |\n",
      "| log_every_xth_iteration |    -1    |\n",
      "|        num_epochs       |    15    |\n",
      "|      embedding_type     | fasttext |\n",
      "|      embedding_name     |    6B    |\n",
      "|      embedding_dim      |   300    |\n",
      "|     clip_comments_to    |   100    |\n",
      "|         language        |    de    |\n",
      "|      use_stop_words     |   True   |\n",
      "|           seed          |   None   |\n",
      "+-------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "default_hp = get_default_params(use_cuda)\n",
    "default_hp.num_epochs = 15\n",
    "default_hp.seed = None\n",
    "\n",
    "logger.info(default_hp)\n",
    "print(default_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+-------------------------------------------------+\n",
      "Optim Iteration: 0\n",
      "\n",
      "\n",
      "+------------------------------------------------+\n",
      "|                Hyperparameters                 |\n",
      "+-------------------------+----------------------+\n",
      "|        Parameter        |        Value         |\n",
      "+-------------------------+----------------------+\n",
      "|        batch_size       |          77          |\n",
      "|        model_size       |         300          |\n",
      "|    learning_rate_type   |         noam         |\n",
      "|      learning_rate      | 0.007356966200773427 |\n",
      "|   learning_rate_warmup  |  4249.585069120592   |\n",
      "|   learning_rate_factor  |  2.2919056206670465  |\n",
      "|     optim_adam_beta1    |  0.5585246333323819  |\n",
      "|     optim_adam_beta2    |   0.61480765986842   |\n",
      "|      early_stopping     |          5           |\n",
      "|         use_cuda        |         True         |\n",
      "|       n_enc_blocks      |          6           |\n",
      "|         n_heads         |          4           |\n",
      "|           d_k           |          75          |\n",
      "|           d_v           |          75          |\n",
      "|       dropout_rate      |  0.7187980603588087  |\n",
      "|   pointwise_layer_size  |         1975         |\n",
      "| log_every_xth_iteration |          -1          |\n",
      "|        num_epochs       |          15          |\n",
      "|      embedding_type     |       fasttext       |\n",
      "|      embedding_name     |          6B          |\n",
      "|      embedding_dim      |         300          |\n",
      "|     clip_comments_to    |         468          |\n",
      "|         language        |          de          |\n",
      "|      use_stop_words     |         True         |\n",
      "|           seed          |         None         |\n",
      "+-------------------------+----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_training - DEBUG - 20 initialized\n",
      "pre_training - DEBUG - Initilize parameters with nn.init.xavier_uniform_\n",
      "pre_training - DEBUG - Tagger initialized\n",
      "Epoch 1:   0%|                                                                                 | 0/222 [00:00<?, ?it/s]\n",
      "Could not complete iteration 0 because of CUDA out of memory. Tried to allocate 257.38 MiB (GPU 0; 6.00 GiB total capacity; 4.56 GiB already allocated; 211.76 MiB free; 1.56 MiB cached)\n",
      "\n",
      "\n",
      "+-------------------------------------------------+\n",
      "Optim Iteration: 1\n",
      "\n",
      "\n",
      "+-------------------------------------------------+\n",
      "|                 Hyperparameters                 |\n",
      "+-------------------------+-----------------------+\n",
      "|        Parameter        |         Value         |\n",
      "+-------------------------+-----------------------+\n",
      "|        batch_size       |           52          |\n",
      "|        model_size       |          300          |\n",
      "|    learning_rate_type   |          noam         |\n",
      "|      learning_rate      | 0.0008086767676339979 |\n",
      "|   learning_rate_warmup  |   1166.1701260453247  |\n",
      "|   learning_rate_factor  |   3.674485042607076   |\n",
      "|     optim_adam_beta1    |   0.6510125783837996  |\n",
      "|     optim_adam_beta2    |   0.6918758941218954  |\n",
      "|      early_stopping     |           5           |\n",
      "|         use_cuda        |          True         |\n",
      "|       n_enc_blocks      |           1           |\n",
      "|         n_heads         |           2           |\n",
      "|           d_k           |          150          |\n",
      "|           d_v           |          150          |\n",
      "|       dropout_rate      |   0.6592211012121947  |\n",
      "|   pointwise_layer_size  |          971          |\n",
      "| log_every_xth_iteration |           -1          |\n",
      "|        num_epochs       |           15          |\n",
      "|      embedding_type     |        fasttext       |\n",
      "|      embedding_name     |           6B          |\n",
      "|      embedding_dim      |          300          |\n",
      "|     clip_comments_to    |          184          |\n",
      "|         language        |           de          |\n",
      "|      use_stop_words     |          True         |\n",
      "|           seed          |          None         |\n",
      "+-------------------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_training - DEBUG - 20 initialized\n",
      "pre_training - DEBUG - Initilize parameters with nn.init.xavier_uniform_\n",
      "pre_training - DEBUG - Tagger initialized\n",
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time                                                             \n",
      "1\t328\t550.54\t\t                                                                                                         53.72\t\t0.225\t\t0.739\t\t2.71m - 2.7m / 0.0m\n",
      "2\t656\t339.47\t\t                                                                                                         53.16\t\t0.210\t\t0.674\t\t2.57m - 5.5m / 40.8m\n",
      "3\t984\t174.13\t\t                                                                                                         14.54\t\t0.235\t\t0.759\t\t2.62m - 8.1m / 38.8m\n",
      "4\t1312\t56.95\t\t                                                                                                         13.98\t\t0.238\t\t0.758\t\t2.69m - 10.8m / 39.5m\n",
      "5\t1640\t30.30\t\t                                                                                                         14.23\t\t0.245\t\t0.771\t\t2.78m - 13.6m / 40.4m\n",
      "6\t1968\t24.31\t\t                                                                                                         13.35\t\t0.258\t\t0.804\t\t2.64m - 16.3m / 41.4m\n",
      "7\t2296\t21.59\t\t                                                                                                         10.97\t\t0.278\t\t0.893\t\t2.57m - 18.9m / 40.1m\n",
      "8\t2624\t19.55\t\t                                                                                                         10.70\t\t0.278\t\t0.902\t\t2.57m - 21.5m / 39.5m\n",
      "9\t2952\t18.15\t\t                                                                                                         11.91\t\t0.270\t\t0.856\t\t2.58m - 24.1m / 39.5m\n",
      "10\t3280\t17.12\t\t                                                                                                        10.76\t\t0.281\t\t0.888\t\t2.57m - 26.7m / 39.6m\n",
      "11\t3608\t16.03\t\t                                                                                                        11.29\t\t0.283\t\t0.870\t\t2.57m - 29.3m / 39.5m\n",
      "12\t3936\t15.86\t\t                                                                                                        10.74\t\t0.289\t\t0.889\t\t2.55m - 31.9m / 39.6m\n",
      "13\t4264\t14.70\t\t                                                                                                        11.15\t\t0.275\t\t0.848\t\t2.56m - 34.6m / 39.7m\n",
      "14\t4592\t14.82\t\t                                                                                                        11.11\t\t0.281\t\t0.871\t\t2.55m - 37.2m / 39.7m\n",
      "15\t4920\t14.62\t\t                                                                                                        9.68\t\t0.288\t\t0.896\t\t2.52m - 39.7m / 39.7m\n",
      "Epoch 15: 100%|██████████████████████████████████████████████████████████████████████| 328/328 [39:41<00:00,  6.30it/s]\n",
      "+-------------------------------------------------+\n",
      "Best Valid Result: 0.2994453369624855\n",
      "+-------------------------------------------------+\n",
      "\n",
      "\n",
      "###################################################\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------------------------+\n",
      "Optim Iteration: 2\n",
      "\n",
      "\n",
      "+-------------------------------------------------+\n",
      "|                 Hyperparameters                 |\n",
      "+-------------------------+-----------------------+\n",
      "|        Parameter        |         Value         |\n",
      "+-------------------------+-----------------------+\n",
      "|        batch_size       |           77          |\n",
      "|        model_size       |          300          |\n",
      "|    learning_rate_type   |          noam         |\n",
      "|      learning_rate      | 0.0056496062931314225 |\n",
      "|   learning_rate_warmup  |   2138.6470721203805  |\n",
      "|   learning_rate_factor  |   2.0502767040431733  |\n",
      "|     optim_adam_beta1    |   0.9280180793816512  |\n",
      "|     optim_adam_beta2    |   0.860109469187704   |\n",
      "|      early_stopping     |           5           |\n",
      "|         use_cuda        |          True         |\n",
      "|       n_enc_blocks      |           9           |\n",
      "|         n_heads         |           3           |\n",
      "|           d_k           |          100          |\n",
      "|           d_v           |          100          |\n",
      "|       dropout_rate      |   0.3904539672995595  |\n",
      "|   pointwise_layer_size  |          765          |\n",
      "| log_every_xth_iteration |           -1          |\n",
      "|        num_epochs       |           15          |\n",
      "|      embedding_type     |        fasttext       |\n",
      "|      embedding_name     |           6B          |\n",
      "|      embedding_dim      |          300          |\n",
      "|     clip_comments_to    |          160          |\n",
      "|         language        |           de          |\n",
      "|      use_stop_words     |          True         |\n",
      "|           seed          |          None         |\n",
      "+-------------------------+-----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_training - DEBUG - 20 initialized\n",
      "pre_training - DEBUG - Initilize parameters with nn.init.xavier_uniform_\n",
      "pre_training - DEBUG - Tagger initialized\n",
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time                                                             \n",
      "1\t222\t530.26\t\t                                                                                                         48.22\t\t0.203\t\t0.674\t\t6.68m - 6.7m / 0.0m\n",
      "2\t444\t366.13\t\t                                                                                                         48.26\t\t0.192\t\t0.611\t\t6.81m - 13.5m / 100.3m\n",
      "3\t666\t336.59\t\t                                                                                                         32.67\t\t0.212\t\t0.691\t\t6.71m - 20.2m / 102.0m\n",
      "4\t888\t349.79\t\t                                                                                                         46.70\t\t0.192\t\t0.616\t\t6.70m - 26.9m / 100.8m\n",
      "5\t1110\t313.65\t\t                                                                                                        49.38\t\t0.183\t\t0.575\t\t6.75m - 33.7m / 100.7m\n",
      "6\t1332\t284.28\t\t                                                                                                        43.21\t\t0.189\t\t0.612\t\t6.87m - 40.6m / 101.2m\n",
      "7\t1554\t250.57\t\t                                                                                                        42.48\t\t0.178\t\t0.549\t\t7.26m - 47.8m / 102.4m\n",
      "8\t1776\t207.85\t\t                                                                                                        28.27\t\t0.196\t\t0.602\t\t6.94m - 54.8m / 105.9m\n",
      "Epoch 8: 100%|███████████████████████████████████████████████████████████████████████| 222/222 [54:46<00:00,  1.39it/s]\n",
      "\n",
      "Valid Result: 0.2994453369624855\n",
      "\n",
      "\n",
      "\n",
      "###################################################\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------------------------+\n",
      "Optim Iteration: 3\n",
      "\n",
      "\n",
      "+------------------------------------------------+\n",
      "|                Hyperparameters                 |\n",
      "+-------------------------+----------------------+\n",
      "|        Parameter        |        Value         |\n",
      "+-------------------------+----------------------+\n",
      "|        batch_size       |          7           |\n",
      "|        model_size       |         300          |\n",
      "|    learning_rate_type   |         noam         |\n",
      "|      learning_rate      | 0.006256950300715785 |\n",
      "|   learning_rate_warmup  |   4142.08866211718   |\n",
      "|   learning_rate_factor  |  2.8894856288135577  |\n",
      "|     optim_adam_beta1    |  0.5263864159083468  |\n",
      "|     optim_adam_beta2    |  0.5459422878548473  |\n",
      "|      early_stopping     |          5           |\n",
      "|         use_cuda        |         True         |\n",
      "|       n_enc_blocks      |          2           |\n",
      "|         n_heads         |          10          |\n",
      "|           d_k           |          30          |\n",
      "|           d_v           |          30          |\n",
      "|       dropout_rate      |  0.6788324534865056  |\n",
      "|   pointwise_layer_size  |         1602         |\n",
      "| log_every_xth_iteration |          -1          |\n",
      "|        num_epochs       |          15          |\n",
      "|      embedding_type     |       fasttext       |\n",
      "|      embedding_name     |          6B          |\n",
      "|      embedding_dim      |         300          |\n",
      "|     clip_comments_to    |         180          |\n",
      "|         language        |          de          |\n",
      "|      use_stop_words     |         True         |\n",
      "|           seed          |         None         |\n",
      "+-------------------------+----------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_training - DEBUG - 20 initialized\n",
      "pre_training - DEBUG - Initilize parameters with nn.init.xavier_uniform_\n",
      "pre_training - DEBUG - Tagger initialized\n",
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time                                                             \n",
      "1\t2435\t149.16\t\t                                                                                                        20.84\t\t0.229\t\t0.805\t\t7.13m - 7.1m / 0.0m\n",
      "Epoch 2:  43%|█████████████████████████████▍                                       | 1040/2435 [09:11<03:00,  7.72it/s]"
     ]
    }
   ],
   "source": [
    "dataset_logger = logging.getLogger('data_loader')\n",
    "best_f1 = 0.0\n",
    "best_model = None\n",
    "best_hp = None\n",
    "best_iteration = -1\n",
    "for optim_iteration in range(num_optim_iterations):\n",
    "        \n",
    "    print(f'\\n\\n+-------------------------------------------------+\\nOptim Iteration: {optim_iteration}\\n\\n')\n",
    "    logger.info(f'\\n\\n=================================\\nOptim Iteration: {optim_iteration}\\n=================================')\n",
    "    \n",
    "    # generate iteration hyper parameters\n",
    "    hp = randomize_params(default_hp, random_grid_search_ranges)\n",
    "        \n",
    "    logger.info('New Params:')\n",
    "    logger.info(hp)\n",
    "    print(hp)\n",
    "    \n",
    "    logger.debug('Load dataset')\n",
    "    dataset = load(hp, dataset_logger)\n",
    "    logger.debug('dataset loaded')\n",
    "    logger.debug('Load model')\n",
    "    trainer = load_model(dataset, hp, experiment_name)\n",
    "    logger.debug('model loaded')\n",
    "    \n",
    "    logger.debug('Begin training')\n",
    "    model = None\n",
    "    try:\n",
    "        result = trainer.train(use_cuda=hp.use_cuda, perform_evaluation=False)\n",
    "        model = result['model']\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not complete iteration \" + str(optim_iteration))\n",
    "        print(f'Could not complete iteration {optim_iteration} because of {str(err)}')\n",
    "        continue\n",
    "        \n",
    "    # perform evaluation and log results\n",
    "    result = None\n",
    "    try:\n",
    "        result = trainer.perform_final_evaluation(use_test_set=True, verbose=False)\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not complete iteration evaluation for it \" + str(optim_iteration))\n",
    "        print(f'Could not complete iteration {optim_iteration} evaluation because of {str(err)}')\n",
    "        continue\n",
    "    \n",
    "    it_f1 = result[1][1]\n",
    "    if best_f1 < it_f1:\n",
    "        best_f1 = it_f1\n",
    "        best_model = model\n",
    "        best_hp = copy.copy(hp)\n",
    "        best_iteration = optim_iteration\n",
    "        print('+-------------------------------------------------+')\n",
    "        print(f'Best Valid Result: {best_f1}')\n",
    "        print('+-------------------------------------------------+')\n",
    "    else:\n",
    "        print(f'\\nValid Result: {best_f1}\\n')    \n",
    "    print('\\n\\n###################################################\\n')\n",
    "    \n",
    "print('Best iteration: ' + str(best_iteration))\n",
    "print('Best f1: ' + str(best_f1))\n",
    "print('Best HP:')\n",
    "print(best_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
