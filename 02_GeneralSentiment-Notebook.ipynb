{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\Anaconda3\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "from data.data_loader import Dataset\n",
    "from data.germeval2017 import germeval2017_dataset\n",
    "\n",
    "from misc.preferences import PREFERENCES\n",
    "from misc.visualizer import *\n",
    "from misc.run_configuration import get_default_params\n",
    "from misc import utils\n",
    "\n",
    "from optimizer import get_default_optimizer\n",
    "from criterion import NllLoss\n",
    "\n",
    "from models.transformer.encoder import TransformerEncoder\n",
    "from models.softmax_output import SoftmaxOutputLayer, OutputLayer, SoftmaxOutputLayerWithCommentWiseClass\n",
    "from models.transformer_tagger import TransformerTagger\n",
    "from models.transformer.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_name = 'generalSentimentStdModel25Ep'\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\generalSentimentStdModel25Ep\n"
     ]
    }
   ],
   "source": [
    "PREFERENCES.defaults(\n",
    "    data_root='./data/germeval2017',\n",
    "    data_train='train_v1.4.tsv',    \n",
    "    data_validation='dev_v1.4.tsv',\n",
    "    data_test='test_TIMESTAMP1.tsv',\n",
    "    early_stopping='highest_5_F1'\n",
    ")\n",
    "\n",
    "hyperparameters = get_default_params()\n",
    "hyperparameters.model_size = 300\n",
    "hyperparameters.batch_size = 12\n",
    "hyperparameters.early_stopping = -1\n",
    "hyperparameters.use_cuda = use_cuda\n",
    "hyperparameters.language = 'de'\n",
    "hyperparameters.num_epochs = 25\n",
    "experiment_name = utils.create_loggers(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_training - INFO - Getting glove with dimension 300\n",
      "pre_training - INFO - Word vectors successfully loaded.\n",
      "pre_training - DEBUG - Start loading dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e507872896504b1d8c9287b3de2db631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c28ca1baa24deeb61149e4d81d6267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709afbb77c94478b847b5f884d4070a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pre_training - INFO - +-------------------------------------------------------------------------------------+\n",
      "|                                     Data Loader                                     |\n",
      "+---------------------------------+---------------------------------------------------+\n",
      "|            Parameter            |                       Value                       |\n",
      "+---------------------------------+---------------------------------------------------+\n",
      "|            embedding            |           (Embedding(95161, 300), None)           |\n",
      "|            train_iter           | <torchtext.data.iterator.BucketIterator [...]C50> |\n",
      "|            valid_iter           | <torchtext.data.iterator.BucketIterator [...]860> |\n",
      "|            test_iter            | <torchtext.data.iterator.BucketIterator [...]080> |\n",
      "|            word_field           |                        None                       |\n",
      "|               name              |                      germeval                     |\n",
      "|             dataset             | {'task': 'germeval2017', 'split_length':[...]]])} |\n",
      "|              vocabs             | (<torchtext.vocab.Vocab object at 0x0000[...]C8>) |\n",
      "|               task              |                    germeval2017                   |\n",
      "|             exaples             | [<torchtext.data.example.Example object [...]20>] |\n",
      "|            batch_size           |                         12                        |\n",
      "|             language            |                         de                        |\n",
      "|            data_path            |                ./data/germeval2017                |\n",
      "|            train_file           |                   train_v1.4.tsv                  |\n",
      "|            valid_file           |                    dev_v1.4.tsv                   |\n",
      "|            test_file            |                test_TIMESTAMP1.tsv                |\n",
      "|           file_format           |                        .tsv                       |\n",
      "|            init_token           |                        None                       |\n",
      "|            eos_token            |                        None                       |\n",
      "|    pretrained_word_embeddings   |                       glove                       |\n",
      "|  pretrained_word_embeddings_dim |                        300                        |\n",
      "| pretrained_word_embeddings_name |                         6B                        |\n",
      "|             use_cuda            |                        True                       |\n",
      "|          use_stop_words         |                        True                       |\n",
      "|              logger             |           <Logger pre_training (DEBUG)>           |\n",
      "|           split_length          |                (17043, 2049, 2095)                |\n",
      "|          total_samples          |                         -1                        |\n",
      "|           source_index          |                         0                         |\n",
      "|        target_vocab_index       |                         1                         |\n",
      "|           target_size           |                         4                         |\n",
      "|         source_embedding        |               Embedding(95161, 300)               |\n",
      "|           class_labels          | [' UNK ', 'neutral', 'negative', 'positi[...]ve'] |\n",
      "|        source_field_name        |                      comments                     |\n",
      "|        target_field_name        |                 general_sentiments                |\n",
      "|        padding_field_name       |                      padding                      |\n",
      "|         source_reverser         | <data.custom_fields.ReversibleField obje[...]C50> |\n",
      "|         target_reverser         | <torchtext.data.field.Field object at 0x[...]7F0> |\n",
      "|              fields             | [<data.custom_fields.ReversibleField obj[...]B8>] |\n",
      "|           dummy_input           | tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0[...]0]]) |\n",
      "+---------------------------------+---------------------------------------------------+\n",
      "pre_training - INFO - +-------------------------+\n",
      "|  GERM EVAL 2017 DATASET |\n",
      "+---------------+---------+\n",
      "|     Split     |   Size  |\n",
      "+---------------+---------+\n",
      "|     train     |  17043  |\n",
      "|   validation  |   2049  |\n",
      "|      test     |   2095  |\n",
      "+---------------+---------+\n",
      "+-------------------------+\n",
      "|  GERM EVAL 2017 DATASET |\n",
      "+---------------+---------+\n",
      "|     Split     |   Size  |\n",
      "+---------------+---------+\n",
      "|     train     |  17043  |\n",
      "|   validation  |   2049  |\n",
      "|      test     |   2095  |\n",
      "+---------------+---------+\n",
      "pre_training - INFO - +---------------------------+\n",
      "|      Vocabulary Stats     |\n",
      "+-------------------+-------+\n",
      "|     Vocabulary    |  Size |\n",
      "+-------------------+-------+\n",
      "|      Comments     | 95161 |\n",
      "|      Relevant     |   2   |\n",
      "| General Sentiment |   4   |\n",
      "|      Padding      |   2   |\n",
      "+-------------------+-------+\n",
      "+---------------------------+\n",
      "|      Vocabulary Stats     |\n",
      "+-------------------+-------+\n",
      "|     Vocabulary    |  Size |\n",
      "+-------------------+-------+\n",
      "|      Comments     | 95161 |\n",
      "|      Relevant     |   2   |\n",
      "| General Sentiment |   4   |\n",
      "|      Padding      |   2   |\n",
      "+-------------------+-------+\n",
      "pre_training - INFO - +--------------------+\n",
      "| General Sentiment  |\n",
      "+----------+---------+\n",
      "|  Label   | Samples |\n",
      "+----------+---------+\n",
      "| neutral  |  10599  |\n",
      "| positive |   1216  |\n",
      "| negative |   5228  |\n",
      "|   Sum    |  17043  |\n",
      "+----------+---------+\n",
      "\n",
      "+-------------------------------+\n",
      "|       General Sentiment       |\n",
      "+----------+--------------------+\n",
      "|  Label   |   Triv. Accuracy   |\n",
      "+----------+--------------------+\n",
      "| neutral  | 62.18975532476677  |\n",
      "| positive | 7.1348940914158305 |\n",
      "| negative |  30.6753505838174  |\n",
      "+----------+--------------------+\n",
      "+--------------------+\n",
      "| General Sentiment  |\n",
      "+----------+---------+\n",
      "|  Label   | Samples |\n",
      "+----------+---------+\n",
      "| neutral  |  10599  |\n",
      "| positive |   1216  |\n",
      "| negative |   5228  |\n",
      "|   Sum    |  17043  |\n",
      "+----------+---------+\n",
      "\n",
      "+-------------------------------+\n",
      "|       General Sentiment       |\n",
      "+----------+--------------------+\n",
      "|  Label   |   Triv. Accuracy   |\n",
      "+----------+--------------------+\n",
      "| neutral  | 62.18975532476677  |\n",
      "| positive | 7.1348940914158305 |\n",
      "| negative |  30.6753505838174  |\n",
      "+----------+--------------------+\n",
      "pre_training - INFO - Dataset loaded. Ready for training\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(\n",
    "    'germeval',\n",
    "    logging.getLogger('pre_training'),\n",
    "    hyperparameters,\n",
    "    source_index=0,\n",
    "    target_vocab_index=1,\n",
    "    data_path=PREFERENCES.data_root,\n",
    "    train_file=PREFERENCES.data_train,\n",
    "    valid_file=PREFERENCES.data_validation,\n",
    "    test_file=PREFERENCES.data_test,\n",
    "    file_format='.tsv',\n",
    "    init_token=None,\n",
    "    eos_token=None\n",
    ")\n",
    "dataset.load_data(germeval2017_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = NllLoss(dataset.target_size)\n",
    "transformer = TransformerEncoder(dataset.source_embedding,\n",
    "                                 hyperparameters=hyperparameters)\n",
    "tagging_softmax = SoftmaxOutputLayerWithCommentWiseClass(hyperparameters.model_size, dataset.target_size)\n",
    "model = TransformerTagger(transformer, tagging_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_training - INFO - TransformerTagger (\n",
      "  (encoder): TransformerEncoder(\n",
      "    (src_embeddings): Embedding(95161, 300)\n",
      "    (positional_encoding): PositionalEncoding2(\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder_blocks): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (self_attention_layer): MultiHeadedSelfAttentionLayer(\n",
      "          (query_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (key_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (value_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (attention_layer): ScaledDotProductAttentionLayer(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_0): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (feed_forward_layer): PointWiseFCLayer(\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_1): Linear(in_features=300, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=300, bias=True)\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "      )\n",
      "      (1): EncoderBlock(\n",
      "        (self_attention_layer): MultiHeadedSelfAttentionLayer(\n",
      "          (query_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (key_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (value_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (attention_layer): ScaledDotProductAttentionLayer(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_0): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (feed_forward_layer): PointWiseFCLayer(\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_1): Linear(in_features=300, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=300, bias=True)\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  ), weights=((95161, 300), (300, 300), (300, 300), (300, 300), (300,), (300,), (300, 300), (300,), (300,), (2048, 300), (2048,), (300, 2048), (300,), (300,), (300,), (300, 300), (300, 300), (300, 300), (300,), (300,), (300, 300), (300,), (300,), (2048, 300), (2048,), (300, 2048), (300,), (300,), (300,), (300,), (300,)), parameters=31734796\n",
      "  (taggingLayer): SoftmaxOutputLayerWithCommentWiseClass(\n",
      "    (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "  ), weights=((4, 300), (4,)), parameters=1204\n",
      ")\n",
      "==================================\n",
      "Total Number of parameters: 31.736.000\n",
      "==================================\n",
      "\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1              [-1, 42, 300]      28,548,300\n",
      "           Dropout-2              [-1, 42, 300]               0\n",
      "PositionalEncoding2-3              [-1, 42, 300]               0\n",
      "            Linear-4              [-1, 42, 300]          90,000\n",
      "            Linear-5              [-1, 42, 300]          90,000\n",
      "            Linear-6              [-1, 42, 300]          90,000\n",
      "           Dropout-7               [-1, 42, 42]               0\n",
      "ScaledDotProductAttentionLayer-8              [-1, 42, 100]               0\n",
      "            Linear-9              [-1, 42, 300]          90,000\n",
      "          Dropout-10              [-1, 42, 300]               0\n",
      "        LayerNorm-11              [-1, 42, 300]               0\n",
      "MultiHeadedSelfAttentionLayer-12              [-1, 42, 300]               0\n",
      "           Linear-13             [-1, 42, 2048]         616,448\n",
      "           Linear-14              [-1, 42, 300]         614,700\n",
      "          Dropout-15              [-1, 42, 300]               0\n",
      "        LayerNorm-16              [-1, 42, 300]               0\n",
      "     EncoderBlock-17              [-1, 42, 300]               0\n",
      "           Linear-18              [-1, 42, 300]          90,000\n",
      "           Linear-19              [-1, 42, 300]          90,000\n",
      "           Linear-20              [-1, 42, 300]          90,000\n",
      "          Dropout-21               [-1, 42, 42]               0\n",
      "ScaledDotProductAttentionLayer-22              [-1, 42, 100]               0\n",
      "           Linear-23              [-1, 42, 300]          90,000\n",
      "          Dropout-24              [-1, 42, 300]               0\n",
      "        LayerNorm-25              [-1, 42, 300]               0\n",
      "MultiHeadedSelfAttentionLayer-26              [-1, 42, 300]               0\n",
      "           Linear-27             [-1, 42, 2048]         616,448\n",
      "           Linear-28              [-1, 42, 300]         614,700\n",
      "          Dropout-29              [-1, 42, 300]               0\n",
      "        LayerNorm-30              [-1, 42, 300]               0\n",
      "     EncoderBlock-31              [-1, 42, 300]               0\n",
      "TransformerEncoder-32              [-1, 42, 300]               0\n",
      "           Linear-33                [-1, 42, 4]           1,204\n",
      "SoftmaxOutputLayerWithCommentWiseClass-34                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 31,731,800\n",
      "Trainable params: 31,731,800\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.90\n",
      "Params size (MB): 121.05\n",
      "Estimated Total Size (MB): 124.95\n",
      "----------------------------------------------------------------\n",
      "pre_training - INFO - TransformerTagger (\n",
      "  (encoder): TransformerEncoder(\n",
      "    (src_embeddings): Embedding(95161, 300)\n",
      "    (positional_encoding): PositionalEncoding2(\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder_blocks): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (self_attention_layer): MultiHeadedSelfAttentionLayer(\n",
      "          (query_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (key_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (value_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (attention_layer): ScaledDotProductAttentionLayer(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_0): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (feed_forward_layer): PointWiseFCLayer(\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_1): Linear(in_features=300, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=300, bias=True)\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "      )\n",
      "      (1): EncoderBlock(\n",
      "        (self_attention_layer): MultiHeadedSelfAttentionLayer(\n",
      "          (query_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (key_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (value_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (attention_layer): ScaledDotProductAttentionLayer(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_0): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (feed_forward_layer): PointWiseFCLayer(\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_1): Linear(in_features=300, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=300, bias=True)\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  ), weights=((95161, 300), (300, 300), (300, 300), (300, 300), (300,), (300,), (300, 300), (300,), (300,), (2048, 300), (2048,), (300, 2048), (300,), (300,), (300,), (300, 300), (300, 300), (300, 300), (300,), (300,), (300, 300), (300,), (300,), (2048, 300), (2048,), (300, 2048), (300,), (300,), (300,), (300,), (300,)), parameters=31734796\n",
      "  (taggingLayer): SoftmaxOutputLayerWithCommentWiseClass(\n",
      "    (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "  ), weights=((4, 300), (4,)), parameters=1204\n",
      ")\n",
      "==================================\n",
      "Total Number of parameters: 31.736.000\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pre_training - INFO - Classes: [' UNK ', 'neutral', 'negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "optimizer = get_default_optimizer(model, hyperparameters)\n",
    "trainer = Trainer(\n",
    "                    model,\n",
    "                    loss,\n",
    "                    optimizer,\n",
    "                    hyperparameters,\n",
    "                    dataset,\n",
    "                    experiment_name,\n",
    "                    enable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_training - DEBUG - train with cuda support\n",
      "pre_training - INFO - 1421 Iterations per epoch with batch size of 12\n",
      "pre_training - INFO - START training.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bb3646988b430a976090af0d9b56ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43da07904ba945b0b95b32a3089f24a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t200\t123.442\t\t25.632\t\t0.620\t\t0.620\t\t0.00m - 0.4m / 0.0m\n",
      "1\t400\t117.662\t\t23.960\t\t0.288\t\t0.288\t\t0.00m - 1.7m / 0.0m\n",
      "1\t600\t137.199\t\t16.322\t\t0.311\t\t0.311\t\t0.00m - 3.0m / 0.0m\n",
      "1\t800\t69.068\t\t49.160\t\t0.301\t\t0.301\t\t0.00m - 4.3m / 0.0m\n",
      "1\t1000\t56.157\t\t2.437\t\t0.618\t\t0.618\t\t0.00m - 5.5m / 0.0m\n",
      "1\t1200\t64.892\t\t3.757\t\t0.623\t\t0.623\t\t0.00m - 6.8m / 0.0m\n",
      "1\t1400\t36.405\t\t6.926\t\t0.624\t\t0.624\t\t0.00m - 8.1m / 0.0m\n",
      "1\t1421\t85.839\t\t3.336\t\t0.630\t\t0.630\t\t9.76m - 9.8m / 0.0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebcb953b2414005ae5e4e0467cd319d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c789aed5e4d4641aaf0ff14e6981bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t1600\t41.635\t\t8.276\t\t0.301\t\t0.301\t\t9.76m - 10.2m / 244.1m\n",
      "2\t1800\t26.139\t\t2.438\t\t0.627\t\t0.627\t\t9.76m - 11.4m / 244.1m\n",
      "2\t2000\t29.028\t\t1.630\t\t0.677\t\t0.677\t\t9.76m - 12.6m / 244.1m\n",
      "2\t2200\t17.640\t\t1.423\t\t0.612\t\t0.612\t\t9.76m - 13.8m / 244.1m\n",
      "2\t2400\t13.518\t\t5.375\t\t0.301\t\t0.301\t\t9.76m - 15.0m / 244.1m\n",
      "2\t2600\t13.359\t\t2.297\t\t0.205\t\t0.205\t\t9.76m - 16.4m / 244.1m\n",
      "2\t2800\t8.509\t\t3.650\t\t0.623\t\t0.623\t\t9.76m - 17.7m / 244.1m\n",
      "2\t2842\t20.804\t\t1.730\t\t0.647\t\t0.647\t\t9.77m - 19.6m / 244.1m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf88589b732644498c5ac30e32a8e8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f859a41b264754b1d33db490f39604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t3000\t9.639\t\t1.022\t\t0.641\t\t0.641\t\t9.77m - 20.0m / 244.2m\n",
      "3\t3200\t8.738\t\t0.960\t\t0.675\t\t0.675\t\t9.77m - 21.3m / 244.2m\n",
      "3\t3400\t6.824\t\t1.259\t\t0.625\t\t0.625\t\t9.77m - 22.6m / 244.2m\n",
      "3\t3600\t6.199\t\t2.024\t\t0.439\t\t0.439\t\t9.77m - 23.9m / 244.2m\n",
      "3\t3800\t5.518\t\t1.837\t\t0.301\t\t0.301\t\t9.77m - 25.1m / 244.2m\n",
      "3\t4000\t4.715\t\t1.961\t\t0.301\t\t0.301\t\t9.77m - 26.4m / 244.2m\n",
      "3\t4200\t5.678\t\t1.733\t\t0.611\t\t0.611\t\t9.77m - 27.7m / 244.2m\n",
      "3\t4263\t6.372\t\t1.699\t\t0.614\t\t0.614\t\t9.91m - 29.5m / 244.2m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a1cf6b0910409284215a5a2f9c5164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f1b12ae42b4da0b07d6123c462334a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\t4400\t5.663\t\t1.778\t\t0.301\t\t0.301\t\t9.91m - 29.8m / 247.6m\n",
      "4\t4600\t4.886\t\t1.979\t\t0.079\t\t0.079\t\t9.91m - 31.1m / 247.6m\n",
      "4\t4800\t4.693\t\t0.989\t\t0.672\t\t0.672\t\t9.91m - 32.3m / 247.6m\n",
      "4\t5000\t3.019\t\t1.174\t\t0.624\t\t0.624\t\t9.91m - 33.5m / 247.6m\n",
      "4\t5200\t2.728\t\t1.039\t\t0.644\t\t0.644\t\t9.91m - 34.7m / 247.6m\n",
      "4\t5400\t2.295\t\t1.012\t\t0.629\t\t0.629\t\t9.91m - 36.0m / 247.6m\n",
      "4\t5600\t1.768\t\t1.115\t\t0.622\t\t0.622\t\t9.91m - 37.2m / 247.6m\n",
      "4\t5684\t3.475\t\t1.007\t\t0.642\t\t0.642\t\t9.55m - 39.1m / 247.6m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd82a61e93824b94b853d1adc353af95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762d834d93a64e43a67306d6e0311cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\t5800\t1.355\t\t0.990\t\t0.616\t\t0.616\t\t9.55m - 39.3m / 239.6m\n",
      "5\t6000\t2.772\t\t1.027\t\t0.623\t\t0.623\t\t9.55m - 40.6m / 239.6m\n",
      "5\t6200\t1.365\t\t1.004\t\t0.673\t\t0.673\t\t9.55m - 41.8m / 239.6m\n",
      "5\t6400\t1.443\t\t1.001\t\t0.658\t\t0.658\t\t9.55m - 43.1m / 239.6m\n",
      "5\t6600\t1.468\t\t1.002\t\t0.654\t\t0.654\t\t9.55m - 44.3m / 239.6m\n",
      "5\t6800\t1.198\t\t1.036\t\t0.622\t\t0.622\t\t9.55m - 45.6m / 239.6m\n",
      "5\t7000\t1.352\t\t1.086\t\t0.428\t\t0.428\t\t9.55m - 46.9m / 239.6m\n",
      "5\t7105\t1.523\t\t1.018\t\t0.665\t\t0.665\t\t9.58m - 48.7m / 239.6m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e847659ba34006b96ff892bafc673f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16859ce7b3e64c3799a74eaa8ad936a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\t7200\t1.030\t\t0.999\t\t0.658\t\t0.658\t\t9.58m - 48.9m / 240.4m\n",
      "6\t7400\t0.969\t\t1.075\t\t0.543\t\t0.543\t\t9.58m - 50.1m / 240.4m\n",
      "6\t7600\t1.262\t\t1.092\t\t0.433\t\t0.433\t\t9.58m - 51.3m / 240.4m\n",
      "6\t7800\t0.916\t\t1.023\t\t0.623\t\t0.623\t\t9.58m - 52.5m / 240.4m\n",
      "6\t8000\t1.105\t\t1.020\t\t0.635\t\t0.635\t\t9.58m - 53.7m / 240.4m\n",
      "6\t8200\t1.037\t\t1.012\t\t0.673\t\t0.673\t\t9.58m - 54.9m / 240.4m\n",
      "6\t8400\t1.242\t\t1.034\t\t0.628\t\t0.628\t\t9.58m - 56.1m / 240.4m\n",
      "6\t8526\t1.080\t\t1.032\t\t0.653\t\t0.653\t\t9.24m - 57.9m / 240.4m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640cfa6aa6814a62ab587603527fd8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6211f1c07725409faf04bb5123e469d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\t8600\t1.055\t\t0.961\t\t0.662\t\t0.662\t\t9.24m - 58.1m / 233.5m\n",
      "7\t8800\t1.062\t\t1.041\t\t0.660\t\t0.660\t\t9.24m - 59.5m / 233.5m\n",
      "7\t9000\t0.986\t\t1.024\t\t0.657\t\t0.657\t\t9.24m - 60.7m / 233.5m\n",
      "7\t9200\t1.039\t\t0.997\t\t0.635\t\t0.635\t\t9.24m - 62.0m / 233.5m\n",
      "7\t9400\t0.980\t\t0.988\t\t0.626\t\t0.626\t\t9.24m - 63.3m / 233.5m\n",
      "7\t9600\t0.910\t\t0.999\t\t0.662\t\t0.662\t\t9.24m - 64.5m / 233.5m\n",
      "7\t9800\t0.847\t\t1.002\t\t0.649\t\t0.649\t\t9.24m - 65.8m / 233.5m\n",
      "7\t9947\t0.981\t\t1.049\t\t0.611\t\t0.611\t\t9.82m - 67.8m / 233.5m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14571aa523e54cbd91338b4f8c6907ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6d44cdfdfe494cb4fa8a949180b846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\t10000\t0.994\t\t1.016\t\t0.636\t\t0.636\t\t9.82m - 68.1m / 244.7m\n",
      "8\t10200\t1.297\t\t1.028\t\t0.651\t\t0.651\t\t9.82m - 69.4m / 244.7m\n",
      "8\t10400\t0.897\t\t0.985\t\t0.639\t\t0.639\t\t9.82m - 70.7m / 244.7m\n",
      "8\t10600\t0.998\t\t1.057\t\t0.651\t\t0.651\t\t9.82m - 72.0m / 244.7m\n",
      "8\t10800\t0.930\t\t1.093\t\t0.530\t\t0.530\t\t9.82m - 73.2m / 244.7m\n",
      "8\t11000\t0.924\t\t1.001\t\t0.673\t\t0.673\t\t9.82m - 74.5m / 244.7m\n",
      "8\t11200\t0.910\t\t1.014\t\t0.637\t\t0.637\t\t9.82m - 75.8m / 244.7m\n",
      "8\t11368\t0.986\t\t1.026\t\t0.643\t\t0.643\t\t9.83m - 77.8m / 244.7m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707cc6815ae341c299e0313b15e51139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.train(hyperparameters.num_epochs, use_cuda=hyperparameters.use_cuda, perform_evaluation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = result['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels = trainer.classify_sentence('I was born in 1993 in Stuttgart')\n",
    "\n",
    "\n",
    "\n",
    "print(result_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = trainer.text_reverser[1]\n",
    "lr = trainer.label_reverser\n",
    "\n",
    "test_sentence = ['china', 'controlled', 'most', 'of', 'the', 'match']\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = tr.preprocess('Die Bahn ist nicht gut')\n",
    "\n",
    "#test_sentence = tr.preprocess('china controlled most of the match on 1993')\n",
    "test_sentence = [x.strip(' ') for x in test_sentence]\n",
    "test_sentence = [test_sentence]\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test = tr.preprocess('china controlled most of the match')\n",
    "#print(test)\n",
    "\n",
    "#test_sentence = [['china', 'controlled', 'most', 'of', 'the', 'match']]\n",
    "x = tr.process(test_sentence)\n",
    "\n",
    "print(\"X TENSOR \",x)\n",
    "print('X Size', x.size())\n",
    "print(\"Reversed X\", tr.reverse(x))\n",
    "x = x.cuda()\n",
    "y_hat = model.predict(x)\n",
    "y_hat_label = lr.reverse(y_hat)\n",
    "print(y_hat_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tb_writer = None\n",
    "trainer.enable_tensorboard = False\n",
    "evaluation_results = trainer.perform_final_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_matrix = evaluation_results[1][2]\n",
    "c_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plot_confusion_matrix(c_matrix, class_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plot_confusion_matrix(c_matrix, class_labels, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict now to see model in final state\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "df = predict_some_examples_to_df(model, conll2003['iters'][2], num_samples=800)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = predict_some_examples_to_df(model, conll2003['iters'][1], num_samples=800)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = predict_some_examples_to_df(model, conll2003['iters'][0], num_samples=800)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([\n",
    "    np.array([[1, 1], [1, 1]]),\n",
    "    np.array([[2, 2], [-2, -3]])\n",
    "])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = predict_some_examples_to_df(model, test_sample_iter)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(tr_loss, tr_f1) = result['result_train']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
