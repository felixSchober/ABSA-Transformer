{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from data.data_loader import Dataset\n",
    "from data.germeval2017 import germeval2017_dataset\n",
    "from misc.preferences import PREFERENCES\n",
    "from misc.run_configuration import get_default_params, randomize_params, OutputLayerType, hyperOpt_goodParams\n",
    "from misc import utils\n",
    "from misc.hyperopt_space import *\n",
    "\n",
    "from optimizer import get_optimizer\n",
    "from criterion import NllLoss, LossCombiner\n",
    "\n",
    "from models.transformer.encoder import TransformerEncoder\n",
    "from models.jointAspectTagger import JointAspectTagger\n",
    "from trainer.train import Trainer\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Functions\n",
    "\n",
    "These functions will load the dataset and the model. The run configuration will determine the architecture and hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(dataset, rc, experiment_name):\n",
    "    loss = LossCombiner(4, dataset.class_weights, NllLoss)\n",
    "    transformer = TransformerEncoder(dataset.source_embedding,\n",
    "                                     hyperparameters=rc)\n",
    "    model = JointAspectTagger(transformer, rc, 4, 20, dataset.target_names)\n",
    "    optimizer = get_optimizer(model, rc)\n",
    "    trainer = Trainer(\n",
    "                        model,\n",
    "                        loss,\n",
    "                        optimizer,\n",
    "                        rc,\n",
    "                        dataset,\n",
    "                        experiment_name,\n",
    "                        enable_tensorboard=False,\n",
    "                        verbose=True)\n",
    "    return trainer\n",
    "\n",
    "def load_dataset(rc, logger):\n",
    "    dataset = Dataset(\n",
    "        'germeval',\n",
    "        logger,\n",
    "        rc,\n",
    "        source_index=0,\n",
    "        target_vocab_index=2,\n",
    "        data_path=PREFERENCES.data_root,\n",
    "        train_file=PREFERENCES.data_train,\n",
    "        valid_file=PREFERENCES.data_validation,\n",
    "        test_file=PREFERENCES.data_test,\n",
    "        file_format='.tsv',\n",
    "        init_token=None,\n",
    "        eos_token=None\n",
    "    )\n",
    "    dataset.load_data(germeval2017_dataset, verbose=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble - Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\HyperParameterTest\\20190317\\1\n"
     ]
    }
   ],
   "source": [
    "PREFERENCES.defaults(\n",
    "    data_root='./data/germeval2017',\n",
    "    data_train='train_v1.4.tsv',    \n",
    "    data_validation='dev_v1.4.tsv',\n",
    "    data_test='test_TIMESTAMP1.tsv',\n",
    "    early_stopping='highest_5_F1'\n",
    ")\n",
    "experiment_name = 'HyperParameterTest'\n",
    "use_cuda = True\n",
    "\n",
    "# get general logger just for search\n",
    "experiment_name = utils.create_loggers(experiment_name=experiment_name)\n",
    "logger = logging.getLogger(__name__)\n",
    "dataset_logger = logging.getLogger('data_loader')\n",
    "logger.info('Run hyper parameter random grid search for experiment with name ' + experiment_name)\n",
    "\n",
    "num_optim_iterations = 100\n",
    "logger.info('num_optim_iterations: ' + str(num_optim_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current commit: b'a25c482'\n"
     ]
    }
   ],
   "source": [
    "utils.get_current_git_commit()\n",
    "logger.info('Current commit: ' + utils.get_current_git_commit())\n",
    "print('Current commit: ' + utils.get_current_git_commit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective():\n",
    "    run_time = time.time()\n",
    "\n",
    "    # generate hp's from parameters\n",
    "    rc = get_default_params(use_cuda, hyperOpt_goodParams)\n",
    "    rc.num_epochs = 25\n",
    "    rc.log_every_xth_iteration = -1\n",
    "    print(rc)\n",
    "\n",
    "    logger.debug('Load dataset')\n",
    "    try:\n",
    "        dataset = load_dataset(rc, dataset_logger)\n",
    "    except Exception as err:\n",
    "        print('Could load dataset: ' + str(err))\n",
    "        logger.exception(\"Could not load dataset\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "    logger.debug('dataset loaded')\n",
    "    logger.debug('Load model')\n",
    "\n",
    "    try:\n",
    "        trainer = load_model(dataset, rc, experiment_name)\n",
    "    except Exception as err:\n",
    "        print('Could load model: ' + str(err))\n",
    "        logger.exception(\"Could not load model\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "\n",
    "    logger.debug('model loaded')\n",
    "    print('Loading model')\n",
    "    model = None\n",
    "    \n",
    "    trainer.load_model(custom_path='C:\\\\Users\\\\felix\\\\OneDrive\\\\Studium\\\\Studium\\\\6. Semester\\\\MA\\\\Project\\\\ABSA-Transformer\\\\logs\\\\test')\n",
    "    print('Loading finished')\n",
    "    trainer.set_cuda(True)\n",
    "\n",
    "    # perform evaluation and log results\n",
    "    result = None\n",
    "    try:\n",
    "        result = trainer.perform_final_evaluation(use_test_set=True, verbose=False)\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not complete iteration evaluation.\")\n",
    "        print('Could not complete iteration evaluation: ' + str(err))\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "    print(f'Best f1 {trainer.get_best_f1()}')\n",
    "    return {\n",
    "            'loss': result[1][0],\n",
    "            'status': STATUS_OK,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1(),\n",
    "            'results': {\n",
    "                'train': {\n",
    "                    'loss': result[0][0],\n",
    "                    'f1': result[0][1]\n",
    "                },\n",
    "                'validation': {\n",
    "                    'loss': result[1][0],\n",
    "                    'f1': result[1][1]\n",
    "                },\n",
    "                'test': {\n",
    "                    'loss': result[2][0],\n",
    "                    'f1': result[2][1]\n",
    "                }\n",
    "            },\n",
    "            'trainer': trainer,\n",
    "            'model': trainer.model,\n",
    "            'optimizer': trainer.optimizer.optimizer\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'batch_size': 12, 'learning_rate_schedu[...]rue} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|          batch_size          |                         12                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        embedding_type        |                      fasttext                     |\n",
      "|        learning_rate         |                      7.2e-05                      |\n",
      "|  noam_learning_rate_warmup   |                        8000                       |\n",
      "|  noam_learning_rate_factor   |                       1.418                       |\n",
      "|          adam_beta1          |                        0.81                       |\n",
      "|          adam_beta2          |                       0.7173                      |\n",
      "|           adam_eps           |                      0.000814                     |\n",
      "|      adam_weight_decay       |                         0                         |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         1                         |\n",
      "|             d_k              |                        300                        |\n",
      "|             d_v              |                        300                        |\n",
      "|         dropout_rate         |                      0.302424                     |\n",
      "|     pointwise_layer_size     |                        405                        |\n",
      "|      last_layer_dropout      |                  0.79602089766246                 |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         25                        |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        113                        |\n",
      "|           language           |                         de                        |\n",
      "|        use_stop_words        |                        True                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                        True                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "pre_training - INFO - Classes: ['n/a', 'neutral', 'negative', 'positive']\n",
      "pre_training - INFO - JointAspectTagger (\n",
      "  (encoder): TransformerEncoder(\n",
      "    (src_embeddings): Embedding(68611, 300)\n",
      "    (positional_encoding): PositionalEncoding2(\n",
      "      (dropout): Dropout(p=0.302424)\n",
      "    )\n",
      "    (encoder_blocks): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (self_attention_layer): MultiHeadedSelfAttentionLayer(\n",
      "          (query_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (key_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (value_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (attention_layer): ScaledDotProductAttentionLayer(\n",
      "            (dropout): Dropout(p=0.302424)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_0): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (dropout): Dropout(p=0.302424)\n",
      "        )\n",
      "        (feed_forward_layer): PointWiseFCLayer(\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_1): Linear(in_features=300, out_features=405, bias=False)\n",
      "          (w_2): Linear(in_features=405, out_features=300, bias=False)\n",
      "          (dropout): Dropout(p=0.302424)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "      )\n",
      "      (1): EncoderBlock(\n",
      "        (self_attention_layer): MultiHeadedSelfAttentionLayer(\n",
      "          (query_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (key_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (value_projections): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (attention_layer): ScaledDotProductAttentionLayer(\n",
      "            (dropout): Dropout(p=0.302424)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_0): Linear(in_features=300, out_features=300, bias=False)\n",
      "          (dropout): Dropout(p=0.302424)\n",
      "        )\n",
      "        (feed_forward_layer): PointWiseFCLayer(\n",
      "          (layer_norm): LayerNorm()\n",
      "          (w_1): Linear(in_features=300, out_features=405, bias=False)\n",
      "          (w_2): Linear(in_features=405, out_features=300, bias=False)\n",
      "          (dropout): Dropout(p=0.302424)\n",
      "        )\n",
      "        (layer_norm): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  ), weights=((68611, 300), (300, 300), (300, 300), (300, 300), (300,), (300,), (300, 300), (300,), (300,), (405, 300), (300, 405), (300,), (300,), (300, 300), (300, 300), (300, 300), (300,), (300,), (300, 300), (300,), (300,), (405, 300), (300, 405), (300,), (300,), (300,), (300,)), parameters=21793500\n",
      "  (taggers): ModuleList(\n",
      "    (0): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (1): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (2): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (3): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (4): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (5): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (6): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (7): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (8): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (9): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (10): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (11): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (12): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (13): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (14): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (15): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (16): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (17): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (18): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "    (19): CommentWiseSumLogSoftmax(\n",
      "      (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
      "      (dropout): Dropout(p=0.79602089766246)\n",
      "    )\n",
      "  ), weights=((4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,), (4, 300), (4,)), parameters=24080\n",
      ")\n",
      "==================================\n",
      "Total Number of parameters: 21.817.580\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1             [-1, 113, 300]      20,583,300\n",
      "           Dropout-2             [-1, 113, 300]               0\n",
      "PositionalEncoding2-3             [-1, 113, 300]               0\n",
      "            Linear-4             [-1, 113, 300]          90,000\n",
      "            Linear-5             [-1, 113, 300]          90,000\n",
      "            Linear-6             [-1, 113, 300]          90,000\n",
      "           Dropout-7             [-1, 113, 113]               0\n",
      "ScaledDotProductAttentionLayer-8             [-1, 113, 300]               0\n",
      "            Linear-9             [-1, 113, 300]          90,000\n",
      "          Dropout-10             [-1, 113, 300]               0\n",
      "        LayerNorm-11             [-1, 113, 300]               0\n",
      "MultiHeadedSelfAttentionLayer-12             [-1, 113, 300]               0\n",
      "           Linear-13             [-1, 113, 405]         121,500\n",
      "           Linear-14             [-1, 113, 300]         121,500\n",
      "          Dropout-15             [-1, 113, 300]               0\n",
      "        LayerNorm-16             [-1, 113, 300]               0\n",
      "     EncoderBlock-17             [-1, 113, 300]               0\n",
      "           Linear-18             [-1, 113, 300]          90,000\n",
      "           Linear-19             [-1, 113, 300]          90,000\n",
      "           Linear-20             [-1, 113, 300]          90,000\n",
      "          Dropout-21             [-1, 113, 113]               0\n",
      "ScaledDotProductAttentionLayer-22             [-1, 113, 300]               0\n",
      "           Linear-23             [-1, 113, 300]          90,000\n",
      "          Dropout-24             [-1, 113, 300]               0\n",
      "        LayerNorm-25             [-1, 113, 300]               0\n",
      "MultiHeadedSelfAttentionLayer-26             [-1, 113, 300]               0\n",
      "           Linear-27             [-1, 113, 405]         121,500\n",
      "           Linear-28             [-1, 113, 300]         121,500\n",
      "          Dropout-29             [-1, 113, 300]               0\n",
      "        LayerNorm-30             [-1, 113, 300]               0\n",
      "     EncoderBlock-31             [-1, 113, 300]               0\n",
      "TransformerEncoder-32             [-1, 113, 300]               0\n",
      "           Linear-33               [-1, 113, 4]           1,204\n",
      "          Dropout-34               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-35                    [-1, 4]               0\n",
      "           Linear-36               [-1, 113, 4]           1,204\n",
      "          Dropout-37               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-38                    [-1, 4]               0\n",
      "           Linear-39               [-1, 113, 4]           1,204\n",
      "          Dropout-40               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-41                    [-1, 4]               0\n",
      "           Linear-42               [-1, 113, 4]           1,204\n",
      "          Dropout-43               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-44                    [-1, 4]               0\n",
      "           Linear-45               [-1, 113, 4]           1,204\n",
      "          Dropout-46               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-47                    [-1, 4]               0\n",
      "           Linear-48               [-1, 113, 4]           1,204\n",
      "          Dropout-49               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-50                    [-1, 4]               0\n",
      "           Linear-51               [-1, 113, 4]           1,204\n",
      "          Dropout-52               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-53                    [-1, 4]               0\n",
      "           Linear-54               [-1, 113, 4]           1,204\n",
      "          Dropout-55               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-56                    [-1, 4]               0\n",
      "           Linear-57               [-1, 113, 4]           1,204\n",
      "          Dropout-58               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-59                    [-1, 4]               0\n",
      "           Linear-60               [-1, 113, 4]           1,204\n",
      "          Dropout-61               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-62                    [-1, 4]               0\n",
      "           Linear-63               [-1, 113, 4]           1,204\n",
      "          Dropout-64               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-65                    [-1, 4]               0\n",
      "           Linear-66               [-1, 113, 4]           1,204\n",
      "          Dropout-67               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-68                    [-1, 4]               0\n",
      "           Linear-69               [-1, 113, 4]           1,204\n",
      "          Dropout-70               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-71                    [-1, 4]               0\n",
      "           Linear-72               [-1, 113, 4]           1,204\n",
      "          Dropout-73               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-74                    [-1, 4]               0\n",
      "           Linear-75               [-1, 113, 4]           1,204\n",
      "          Dropout-76               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-77                    [-1, 4]               0\n",
      "           Linear-78               [-1, 113, 4]           1,204\n",
      "          Dropout-79               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-80                    [-1, 4]               0\n",
      "           Linear-81               [-1, 113, 4]           1,204\n",
      "          Dropout-82               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-83                    [-1, 4]               0\n",
      "           Linear-84               [-1, 113, 4]           1,204\n",
      "          Dropout-85               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-86                    [-1, 4]               0\n",
      "           Linear-87               [-1, 113, 4]           1,204\n",
      "          Dropout-88               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-89                    [-1, 4]               0\n",
      "           Linear-90               [-1, 113, 4]           1,204\n",
      "          Dropout-91               [-1, 113, 4]               0\n",
      "CommentWiseSumLogSoftmax-92                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 21,813,380\n",
      "Trainable params: 21,813,380\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 8.27\n",
      "Params size (MB): 83.21\n",
      "Estimated Total Size (MB): 91.49\n",
      "----------------------------------------------------------------\n",
      "Loading model\n",
      "pre_training - INFO - Try to load model at C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\test\n",
      "pre_training - INFO - Load checkpoint at C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\test\\checkpoint_11368.data\n",
      "pre_training - INFO - Loaded model at epoch 7 with reported f1 of 0.3213599383691035\n",
      "pre_training - INFO - Model should be used with following hyper parameters: \n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'batch_size': 12, 'learning_rate_schedu[...]rue} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|          batch_size          |                         12                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        embedding_type        |                      fasttext                     |\n",
      "|        learning_rate         |                      7.2e-05                      |\n",
      "|  noam_learning_rate_warmup   |                        8000                       |\n",
      "|  noam_learning_rate_factor   |                       1.418                       |\n",
      "|          adam_beta1          |                        0.81                       |\n",
      "|          adam_beta2          |                       0.7173                      |\n",
      "|           adam_eps           |                      0.000814                     |\n",
      "|      adam_weight_decay       |                       1e-05                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         1                         |\n",
      "|             d_k              |                        300                        |\n",
      "|             d_v              |                        300                        |\n",
      "|         dropout_rate         |                      0.302424                     |\n",
      "|     pointwise_layer_size     |                        405                        |\n",
      "|      last_layer_dropout      |                  0.79602089766246                 |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         20                        |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        113                        |\n",
      "|           language           |                         de                        |\n",
      "|        use_stop_words        |                        True                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                        True                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                       False                       |\n",
      "|             seed             |                         42                        |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------------------------------------------------+\n",
      "pre_training - INFO - Hyperparameters are compatible!\n",
      "Loading finished\n",
      "pre_training - DEBUG - train with cuda support\n",
      "pre_training - DEBUG - --- Valid Scores ---\n",
      "Best f1 0.3213599383691035\n"
     ]
    }
   ],
   "source": [
    "result = objective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['trainer'].optimizer.optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_f1': 0.3213599383691035,\n",
       " 'best_loss': 1000.0,\n",
       " 'eval_time': 205.22377395629883,\n",
       " 'loss': 11.043194846103066,\n",
       " 'model': JointAspectTagger(\n",
       "   (encoder): TransformerEncoder(\n",
       "     (src_embeddings): Embedding(68611, 300)\n",
       "     (positional_encoding): PositionalEncoding2(\n",
       "       (dropout): Dropout(p=0.302424)\n",
       "     )\n",
       "     (encoder_blocks): ModuleList(\n",
       "       (0): EncoderBlock(\n",
       "         (self_attention_layer): MultiHeadedSelfAttentionLayer(\n",
       "           (query_projections): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (key_projections): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (value_projections): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (attention_layer): ScaledDotProductAttentionLayer(\n",
       "             (dropout): Dropout(p=0.302424)\n",
       "           )\n",
       "           (layer_norm): LayerNorm()\n",
       "           (w_0): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (dropout): Dropout(p=0.302424)\n",
       "         )\n",
       "         (feed_forward_layer): PointWiseFCLayer(\n",
       "           (layer_norm): LayerNorm()\n",
       "           (w_1): Linear(in_features=300, out_features=405, bias=False)\n",
       "           (w_2): Linear(in_features=405, out_features=300, bias=False)\n",
       "           (dropout): Dropout(p=0.302424)\n",
       "         )\n",
       "         (layer_norm): LayerNorm()\n",
       "       )\n",
       "       (1): EncoderBlock(\n",
       "         (self_attention_layer): MultiHeadedSelfAttentionLayer(\n",
       "           (query_projections): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (key_projections): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (value_projections): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (attention_layer): ScaledDotProductAttentionLayer(\n",
       "             (dropout): Dropout(p=0.302424)\n",
       "           )\n",
       "           (layer_norm): LayerNorm()\n",
       "           (w_0): Linear(in_features=300, out_features=300, bias=False)\n",
       "           (dropout): Dropout(p=0.302424)\n",
       "         )\n",
       "         (feed_forward_layer): PointWiseFCLayer(\n",
       "           (layer_norm): LayerNorm()\n",
       "           (w_1): Linear(in_features=300, out_features=405, bias=False)\n",
       "           (w_2): Linear(in_features=405, out_features=300, bias=False)\n",
       "           (dropout): Dropout(p=0.302424)\n",
       "         )\n",
       "         (layer_norm): LayerNorm()\n",
       "       )\n",
       "     )\n",
       "     (layer_norm): LayerNorm()\n",
       "   )\n",
       "   (taggers): ModuleList(\n",
       "     (0): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (1): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (2): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (3): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (4): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (5): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (6): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (7): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (8): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (9): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (10): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (11): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (12): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (13): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (14): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (15): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (16): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (17): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (18): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "     (19): CommentWiseSumLogSoftmax(\n",
       "       (output_projection): Linear(in_features=300, out_features=4, bias=True)\n",
       "       (dropout): Dropout(p=0.79602089766246)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.81, 0.7173)\n",
       "     eps: 0.000814\n",
       "     lr: 0.0007678450184249178\n",
       "     weight_decay: 1e-05\n",
       " ),\n",
       " 'results': {'test': {'f1': 0.30326267795740247, 'loss': 13.447886896410159},\n",
       "  'train': {'f1': 0.43107196274346204, 'loss': 3.625991982489793},\n",
       "  'validation': {'f1': 0.3213599383691035, 'loss': 11.043194846103066}},\n",
       " 'status': 'ok',\n",
       " 'trainer': <trainer.train.Trainer at 0x1dab4d73470>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path = 'C:\\\\Users\\\\felix\\\\OneDrive\\\\Studium\\\\Studium\\\\6. Semester\\\\MA\\\\Project\\\\ABSA-Transformer\\\\logs\\\\test\\\\checkpoint_5684.data'\n",
    "checkpoint = torch.load(path)\n",
    "result['trainer'].optimizer.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "result['trainer'].model.load_state_dict(checkpoint['state_dict'])\n",
    "result['trainer'].early_stopping.best_model_checkpoint = checkpoint\n",
    "if torch.cuda.is_available():\n",
    "    for state in result['trainer'].optimizer.optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['optimizer'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['trainer'].train_iterator.train = True\n",
    "result['trainer'].evaluator.evaluate(result['trainer'].train_iterator, show_progress=True, progress_label=\"Evaluating TRAIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sommerhitze und keine klimaanlage  endstation für züge rheinpfalzde sommerhitze und keine klimaanlage  endstation für züge berlin dpa  am bislang heißesten tag des jahres haben die klimaanlagen mehrerer züge schlapp gemacht im fernverkehr seien rund ein dutzend alte ics betroffen gewesen sagte ein sprecher']\n",
      "Allgemein: n/a\n",
      "Atmosphäre: negative\n",
      "Auslastung_und_Platzangebot: n/a\n",
      "Barrierefreiheit: n/a\n",
      "Connectivity: n/a\n",
      "DB_App_und_Website: n/a\n",
      "Design: n/a\n",
      "Gastronomisches_Angebot: n/a\n",
      "Gepäck: n/a\n",
      "Image: n/a\n",
      "Informationen: n/a\n",
      "Komfort_und_Ausstattung: n/a\n",
      "QR-Code: n/a\n",
      "Reisen_mit_Kindern: n/a\n",
      "Service_und_Kundenbetreuung: n/a\n",
      "Sicherheit: n/a\n",
      "Sonstige_Unregelmässigkeiten: n/a\n",
      "Ticketkauf: n/a\n",
      "Toiletten: n/a\n",
      "Zugfahrt: n/a\n"
     ]
    }
   ],
   "source": [
    "from trainer.utils import *\n",
    "t = result['trainer']\n",
    "ds = t.dataset\n",
    "field = ds.fields['comments']\n",
    "t.train_iterator.init_epoch()\n",
    "for b in t.train_iterator:\n",
    "    x, _, padding, y = b.comments, b.general_sentiments, b.padding, b.aspect_sentiments\n",
    "    print(field.reverse(x[0].unsqueeze(0)))\n",
    "    \n",
    "    source_mask = create_padding_masks(padding, 1)\n",
    "    prediction = t.model.predict(x, source_mask)\n",
    "    aspect_sentiment = ds.fields['aspect_sentiments'].reverse(prediction, detokenize=False)\n",
    "    \n",
    "    for s, name in zip(aspect_sentiment[0], ds.target_names):\n",
    "        print(f'{name}: {s}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
