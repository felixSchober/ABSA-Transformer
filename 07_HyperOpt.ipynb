{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from data.data_loader import Dataset\n",
    "from data.germeval2017 import germeval2017_dataset\n",
    "from misc.preferences import PREFERENCES\n",
    "from misc.run_configuration import from_hyperopt, OutputLayerType, LearningSchedulerType, OptimizerType\n",
    "from misc import utils\n",
    "from misc.hyperopt_space import *\n",
    "\n",
    "from optimizer import get_optimizer\n",
    "from criterion import NllLoss, LossCombiner\n",
    "\n",
    "from models.transformer.encoder import TransformerEncoder\n",
    "from models.jointAspectTagger import JointAspectTagger\n",
    "from trainer.train import Trainer\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Functions\n",
    "\n",
    "These functions will load the dataset and the model. The run configuration will determine the architecture and hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(dataset, rc, experiment_name):\n",
    "    loss = LossCombiner(4, dataset.class_weights, NllLoss)\n",
    "    transformer = TransformerEncoder(dataset.source_embedding,\n",
    "                                     hyperparameters=rc)\n",
    "    model = JointAspectTagger(transformer, rc, 4, 20, dataset.target_names)\n",
    "    optimizer = get_optimizer(model, rc)\n",
    "    trainer = Trainer(\n",
    "                        model,\n",
    "                        loss,\n",
    "                        optimizer,\n",
    "                        rc,\n",
    "                        dataset,\n",
    "                        experiment_name,\n",
    "                        enable_tensorboard=False,\n",
    "                        verbose=False)\n",
    "    return trainer\n",
    "\n",
    "def load_dataset(rc, logger):\n",
    "    dataset = Dataset(\n",
    "        'germeval',\n",
    "        logger,\n",
    "        rc,\n",
    "        source_index=0,\n",
    "        target_vocab_index=2,\n",
    "        data_path=PREFERENCES.data_root,\n",
    "        train_file=PREFERENCES.data_train,\n",
    "        valid_file=PREFERENCES.data_validation,\n",
    "        test_file=PREFERENCES.data_test,\n",
    "        file_format='.tsv',\n",
    "        init_token=None,\n",
    "        eos_token=None\n",
    "    )\n",
    "    dataset.load_data(germeval2017_dataset, verbose=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble - Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFERENCES.defaults(\n",
    "    data_root='./data/germeval2017',\n",
    "    data_train='train_v1.4.tsv',    \n",
    "    data_validation='dev_v1.4.tsv',\n",
    "    data_test='test_TIMESTAMP1.tsv',\n",
    "    early_stopping='highest_5_F1'\n",
    ")\n",
    "experiment_name = 'HyperOpt'\n",
    "use_cuda = True\n",
    "\n",
    "# get general logger just for search\n",
    "experiment_name = utils.create_loggers(experiment_name=experiment_name)\n",
    "logger = logging.getLogger(__name__)\n",
    "dataset_logger = logging.getLogger('data_loader')\n",
    "logger.info('Run hyper parameter random grid search for experiment with name ' + experiment_name)\n",
    "\n",
    "num_optim_iterations = 60\n",
    "logger.info('num_optim_iterations: ' + str(num_optim_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_current_git_commit()\n",
    "logger.info('Current commit: ' + utils.get_current_git_commit())\n",
    "print('Current commit: ' + utils.get_current_git_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Spaces\n",
    "\n",
    "- BatchSize:\n",
    "    How big should each batch be?\n",
    "- Num Encoder Blocks\n",
    "    How many encoder blocks should be replicated?\n",
    "    AYNIA: 2-8\n",
    "    \n",
    "- Pointwise Layer Size\n",
    "    How big should the layer between attention heads be?\n",
    "    AYNIA: 1024 - 4096\n",
    "    This: 64 - 2048\n",
    "    \n",
    "    64: Prev. Experiments have shown that a smaller size can be beneficial because a smaller layer contains less parameters.\n",
    "    2048: This model has about a third of the AYNIA model size (1000 vs. 300). Going to big, therefore doesn't make much sense.\n",
    "\n",
    "- Clip Comments to \n",
    "    How long should comments be\n",
    "    This: 30 - 500\n",
    "    \n",
    "- Initial Learning Rate\n",
    "    What is the initial learning rate\n",
    "- Optimizer:\n",
    "    - Noam:\n",
    "        (FROM: https://github.com/tensorflow/tensor2tensor/issues/280#issuecomment-359477755)\n",
    "        decreasing the learning rate aka learning rate decay (usually exponential, piecewise-constant or inverse-time) is a standard practice in ML for decades. Increasing the learning rate in the early stages with a warmup (usually linear or exponential growth) is a more recent practice, popular esp. in deep learning on ImageNet, see e.g. He et al. 2016 or Goyal et al. 2017.\n",
    "        The \"noam\" scheme is just a particular way how to put the warmup and decay together (linear warmup for a given number of steps followed by exponential decay).\n",
    "\n",
    "        Learning rate schedules is an active research area. See e.g. papers on cyclical learning rate (corresponding to learning_rate_decay_scheme=cosine available in tensor2tensor) and super-convergence, which provide also more insights into the theory behind the learning rate, batch size, gradient noise etc.\n",
    "    \n",
    "        - learning rate factor\n",
    "        - learning rate warmup (steps)\n",
    "            AYNIA: 4000\n",
    "            THIS: 100 - 8000\n",
    "    - Adam:\n",
    "        - Beta 1\n",
    "            AYNIA: 0.9\n",
    "\n",
    "        - Beta 2\n",
    "            AYNIA: 0.98\n",
    "\n",
    "\n",
    "    - ?\n",
    "- Transformer Dropout Rate\n",
    "    Dropout rate for the transformer layers.\n",
    "    AYNIA: 0.1\n",
    "    THIS: 0.1 - 0.8\n",
    "- Number of Transformer Heads\n",
    "    How many attention heads should be used:\n",
    "    AYNIA: 8\n",
    "    THIS: [1, 2, 3, 4, 5, 6, 10, 12, 15, 20] (Have to be divide 300)\n",
    "    \n",
    "- Last Layer Dropout Rate\n",
    "    Dropout rate right before the last layer\n",
    "    AYNIA: -\n",
    "    This 0.0 - 0.8\n",
    "- Last Layer Types\n",
    "    - Sum\n",
    "    - Convolutions:\n",
    "        - num conv filters\n",
    "        - kernel size\n",
    "        - stride\n",
    "        - padding\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_space = hp\n",
    "search_space = {\n",
    "    'batch_size': hp.quniform('batch_size', 10, 100, 1),\n",
    "    'num_encoder_blocks': hp.quniform('num_encoder_blocks', 1, 8, 1),\n",
    "    'pointwise_layer_size': hp.quniform('pointwise_layer_size', 32, 256, 1),\n",
    "    'clip_comments_to': hp.quniform('clip_comments_to', 10, 250, 1),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.8),\n",
    "    'output_dropout_rate': hp.uniform('last_layer_dropout', 0.0, 0.8),\n",
    "    'num_heads': hp.choice('num_heads', [1, 2, 3, 4, 5]),\n",
    "    'transformer_use_bias': hp_bool('transformer_use_bias'),\n",
    "    'output_layer': hp.choice('output_layer', [\n",
    "        {\n",
    "            'type': OutputLayerType.Convolutions,\n",
    "            'output_conv_num_filters': hp.quniform('output_conv_num_filters', 1, 400, 1),\n",
    "            'output_conv_kernel_size': hp.quniform('output_conv_kernel_size', 1, 10, 1),\n",
    "            'output_conv_stride': hp.quniform('output_conv_stride', 1, 10, 1),\n",
    "            'output_conv_padding': hp.quniform('output_conv_padding', 0, 5, 1),\n",
    "        },\n",
    "        {\n",
    "            'type': OutputLayerType.LinearSum\n",
    "        }\n",
    "    ]),\n",
    "    'learning_rate_scheduler': hp.choice('learning_rate_scheduler', [\n",
    "        {\n",
    "            'type': LearningSchedulerType.Noam,\n",
    "            'noam_learning_rate_warmup': hp.quniform('noam_learning_rate_warmup', 1000, 9000, 1),\n",
    "            'noam_learning_rate_factor': hp.uniform('noam_learning_rate_factor', 0.01, 4)\n",
    "        }\n",
    "    ]),\n",
    "    'optimizer': hp.choice('optimizer', [\n",
    "        {\n",
    "            'type': OptimizerType.Adam,\n",
    "            'adam_beta1': hp.uniform('adam_beta1', 0.7, 0.999),\n",
    "            'adam_beta2': hp.uniform('adam_beta2', 0.7, 0.999),\n",
    "            'adam_eps': hp.loguniform('adam_eps', np.log(1e-10), np.log(1)),\n",
    "            'learning_rate': hp.lognormal('adam_learning_rate', np.log(0.01), np.log(10)),\n",
    "            'adam_weight_decay': 1*10**hp.quniform('adam_weight_decay', -8, -3, 1)\n",
    "        },\n",
    "        #{\n",
    "        #    'type': OptimizerType.SGD,\n",
    "        #    'sgd_momentum': hp.uniform('sgd_momentum', 0.4, 1),\n",
    "        #    'sgd_weight_decay': hp.loguniform('sgd_weight_decay', np.log(1e-4), np.log(1)),\n",
    "        #    'sgd_nesterov': hp_bool('sgd_nesterov'),\n",
    "        #    'learning_rate': hp.lognormal('sgd_learning_rate', np.log(0.01), np.log(10))\n",
    "    ]),\n",
    "    'replace_url_tokens': hp_bool('replace_url_tokens'),\n",
    "    'harmonize_bahn': hp_bool('harmonize_bahn'),\n",
    "    'embedding_type': hp.choice('embedding_type', ['fasttext', 'glove']),\n",
    "    'embedding_name': hp.choice('embedding_name', ['6B']),\n",
    "    'embedding_dim': hp.choice('embedding_dim', [300])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(parameters):\n",
    "    run_time = time.time()\n",
    "\n",
    "    # generate hp's from parameters\n",
    "    try:\n",
    "        rc = from_hyperopt(parameters, use_cuda, 300, 4, 35, -1, 'de')\n",
    "    except Exception as err:\n",
    "        print('Could not convert params: ' + str(err))\n",
    "        logger.exception(\"Could not load parameters from hyperopt configuration: \" + parameters)\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "    logger.info('New Params:')\n",
    "    logger.info(rc)\n",
    "    print('\\n\\n#########################################################################')\n",
    "    print(rc)\n",
    "\n",
    "    logger.debug('Load dataset')\n",
    "    try:\n",
    "        dataset = load_dataset(rc, dataset_logger)\n",
    "    except Exception as err:\n",
    "        print('Could load dataset: ' + str(err))\n",
    "        logger.exception(\"Could not load dataset\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "    logger.debug('dataset loaded')\n",
    "    logger.debug('Load model')\n",
    "\n",
    "    try:\n",
    "        trainer = load_model(dataset, rc, experiment_name)\n",
    "    except Exception as err:\n",
    "        print('Could load model: ' + str(err))\n",
    "        logger.exception(\"Could not load model\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "\n",
    "    logger.debug('model loaded')\n",
    "\n",
    "    logger.debug('Begin training')\n",
    "    model = None\n",
    "    try:\n",
    "        result = trainer.train(use_cuda=rc.use_cuda, perform_evaluation=False)\n",
    "        model = result['model']\n",
    "    except Exception as err:\n",
    "        print('Exception while training: ' + str(err))\n",
    "        logger.exception(\"Could not complete iteration\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "\n",
    "    if math.isnan(trainer.get_best_loss()):\n",
    "        print('Loss is nan')\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "\n",
    "    # perform evaluation and log results\n",
    "    result = None\n",
    "    try:\n",
    "        result = trainer.perform_final_evaluation(use_test_set=True, verbose=False)\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not complete iteration evaluation.\")\n",
    "        print('Could not complete iteration evaluation: ' + str(err))\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "    print(f'VAL f1\\t{trainer.get_best_f1()} - ({result[1][1]})')\n",
    "    print(f'VAL loss\\t{trainer.get_best_loss()}')\n",
    "    return {\n",
    "            'loss': result[1][0],\n",
    "            'status': STATUS_OK,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1(),\n",
    "            'sample_iterations': trainer.get_num_samples_seen(),\n",
    "            'iterations': trainer.get_num_iterations(),\n",
    "            'rc': rc,\n",
    "            'results': {\n",
    "                'train': {\n",
    "                    'loss': result[0][0],\n",
    "                    'f1': result[0][1]\n",
    "                },\n",
    "                'validation': {\n",
    "                    'loss': result[1][0],\n",
    "                    'f1': result[1][1]\n",
    "                },\n",
    "                'test': {\n",
    "                    'loss': result[2][0],\n",
    "                    'f1': result[2][1]\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_objective(params):\n",
    "    rc = from_hyperopt(params, use_cuda, 300, 4, 35, -1, 'de')\n",
    "    print(rc)\n",
    "\n",
    "    return {\n",
    "        'loss': params['x'] ** 2,\n",
    "        'status': STATUS_OK\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best = fmin(objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=num_optim_iterations,\n",
    "    trials=trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
