{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from data.data_loader import Dataset\n",
    "from data.germeval2017 import germeval2017_dataset\n",
    "from misc.preferences import PREFERENCES\n",
    "from misc.run_configuration import from_hyperopt, OutputLayerType, LearningSchedulerType, OptimizerType\n",
    "from misc import utils\n",
    "from misc.hyperopt_space import *\n",
    "\n",
    "from optimizer import get_optimizer\n",
    "from criterion import NllLoss, LossCombiner\n",
    "\n",
    "from models.transformer.encoder import TransformerEncoder\n",
    "from models.jointAspectTagger import JointAspectTagger\n",
    "from trainer.train import Trainer\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Functions\n",
    "\n",
    "These functions will load the dataset and the model. The run configuration will determine the architecture and hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(dataset, rc, experiment_name):\n",
    "    loss = LossCombiner(4, dataset.class_weights, NllLoss)\n",
    "    transformer = TransformerEncoder(dataset.source_embedding,\n",
    "                                     hyperparameters=rc)\n",
    "    model = JointAspectTagger(transformer, rc, 4, 20, dataset.target_names)\n",
    "    optimizer = get_optimizer(model, rc)\n",
    "    trainer = Trainer(\n",
    "                        model,\n",
    "                        loss,\n",
    "                        optimizer,\n",
    "                        rc,\n",
    "                        dataset,\n",
    "                        experiment_name,\n",
    "                        enable_tensorboard=False,\n",
    "                        verbose=False)\n",
    "    return trainer\n",
    "\n",
    "def load_dataset(rc, logger):\n",
    "    dataset = Dataset(\n",
    "        'germeval',\n",
    "        logger,\n",
    "        rc,\n",
    "        source_index=0,\n",
    "        target_vocab_index=2,\n",
    "        data_path=PREFERENCES.data_root,\n",
    "        train_file=PREFERENCES.data_train,\n",
    "        valid_file=PREFERENCES.data_validation,\n",
    "        test_file=PREFERENCES.data_test,\n",
    "        file_format='.tsv',\n",
    "        init_token=None,\n",
    "        eos_token=None\n",
    "    )\n",
    "    dataset.load_data(germeval2017_dataset, verbose=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble - Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\HyperParameterTest\\20190310\\0\n"
     ]
    }
   ],
   "source": [
    "PREFERENCES.defaults(\n",
    "    data_root='./data/germeval2017',\n",
    "    data_train='train_v1.4.tsv',    \n",
    "    data_validation='dev_v1.4.tsv',\n",
    "    data_test='test_TIMESTAMP1.tsv',\n",
    "    early_stopping='highest_5_F1'\n",
    ")\n",
    "experiment_name = 'HyperParameterTest'\n",
    "use_cuda = True\n",
    "\n",
    "# get general logger just for search\n",
    "experiment_name = utils.create_loggers(experiment_name=experiment_name)\n",
    "logger = logging.getLogger(__name__)\n",
    "dataset_logger = logging.getLogger('data_loader')\n",
    "logger.info('Run hyper parameter random grid search for experiment with name ' + experiment_name)\n",
    "\n",
    "num_optim_iterations = 100\n",
    "logger.info('num_optim_iterations: ' + str(num_optim_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current commit: b'3533092'\n"
     ]
    }
   ],
   "source": [
    "utils.get_current_git_commit()\n",
    "logger.info('Current commit: ' + utils.get_current_git_commit())\n",
    "print('Current commit: ' + utils.get_current_git_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Spaces\n",
    "\n",
    "- BatchSize:\n",
    "    How big should each batch be?\n",
    "- Num Encoder Blocks\n",
    "    How many encoder blocks should be replicated?\n",
    "    AYNIA: 2-8\n",
    "    \n",
    "- Pointwise Layer Size\n",
    "    How big should the layer between attention heads be?\n",
    "    AYNIA: 1024 - 4096\n",
    "    This: 64 - 2048\n",
    "    \n",
    "    64: Prev. Experiments have shown that a smaller size can be beneficial because a smaller layer contains less parameters.\n",
    "    2048: This model has about a third of the AYNIA model size (1000 vs. 300). Going to big, therefore doesn't make much sense.\n",
    "\n",
    "- Clip Comments to \n",
    "    How long should comments be\n",
    "    This: 30 - 500\n",
    "    \n",
    "- Initial Learning Rate\n",
    "    What is the initial learning rate\n",
    "- Optimizer:\n",
    "    - Noam:\n",
    "        (FROM: https://github.com/tensorflow/tensor2tensor/issues/280#issuecomment-359477755)\n",
    "        decreasing the learning rate aka learning rate decay (usually exponential, piecewise-constant or inverse-time) is a standard practice in ML for decades. Increasing the learning rate in the early stages with a warmup (usually linear or exponential growth) is a more recent practice, popular esp. in deep learning on ImageNet, see e.g. He et al. 2016 or Goyal et al. 2017.\n",
    "        The \"noam\" scheme is just a particular way how to put the warmup and decay together (linear warmup for a given number of steps followed by exponential decay).\n",
    "\n",
    "        Learning rate schedules is an active research area. See e.g. papers on cyclical learning rate (corresponding to learning_rate_decay_scheme=cosine available in tensor2tensor) and super-convergence, which provide also more insights into the theory behind the learning rate, batch size, gradient noise etc.\n",
    "    \n",
    "        - learning rate factor\n",
    "        - learning rate warmup (steps)\n",
    "            AYNIA: 4000\n",
    "            THIS: 100 - 8000\n",
    "    - Adam:\n",
    "        - Beta 1\n",
    "            AYNIA: 0.9\n",
    "\n",
    "        - Beta 2\n",
    "            AYNIA: 0.98\n",
    "\n",
    "\n",
    "    - ?\n",
    "- Transformer Dropout Rate\n",
    "    Dropout rate for the transformer layers.\n",
    "    AYNIA: 0.1\n",
    "    THIS: 0.1 - 0.8\n",
    "- Number of Transformer Heads\n",
    "    How many attention heads should be used:\n",
    "    AYNIA: 8\n",
    "    THIS: [1, 2, 3, 4, 5, 6, 10, 12, 15, 20] (Have to be divide 300)\n",
    "    \n",
    "- Last Layer Dropout Rate\n",
    "    Dropout rate right before the last layer\n",
    "    AYNIA: -\n",
    "    This 0.0 - 0.8\n",
    "- Last Layer Types\n",
    "    - Sum\n",
    "    - Convolutions:\n",
    "        - num conv filters\n",
    "        - kernel size\n",
    "        - stride\n",
    "        - padding\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#search_space = hp\n",
    "search_space = {\n",
    "    'batch_size': hp.quniform('batch_size', 1, 500, 1),\n",
    "    'num_encoder_blocks': hp.quniform('num_encoder_blocks', 1, 8, 1),\n",
    "    'pointwise_layer_size': hp.quniform('pointwise_layer_size', 64, 2048, 1),\n",
    "    'clip_comments_to': hp.quniform('clip_comments_to', 10, 400, 1),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.8),\n",
    "    'output_dropout_rate': hp.uniform('last_layer_dropout', 0.0, 0.8),\n",
    "    'num_heads': hp.choice('num_heads', [1, 2, 3, 4, 5, 6, 10, 12, 15, 20]),\n",
    "    'output_layer': hp.choice('output_layer', [\n",
    "        {\n",
    "            'type': OutputLayerType.Convolutions,\n",
    "            'output_conv_num_filters': hp.quniform('output_conv_num_filters', 1, 400, 1),\n",
    "            'output_conv_kernel_size': hp.quniform('output_conv_kernel_size', 1, 10, 1),\n",
    "            'output_conv_stride': hp.quniform('output_conv_stride', 1, 10, 1),\n",
    "            'output_conv_padding': hp.quniform('output_conv_padding', 0, 5, 1),\n",
    "        },\n",
    "        {\n",
    "            'type': OutputLayerType.LinearSum\n",
    "        }\n",
    "    ]),\n",
    "    'learning_rate_scheduler': hp.choice('learning_rate_scheduler', [\n",
    "        {\n",
    "            'type': LearningSchedulerType.Noam,\n",
    "            'noam_learning_rate_warmup': hp.quniform('noam_learning_rate_warmup', 1000, 9000, 1),\n",
    "            'noam_learning_rate_factor': hp.uniform('noam_learning_rate_factor', 0.01, 4)\n",
    "        }\n",
    "    ]),\n",
    "    'optimizer': hp.choice('optimizer', [\n",
    "        {\n",
    "            'type': OptimizerType.Adam,\n",
    "            'adam_beta1': hp.uniform('adam_beta1', 0.7, 0.999),\n",
    "            'adam_beta2': hp.uniform('adam_beta2', 0.7, 0.999),\n",
    "            'adam_eps': hp.loguniform('adam_eps', np.log(1e-7), np.log(1)),\n",
    "            'learning_rate': hp.lognormal('adam_learning_rate', np.log(0.01), np.log(10))\n",
    "        },\n",
    "        #{\n",
    "        #    'type': OptimizerType.SGD,\n",
    "        #    'sgd_momentum': hp.uniform('sgd_momentum', 0.4, 1),\n",
    "        #    'sgd_weight_decay': hp.loguniform('sgd_weight_decay', np.log(1e-4), np.log(1)),\n",
    "        #    'sgd_nesterov': hp_bool('sgd_nesterov'),\n",
    "        #    'learning_rate': hp.lognormal('sgd_learning_rate', np.log(0.01), np.log(10))\n",
    "    ]),\n",
    "    'replace_url_tokens': hp_bool('replace_url_tokens'),\n",
    "    'harmonize_bahn': hp_bool('harmonize_bahn'),\n",
    "    'embedding_type': hp.choice('embedding_type', ['fasttext', 'glove']),\n",
    "    'embedding_name': hp.choice('embedding_name', ['6B']),\n",
    "    'embedding_dim': hp.choice('embedding_dim', [300])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(parameters):\n",
    "    run_time = time.time()\n",
    "\n",
    "    # generate hp's from parameters\n",
    "    try:\n",
    "        rc = from_hyperopt(parameters, use_cuda, 300, 4, 35, -1, 'de')\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not load parameters from hyperopt configuration: \" + parameters)\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "    logger.info('New Params:')\n",
    "    logger.info(rc)\n",
    "    print(rc)\n",
    "\n",
    "    logger.debug('Load dataset')\n",
    "    try:\n",
    "        dataset = load_dataset(rc, dataset_logger)\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not load dataset\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "    logger.debug('dataset loaded')\n",
    "    logger.debug('Load model')\n",
    "\n",
    "    try:\n",
    "        trainer = load_model(dataset, rc, experiment_name)\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not load model\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "\n",
    "    logger.debug('model loaded')\n",
    "\n",
    "    logger.debug('Begin training')\n",
    "    model = None\n",
    "    try:\n",
    "        result = trainer.train(use_cuda=rc.use_cuda, perform_evaluation=False)\n",
    "        model = result['model']\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not complete iteration\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "\n",
    "    if math.isnan(trainer.get_best_loss()):\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "\n",
    "    # perform evaluation and log results\n",
    "    result = None\n",
    "    try:\n",
    "        result = trainer.perform_final_evaluation(use_test_set=True, verbose=False)\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not complete iteration evaluation.\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "    return {\n",
    "            'loss': result[1][0],\n",
    "            'status': STATUS_OK,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1(),\n",
    "            'results': {\n",
    "                'train': {\n",
    "                    'loss': result[0][0],\n",
    "                    'f1': result[0][1]\n",
    "                },\n",
    "                'validation': {\n",
    "                    'loss': result[1][0],\n",
    "                    'f1': result[1][1]\n",
    "                },\n",
    "                'test': {\n",
    "                    'loss': result[2][0],\n",
    "                    'f1': result[2][1]\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_objective(params):\n",
    "    rc = from_hyperopt(params, use_cuda, 300, 4, 35, -1, 'de')\n",
    "    print(rc)\n",
    "\n",
    "    return {\n",
    "        'loss': params['x'] ** 2,\n",
    "        'status': STATUS_OK\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+                                   \n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'batch_size': 201.0, 'clip_comments_to'[...]rue} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         4                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|          batch_size          |                        201                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        embedding_type        |                      fasttext                     |\n",
      "|        learning_rate         |                 3.506035033811867                 |\n",
      "|  noam_learning_rate_warmup   |                        7112                       |\n",
      "|  noam_learning_rate_factor   |                0.48486998145821775                |\n",
      "|          adam_beta1          |                 0.8338131444998067                |\n",
      "|          adam_beta2          |                 0.7281566704591333                |\n",
      "|           adam_eps           |               0.0007865819535684946               |\n",
      "|      adam_weight_decay       |                         0                         |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|         n_enc_blocks         |                         4                         |\n",
      "|           n_heads            |                         2                         |\n",
      "|             d_k              |                        150                        |\n",
      "|             d_v              |                        150                        |\n",
      "|         dropout_rate         |                 0.5208324211675149                |\n",
      "|     pointwise_layer_size     |                        1873                       |\n",
      "|      last_layer_dropout      |                 0.7783489827574382                |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        309                        |\n",
      "|           language           |                         de                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                        True                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "  0%|                                                                            | 0/100 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c971d69d8e4a6e83ff3d8d87d7761a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+                                   \n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'batch_size': 64.0, 'clip_comments_to':[...]lse} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         4                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|          batch_size          |                         64                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |            OutputLayerType.Convolutions           |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        embedding_type        |                      fasttext                     |\n",
      "|        learning_rate         |               0.0010556149539180078               |\n",
      "|  noam_learning_rate_warmup   |                        6474                       |\n",
      "|  noam_learning_rate_factor   |                 1.5181150216267567                |\n",
      "|          adam_beta1          |                 0.8299740415822767                |\n",
      "|          adam_beta2          |                  0.71440704265272                 |\n",
      "|           adam_eps           |               1.1744327683357653e-07              |\n",
      "|      adam_weight_decay       |                         0                         |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|         n_enc_blocks         |                         5                         |\n",
      "|           n_heads            |                         1                         |\n",
      "|             d_k              |                        300                        |\n",
      "|             d_v              |                        300                        |\n",
      "|         dropout_rate         |                0.45138305955177405                |\n",
      "|     pointwise_layer_size     |                        206                        |\n",
      "|      last_layer_dropout      |                 0.6900694651835217                |\n",
      "|   output_conv_num_filters    |                        266                        |\n",
      "|   output_conv_kernel_size    |                         7                         |\n",
      "|      output_conv_stride      |                         9                         |\n",
      "|     output_conv_padding      |                         4                         |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        217                        |\n",
      "|           language           |                         de                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                        True                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "  1%|▋                                                                   | 1/100 [00:30<49:46, 30.17s/it, best loss: ?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d49749feb4449da49139e21a680db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+                                   \n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'batch_size': 192.0, 'clip_comments_to'[...]lse} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         4                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|          batch_size          |                        192                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        learning_rate         |                0.01630144121842768                |\n",
      "|  noam_learning_rate_warmup   |                        2590                       |\n",
      "|  noam_learning_rate_factor   |                 2.4013565214312598                |\n",
      "|          adam_beta1          |                 0.8311317730374053                |\n",
      "|          adam_beta2          |                 0.7501664892353157                |\n",
      "|           adam_eps           |               2.953319459783717e-07               |\n",
      "|      adam_weight_decay       |                         0                         |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|         n_enc_blocks         |                         1                         |\n",
      "|           n_heads            |                         4                         |\n",
      "|             d_k              |                         75                        |\n",
      "|             d_v              |                         75                        |\n",
      "|         dropout_rate         |                0.22390089148377817                |\n",
      "|     pointwise_layer_size     |                         69                        |\n",
      "|      last_layer_dropout      |                0.34479660121732136                |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        248                        |\n",
      "|           language           |                         de                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "  2%|█▎                                                                  | 2/100 [01:04<51:19, 31.43s/it, best loss: ?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28e37d73ac74128beadc8b17dfdcb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time                                                             \n",
      "1\t89\t2834.88\t\t                                                                                                         475.13\t\t0.199\t\t0.715\t\t0.59m - 0.6m / 0.0m\n",
      "2\t178\t343.86\t\t                                                                                                         355.47\t\t0.252\t\t0.876\t\t0.60m - 1.2m / 20.8m\n",
      "3\t267\t338.45\t\t                                                                                                         368.79\t\t0.231\t\t0.740\t\t0.60m - 1.8m / 20.9m\n",
      "4\t356\t249.63\t\t                                                                                                         581.14\t\t0.298\t\t0.911\t\t0.60m - 2.4m / 20.9m\n",
      "  2%|█▎                                                                  | 2/100 [04:00<51:19, 31.43s/it, best loss: ?]"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best = fmin(objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
