{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\misc\\run_configuration.py:390: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(config.model_size % config.n_heads == 0, f'number of heads {config.n_heads} is not a valid number of heads for model size {config.model_size}.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from data.data_loader import Dataset\n",
    "from misc.preferences import PREFERENCES\n",
    "from misc.run_configuration import get_default_params, OutputLayerType, LearningSchedulerType, OptimizerType, good_organic_hp_params\n",
    "from misc import utils\n",
    "\n",
    "from optimizer import get_optimizer\n",
    "from criterion import NllLoss, LossCombiner\n",
    "\n",
    "from models.transformer.encoder import TransformerEncoder\n",
    "from models.jointAspectTagger import JointAspectTagger\n",
    "from trainer.train import Trainer\n",
    "import pprint\n",
    "from data.organic2019 import organic_dataset as dsl\n",
    "from data.organic2019 import ORGANIC_TASK_ALL, ORGANIC_TASK_ENTITIES, ORGANIC_TASK_ATTRIBUTES, ORGANIC_TASK_ENTITIES_COMBINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Functions\n",
    "\n",
    "These functions will load the dataset and the model. The run configuration will determine the architecture and hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(dataset, rc, experiment_name):\n",
    "    loss = LossCombiner(4, dataset.class_weights, NllLoss)\n",
    "    transformer = TransformerEncoder(dataset.source_embedding,\n",
    "                                     hyperparameters=rc)\n",
    "    model = JointAspectTagger(transformer, rc, 4, 20, dataset.target_names)\n",
    "    optimizer = get_optimizer(model, rc)\n",
    "    trainer = Trainer(\n",
    "                        model,\n",
    "                        loss,\n",
    "                        optimizer,\n",
    "                        rc,\n",
    "                        dataset,\n",
    "                        experiment_name,\n",
    "                        enable_tensorboard=False,\n",
    "                        verbose=False)\n",
    "    return trainer\n",
    "\n",
    "def load_dataset(rc, logger, task):\n",
    "    dataset = Dataset(\n",
    "        task,\n",
    "        logger,\n",
    "        rc,\n",
    "        source_index=0,\n",
    "        target_vocab_index=1,\n",
    "        data_path=PREFERENCES.data_root,\n",
    "        train_file=PREFERENCES.data_train,\n",
    "        valid_file=PREFERENCES.data_validation,\n",
    "        test_file=PREFERENCES.data_test,\n",
    "        file_format='.tsv',\n",
    "        init_token=None,\n",
    "        eos_token=None\n",
    "    )\n",
    "    dataset.load_data(dsl, verbose=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble - Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREFERENCES.defaults(\n",
    "    data_root='./data/data/organic2019',\n",
    "    data_train='train.csv',    \n",
    "    data_validation='validation.csv',\n",
    "    data_test='test.csv',\n",
    "    early_stopping='highest_5_F1'\n",
    ")\n",
    "main_experiment_name = 'Organic19_Experiments'\n",
    "use_cuda = True\n",
    "STATUS_FAIL = 'fail'\n",
    "STATUS_OK = 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Definition of experiments\n",
    " \n",
    " - SpellChecker On\n",
    " - Fasttext\n",
    " - Single Sentence\n",
    " - Combined Sentence\n",
    " - Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'att_d_k': 100,\n",
      "  'att_d_v': 100,\n",
      "  'batch_size': 20,\n",
      "  'clip_comments_to': 195,\n",
      "  'dropout_rate': 0.392996573831,\n",
      "  'early_stopping': 5,\n",
      "  'embedding_dim': 300,\n",
      "  'embedding_name': '6B',\n",
      "  'embedding_type': 'glove',\n",
      "  'language': 'en',\n",
      "  'learning_rate_scheduler': { 'noam_learning_rate_factor': 3.3368149482,\n",
      "                               'noam_learning_rate_warmup': 4631},\n",
      "  'learning_rate_scheduler_type': <LearningSchedulerType.Noam: 1>,\n",
      "  'log_every_xth_iteration': -1,\n",
      "  'model_size': 300,\n",
      "  'num_encoder_blocks': 2,\n",
      "  'num_epochs': 35,\n",
      "  'num_heads': 3,\n",
      "  'optimizer': { 'adam_beta1': 0.89178641984,\n",
      "                 'adam_beta2': 0.83491754824,\n",
      "                 'adam_eps': 8.734158747166484e-09,\n",
      "                 'adam_weight_decay': 1e-08,\n",
      "                 'learning_rate': 0.001},\n",
      "  'optimizer_type': <OptimizerType.Adam: 1>,\n",
      "  'output_dropout_rate': 0.7608194889605,\n",
      "  'output_layer_type': <OutputLayerType.LinearSum: 1>,\n",
      "  'pointwise_layer_size': 195,\n",
      "  'task': 'entities',\n",
      "  'use_spell_checkers': False}\n"
     ]
    }
   ],
   "source": [
    "baseline = good_organic_hp_params\n",
    "print(pprint.pformat(baseline, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\n",
    "        'name': 'Baseline',\n",
    "        'description': 'Baseline. Uses good_organic_hp_params without any changes',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'SpellChecker On',\n",
    "        'description': 'Uses the baseline parameters but with the spellchecker enabled',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'use_spell_checkers': True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Fasttext',\n",
    "        'description': 'Uses english Fasttext embeddings',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'embedding_type': 'fasttext'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Rolling Sentences',\n",
    "        'description': 'Uses a combination of the last and the current sentence instead of just the sentence alone',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'task': 'entities_combine'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Stop Words',\n",
    "        'description': 'Uses stop words removal',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'use_stop_words': True\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current commit: b'8fa12da'\n"
     ]
    }
   ],
   "source": [
    "utils.get_current_git_commit()\n",
    "print('Current commit: ' + utils.get_current_git_commit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(rc, experiment):\n",
    "    run_time = time.time()\n",
    "    \n",
    "    # reset loggers\n",
    "    utils.reset_loggers()\n",
    "    experiment_name = utils.create_loggers(experiment_name=main_experiment_name)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    dataset_logger = logging.getLogger('data_loader')\n",
    "    \n",
    "    logger.info('Experiment: ' + experiment['name'])\n",
    "    logger.info('Description: ' + experiment['description'])\n",
    "    \n",
    "    logger.info('Parameters')\n",
    "    logger.info(rc)\n",
    "    print('\\n\\n#########################################################################')\n",
    "    print('Name: ' + experiment['name'])\n",
    "    print('Description: ' + experiment['description'])\n",
    "    print('#########################################################################\\n\\n')\n",
    "    print(rc)\n",
    "\n",
    "    logger.debug('Load dataset')\n",
    "    try:\n",
    "        dataset = load_dataset(rc, dataset_logger, rc.task)\n",
    "    except Exception as err:\n",
    "        print('Could load dataset: ' + str(err))\n",
    "        logger.exception(\"Could not load dataset\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "    logger.debug('dataset loaded')\n",
    "    logger.debug('Load model')\n",
    "\n",
    "    try:\n",
    "        trainer = load_model(dataset, rc, experiment_name)\n",
    "    except Exception as err:\n",
    "        print('Could not load model: ' + str(err))\n",
    "        logger.exception(\"Could not load model\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "\n",
    "    logger.debug('model loaded')\n",
    "\n",
    "    logger.debug('Begin training')\n",
    "    model = None\n",
    "    try:\n",
    "        result = trainer.train(use_cuda=rc.use_cuda, perform_evaluation=False)\n",
    "        model = result['model']\n",
    "    except Exception as err:\n",
    "        print('Exception while training: ' + str(err))\n",
    "        logger.exception(\"Could not complete iteration\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "\n",
    "    if math.isnan(trainer.get_best_loss()):\n",
    "        print('Loss is nan')\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "\n",
    "    # perform evaluation and log results\n",
    "    result = None\n",
    "    try:\n",
    "        result = trainer.perform_final_evaluation(use_test_set=True, verbose=False)\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not complete iteration evaluation.\")\n",
    "        print('Could not complete iteration evaluation: ' + str(err))\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "    print(f'VAL f1\\t{trainer.get_best_f1()} - ({result[1][1]})')\n",
    "    print(f'VAL loss\\t{trainer.get_best_loss()}')\n",
    "    return {\n",
    "            'loss': result[1][0],\n",
    "            'status': STATUS_OK,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1(),\n",
    "            'sample_iterations': trainer.get_num_samples_seen(),\n",
    "            'iterations': trainer.get_num_iterations(),\n",
    "            'rc': rc,\n",
    "            'results': {\n",
    "                'train': {\n",
    "                    'loss': result[0][0],\n",
    "                    'f1': result[0][1]\n",
    "                },\n",
    "                'validation': {\n",
    "                    'loss': result[1][0],\n",
    "                    'f1': result[1][1]\n",
    "                },\n",
    "                'test': {\n",
    "                    'loss': result[2][0],\n",
    "                    'f1': result[2][1]\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "\n",
      "Experiment Name: Baseline\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190330\\0\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: Baseline\n",
      "Description: Baseline. Uses good_organic_hp_params without any changes\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'glove', 'learning_ra[...]es'} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                      entities                     |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26cbed0ddb345cabca4ae960a4a949b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.48\t\t0.40\t\t0.000\t\t0.885\t\t0.45m - 0.4m / 0.0m\n",
      "2\t9k\t0.46\t\t0.40\t\t0.026\t\t0.800\t\t0.42m - 0.9m / 15.7m\n",
      "3\t13k\t0.42\t\t0.38\t\t0.217\t\t0.813\t\t0.42m - 1.4m / 14.9m\n",
      "4\t17k\t0.37\t\t0.44\t\t0.206\t\t0.776\t\t0.42m - 1.8m / 14.9m\n",
      "5\t22k\t0.31\t\t0.39\t\t0.228\t\t0.801\t\t0.42m - 2.3m / 14.9m\n",
      "6\t26k\t0.26\t\t0.48\t\t0.240\t\t0.770\t\t0.42m - 2.7m / 14.9m\n",
      "7\t30k\t0.23\t\t0.51\t\t0.241\t\t0.787\t\t0.42m - 3.2m / 14.9m\n",
      "8\t35k\t0.21\t\t0.54\t\t0.198\t\t0.737\t\t0.41m - 3.6m / 15.0m\n",
      "9\t39k\t0.18\t\t0.58\t\t0.235\t\t0.791\t\t0.42m - 4.1m / 14.9m\n",
      "10\t43k\t0.16\t\t0.64\t\t0.204\t\t0.773\t\t0.42m - 4.5m / 15.0m\n",
      "11\t48k\t0.15\t\t0.64\t\t0.250\t\t0.816\t\t0.42m - 5.0m / 15.0m\n",
      "12\t52k\t0.14\t\t0.65\t\t0.231\t\t0.798\t\t0.42m - 5.4m / 15.0m\n",
      "13\t56k\t0.14\t\t0.67\t\t0.244\t\t0.793\t\t0.42m - 5.9m / 15.1m\n",
      "14\t60k\t0.13\t\t0.70\t\t0.268\t\t0.828\t\t0.42m - 6.4m / 15.1m\n",
      "15\t65k\t0.13\t\t0.67\t\t0.241\t\t0.810\t\t0.42m - 6.8m / 15.2m\n",
      "16\t69k\t0.15\t\t0.62\t\t0.241\t\t0.807\t\t0.43m - 7.3m / 15.2m\n",
      "17\t73k\t0.14\t\t0.69\t\t0.235\t\t0.791\t\t0.42m - 7.7m / 15.4m\n",
      "18\t78k\t0.14\t\t0.74\t\t0.243\t\t0.811\t\t0.42m - 8.2m / 15.3m\n",
      "19\t82k\t0.15\t\t0.66\t\t0.255\t\t0.794\t\t0.42m - 8.6m / 15.3m\n",
      "VAL f1\t0.2679425837320574 - (0.2679425837320574)\n",
      "VAL loss\t0.37701771960538977\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: Baseline\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.37701771960538977\n",
      "        | \\     )|_\tf1: 0.2679425837320574\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "Experiment Name: SpellChecker On\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190330\\1\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: SpellChecker On\n",
      "Description: Uses the baseline parameters but with the spellchecker enabled\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'glove', 'learning_ra[...]es'} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                      entities                     |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                        True                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0106146e07964f57b97cc3be3ebc85cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.49\t\t0.40\t\t0.000\t\t0.885\t\t0.41m - 0.4m / 0.0m\n",
      "2\t9k\t0.46\t\t0.39\t\t0.006\t\t0.886\t\t0.41m - 0.9m / 14.3m\n",
      "3\t13k\t0.43\t\t0.37\t\t0.233\t\t0.822\t\t0.42m - 1.3m / 14.6m\n",
      "4\t17k\t0.36\t\t0.41\t\t0.214\t\t0.764\t\t0.42m - 1.8m / 14.8m\n",
      "5\t22k\t0.30\t\t0.40\t\t0.240\t\t0.772\t\t0.42m - 2.2m / 14.7m\n",
      "6\t26k\t0.25\t\t0.50\t\t0.215\t\t0.744\t\t0.42m - 2.7m / 14.8m\n",
      "7\t30k\t0.22\t\t0.49\t\t0.260\t\t0.803\t\t0.42m - 3.1m / 14.8m\n",
      "8\t35k\t0.20\t\t0.57\t\t0.226\t\t0.774\t\t0.42m - 3.6m / 14.9m\n",
      "9\t39k\t0.17\t\t0.62\t\t0.244\t\t0.807\t\t0.42m - 4.0m / 15.0m\n",
      "10\t43k\t0.16\t\t0.56\t\t0.229\t\t0.793\t\t0.42m - 4.5m / 15.0m\n",
      "11\t48k\t0.16\t\t0.62\t\t0.235\t\t0.807\t\t0.41m - 4.9m / 14.9m\n",
      "12\t52k\t0.14\t\t0.59\t\t0.230\t\t0.809\t\t0.41m - 5.4m / 14.9m\n",
      "VAL f1\t0.2602308499475341 - (0.2602308499475341)\n",
      "VAL loss\t0.37236622712191414\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: SpellChecker On\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.37236622712191414\n",
      "        | \\     )|_\tf1: 0.2602308499475341\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "Experiment Name: Fasttext\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190330\\2\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: Fasttext\n",
      "Description: Uses english Fasttext embeddings\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'fasttext', 'learning[...]es'} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                      entities                     |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                      fasttext                     |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f284fb4ce0545bda606819e58e7811c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.48\t\t0.40\t\t0.000\t\t0.885\t\t0.41m - 0.4m / 0.0m\n",
      "2\t9k\t0.46\t\t0.40\t\t0.026\t\t0.800\t\t0.42m - 0.9m / 14.3m\n",
      "3\t13k\t0.42\t\t0.38\t\t0.217\t\t0.813\t\t0.41m - 1.3m / 14.7m\n",
      "4\t17k\t0.37\t\t0.44\t\t0.206\t\t0.776\t\t0.42m - 1.8m / 14.6m\n",
      "5\t22k\t0.31\t\t0.39\t\t0.228\t\t0.801\t\t0.41m - 2.2m / 14.7m\n",
      "6\t26k\t0.26\t\t0.48\t\t0.240\t\t0.770\t\t0.42m - 2.7m / 14.6m\n",
      "7\t30k\t0.23\t\t0.51\t\t0.241\t\t0.787\t\t0.41m - 3.1m / 14.8m\n",
      "8\t35k\t0.21\t\t0.54\t\t0.198\t\t0.737\t\t0.41m - 3.6m / 14.8m\n",
      "9\t39k\t0.18\t\t0.58\t\t0.235\t\t0.791\t\t0.42m - 4.0m / 14.8m\n",
      "10\t43k\t0.16\t\t0.64\t\t0.204\t\t0.773\t\t0.41m - 4.5m / 14.9m\n",
      "11\t48k\t0.15\t\t0.64\t\t0.250\t\t0.816\t\t0.42m - 4.9m / 14.9m\n",
      "12\t52k\t0.14\t\t0.65\t\t0.231\t\t0.798\t\t0.41m - 5.4m / 15.0m\n",
      "13\t56k\t0.14\t\t0.67\t\t0.244\t\t0.793\t\t0.42m - 5.8m / 14.9m\n",
      "14\t60k\t0.13\t\t0.70\t\t0.268\t\t0.828\t\t0.42m - 6.3m / 15.0m\n",
      "15\t65k\t0.13\t\t0.67\t\t0.241\t\t0.810\t\t0.41m - 6.7m / 15.1m\n",
      "16\t69k\t0.15\t\t0.62\t\t0.241\t\t0.807\t\t0.42m - 7.2m / 15.1m\n",
      "17\t73k\t0.14\t\t0.69\t\t0.235\t\t0.791\t\t0.42m - 7.6m / 15.1m\n",
      "18\t78k\t0.14\t\t0.74\t\t0.243\t\t0.811\t\t0.42m - 8.1m / 15.2m\n",
      "19\t82k\t0.15\t\t0.66\t\t0.255\t\t0.794\t\t0.42m - 8.6m / 15.3m\n",
      "VAL f1\t0.2679425837320574 - (0.2679425837320574)\n",
      "VAL loss\t0.37701771960538977\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: Fasttext\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.37701771960538977\n",
      "        | \\     )|_\tf1: 0.2679425837320574\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "Experiment Name: Rolling Sentences\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190330\\3\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: Rolling Sentences\n",
      "Description: Uses a combination of the last and the current sentence instead of just the sentence alone\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'glove', 'learning_ra[...]ne'} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                  entities_combine                 |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f643554ae6e4d618ad9b3deb13cf966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.48\t\t0.40\t\t0.000\t\t0.885\t\t0.41m - 0.4m / 0.0m\n",
      "2\t9k\t0.46\t\t0.39\t\t0.015\t\t0.872\t\t0.41m - 0.9m / 14.4m\n",
      "3\t13k\t0.42\t\t0.36\t\t0.209\t\t0.850\t\t0.41m - 1.3m / 14.6m\n",
      "4\t17k\t0.37\t\t0.43\t\t0.205\t\t0.800\t\t0.42m - 1.8m / 14.5m\n",
      "5\t22k\t0.30\t\t0.44\t\t0.208\t\t0.795\t\t0.41m - 2.2m / 14.8m\n",
      "6\t26k\t0.26\t\t0.55\t\t0.163\t\t0.704\t\t0.42m - 2.7m / 14.6m\n",
      "7\t30k\t0.22\t\t0.54\t\t0.215\t\t0.793\t\t0.42m - 3.1m / 14.7m\n",
      "8\t35k\t0.19\t\t0.54\t\t0.237\t\t0.776\t\t0.42m - 3.6m / 14.8m\n",
      "9\t39k\t0.18\t\t0.63\t\t0.233\t\t0.801\t\t0.42m - 4.0m / 14.9m\n",
      "10\t43k\t0.16\t\t0.59\t\t0.190\t\t0.778\t\t0.42m - 4.5m / 14.9m\n",
      "11\t48k\t0.16\t\t0.71\t\t0.227\t\t0.783\t\t0.42m - 4.9m / 14.9m\n",
      "12\t52k\t0.15\t\t0.68\t\t0.224\t\t0.783\t\t0.42m - 5.4m / 15.2m\n",
      "13\t56k\t0.13\t\t0.71\t\t0.231\t\t0.812\t\t0.41m - 5.8m / 15.1m\n",
      "VAL f1\t0.23715415019762845 - (0.23715415019762845)\n",
      "VAL loss\t0.35933421499588913\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: Rolling Sentences\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.35933421499588913\n",
      "        | \\     )|_\tf1: 0.23715415019762845\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "Experiment Name: Stop Words\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190330\\4\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: Stop Words\n",
      "Description: Uses stop words removal\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'glove', 'learning_ra[...]rue} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                      entities                     |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                        True                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc45afe2d114420adf1beebbb93d388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.48\t\t0.40\t\t0.000\t\t0.885\t\t0.40m - 0.4m / 0.0m\n",
      "2\t9k\t0.46\t\t0.40\t\t0.026\t\t0.800\t\t0.42m - 0.9m / 14.2m\n",
      "3\t13k\t0.42\t\t0.38\t\t0.217\t\t0.813\t\t0.42m - 1.3m / 14.6m\n",
      "4\t17k\t0.37\t\t0.44\t\t0.206\t\t0.776\t\t0.42m - 1.8m / 14.8m\n",
      "5\t22k\t0.31\t\t0.39\t\t0.228\t\t0.801\t\t0.42m - 2.2m / 14.7m\n",
      "6\t26k\t0.26\t\t0.48\t\t0.240\t\t0.770\t\t0.42m - 2.7m / 14.9m\n",
      "7\t30k\t0.23\t\t0.51\t\t0.241\t\t0.787\t\t0.42m - 3.1m / 14.7m\n",
      "8\t35k\t0.21\t\t0.54\t\t0.198\t\t0.737\t\t0.44m - 3.6m / 14.9m\n",
      "9\t39k\t0.18\t\t0.58\t\t0.235\t\t0.791\t\t0.43m - 4.1m / 15.7m\n",
      "10\t43k\t0.16\t\t0.64\t\t0.204\t\t0.773\t\t0.42m - 4.5m / 15.3m\n",
      "11\t48k\t0.15\t\t0.64\t\t0.250\t\t0.816\t\t0.41m - 5.0m / 15.1m\n",
      "12\t52k\t0.14\t\t0.65\t\t0.231\t\t0.798\t\t0.42m - 5.4m / 14.9m\n",
      "13\t56k\t0.14\t\t0.67\t\t0.244\t\t0.793\t\t0.41m - 5.9m / 15.0m\n",
      "14\t60k\t0.13\t\t0.70\t\t0.268\t\t0.828\t\t0.42m - 6.3m / 15.0m\n",
      "15\t65k\t0.13\t\t0.67\t\t0.241\t\t0.810\t\t0.42m - 6.8m / 15.2m\n",
      "16\t69k\t0.15\t\t0.62\t\t0.241\t\t0.807\t\t0.42m - 7.3m / 15.3m\n",
      "17\t73k\t0.14\t\t0.69\t\t0.235\t\t0.791\t\t0.42m - 7.7m / 15.2m\n",
      "18\t78k\t0.14\t\t0.74\t\t0.243\t\t0.811\t\t0.42m - 8.2m / 15.3m\n",
      "19\t82k\t0.15\t\t0.66\t\t0.255\t\t0.794\t\t0.42m - 8.6m / 15.3m\n",
      "VAL f1\t0.2679425837320574 - (0.2679425837320574)\n",
      "VAL loss\t0.37701771960538977\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: Stop Words\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.37701771960538977\n",
      "        | \\     )|_\tf1: 0.2679425837320574\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in experiments:\n",
    "    name = e['name']\n",
    "    print(f'#########################################################################\\n\\nExperiment Name: {name}\\n')\n",
    "    print('#########################################################################\\n\\n')\n",
    "    \n",
    "    # generate rc\n",
    "    rc = get_default_params(use_cuda=True, overwrite=e['rc'], from_default=baseline)\n",
    "    result = objective(rc, e)\n",
    "    \n",
    "    if result['status'] == STATUS_OK:\n",
    "        print(f\"       .---.\\n \\\n",
    "         /     \\\\\\n\\\n",
    "          \\\\.@-@./\\tExperiment: {e['name']}\\n\\\n",
    "          /`\\\\_/`\\\\\\tStatus: {result['status']}\\n\\\n",
    "         //  _  \\\\\\\\\\tLoss: {result['best_loss']}\\n\\\n",
    "        | \\\\     )|_\\tf1: {result['best_f1']}\\n\\\n",
    "       /`\\\\_`>  <_/ \\\\\\n\\\n",
    "       \\\\__/'---'\\\\__/\\n\")\n",
    "    else:\n",
    "        print(f\"       .---.\\n \\\n",
    "         /     \\\\\\n\\\n",
    "          \\\\.@-@./\\tExperiment: {e['name']} (FAIL)\\n\\\n",
    "          /`\\\\_/`\\\\\\n\\\n",
    "         //  _  \\\\\\\\\\\\n\\\n",
    "        | \\\\     )|_\\n\\\n",
    "       /`\\\\_`>  <_/ \\\\\\n\\\n",
    "       \\\\__/'---'\\\\__/\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
