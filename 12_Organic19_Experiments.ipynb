{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from data.data_loader import Dataset\n",
    "from misc.preferences import PREFERENCES\n",
    "from misc.run_configuration import get_default_params, OutputLayerType, LearningSchedulerType, OptimizerType, good_organic_hp_params\n",
    "from misc import utils\n",
    "\n",
    "from optimizer import get_optimizer\n",
    "from criterion import NllLoss, LossCombiner\n",
    "\n",
    "from models.transformer.encoder import TransformerEncoder\n",
    "from models.jointAspectTagger import JointAspectTagger\n",
    "from trainer.train import Trainer\n",
    "import pprint\n",
    "from data.organic2019 import organic_dataset as dsl\n",
    "from data.organic2019 import ORGANIC_TASK_ALL, ORGANIC_TASK_ENTITIES, ORGANIC_TASK_ATTRIBUTES, ORGANIC_TASK_ENTITIES_COMBINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Functions\n",
    "\n",
    "These functions will load the dataset and the model. The run configuration will determine the architecture and hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(dataset, rc, experiment_name):\n",
    "    loss = LossCombiner(4, dataset.class_weights, NllLoss)\n",
    "    transformer = TransformerEncoder(dataset.source_embedding,\n",
    "                                     hyperparameters=rc)\n",
    "    model = JointAspectTagger(transformer, rc, 4, 20, dataset.target_names)\n",
    "    optimizer = get_optimizer(model, rc)\n",
    "    trainer = Trainer(\n",
    "                        model,\n",
    "                        loss,\n",
    "                        optimizer,\n",
    "                        rc,\n",
    "                        dataset,\n",
    "                        experiment_name,\n",
    "                        enable_tensorboard=False,\n",
    "                        verbose=False)\n",
    "    return trainer\n",
    "\n",
    "def load_dataset(rc, logger, task):\n",
    "    dataset = Dataset(\n",
    "        task,\n",
    "        logger,\n",
    "        rc,\n",
    "        source_index=0,\n",
    "        target_vocab_index=1,\n",
    "        data_path=PREFERENCES.data_root,\n",
    "        train_file=PREFERENCES.data_train,\n",
    "        valid_file=PREFERENCES.data_validation,\n",
    "        test_file=PREFERENCES.data_test,\n",
    "        file_format='.tsv',\n",
    "        init_token=None,\n",
    "        eos_token=None\n",
    "    )\n",
    "    dataset.load_data(dsl, verbose=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble - Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREFERENCES.defaults(\n",
    "    data_root='./data/data/organic2019',\n",
    "    data_train='train.csv',    \n",
    "    data_validation='validation.csv',\n",
    "    data_test='test.csv',\n",
    "    early_stopping='highest_5_F1'\n",
    ")\n",
    "main_experiment_name = 'Organic19_Experiments'\n",
    "use_cuda = True\n",
    "STATUS_FAIL = 'fail'\n",
    "STATUS_OK = 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Definition of experiments\n",
    " \n",
    " - SpellChecker On\n",
    " - Fasttext\n",
    " - Single Sentence\n",
    " - Combined Sentence\n",
    " - Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'att_d_k': 100,\n",
      "  'att_d_v': 100,\n",
      "  'batch_size': 20,\n",
      "  'clip_comments_to': 195,\n",
      "  'dropout_rate': 0.392996573831,\n",
      "  'early_stopping': 5,\n",
      "  'embedding_dim': 300,\n",
      "  'embedding_name': '6B',\n",
      "  'embedding_type': 'glove',\n",
      "  'language': 'en',\n",
      "  'learning_rate_scheduler': { 'noam_learning_rate_factor': 3.3368149482,\n",
      "                               'noam_learning_rate_warmup': 4631},\n",
      "  'learning_rate_scheduler_type': <LearningSchedulerType.Noam: 1>,\n",
      "  'log_every_xth_iteration': -1,\n",
      "  'model_size': 300,\n",
      "  'num_encoder_blocks': 2,\n",
      "  'num_epochs': 35,\n",
      "  'num_heads': 3,\n",
      "  'optimizer': { 'adam_beta1': 0.89178641984,\n",
      "                 'adam_beta2': 0.83491754824,\n",
      "                 'adam_eps': 8.734158747166484e-09,\n",
      "                 'adam_weight_decay': 1e-08,\n",
      "                 'learning_rate': 0.001},\n",
      "  'optimizer_type': <OptimizerType.Adam: 1>,\n",
      "  'output_dropout_rate': 0.7608194889605,\n",
      "  'output_layer_type': <OutputLayerType.LinearSum: 1>,\n",
      "  'pointwise_layer_size': 195,\n",
      "  'task': 'entities',\n",
      "  'use_spell_checkers': False}\n"
     ]
    }
   ],
   "source": [
    "baseline = good_organic_hp_params\n",
    "print(pprint.pformat(baseline, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\n",
    "        'name': 'Baseline',\n",
    "        'description': 'Baseline. Uses good_organic_hp_params without any changes',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'SpellChecker On',\n",
    "        'description': 'Uses the baseline parameters but with the spellchecker enabled',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'use_spell_checkers': True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Fasttext',\n",
    "        'description': 'Uses english Fasttext embeddings',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'embedding_type': 'fasttext'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Rolling Sentences',\n",
    "        'description': 'Uses a combination of the last and the current sentence instead of just the sentence alone',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'task': 'entities_combine'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Stop Words',\n",
    "        'description': 'Uses stop words removal',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'use_stop_words': True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Embedding finetuning Off',\n",
    "        'description': 'This experiment prevents finetuning of the embedding layer',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'finetune_embedding': False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Organic Text Cleaning',\n",
    "        'description': 'Remove certain words and replace them with their correct counterparts',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'organic_text_cleaning': True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Full Text Cleaning',\n",
    "        'description': 'Features all text cleaning techniques including organic text cleaning, spell checkers, contraction removal, url token replacement, stop words',\n",
    "        'loss': 1000,\n",
    "        'f1': -1,\n",
    "        'rc': {\n",
    "            'organic_text_cleaning': True,\n",
    "            'use_stop_words': True,\n",
    "            'use_spell_checkers': True,\n",
    "            'contraction_removal': True,\n",
    "            'replace_url_tokens': True\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current commit: b'af92f28'\n"
     ]
    }
   ],
   "source": [
    "utils.get_current_git_commit()\n",
    "print('Current commit: ' + utils.get_current_git_commit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(rc, experiment):\n",
    "    run_time = time.time()\n",
    "    \n",
    "    # reset loggers\n",
    "    utils.reset_loggers()\n",
    "    experiment_name = utils.create_loggers(experiment_name=main_experiment_name)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    dataset_logger = logging.getLogger('data_loader')\n",
    "    \n",
    "    logger.info('Experiment: ' + experiment['name'])\n",
    "    logger.info('Description: ' + experiment['description'])\n",
    "    \n",
    "    logger.info('Parameters')\n",
    "    logger.info(rc)\n",
    "    print('\\n\\n#########################################################################')\n",
    "    print('Name: ' + experiment['name'])\n",
    "    print('Description: ' + experiment['description'])\n",
    "    print('#########################################################################\\n\\n')\n",
    "    print(rc)\n",
    "\n",
    "    logger.debug('Load dataset')\n",
    "    try:\n",
    "        dataset = load_dataset(rc, dataset_logger, rc.task)\n",
    "    except Exception as err:\n",
    "        print('Could load dataset: ' + str(err))\n",
    "        logger.exception(\"Could not load dataset\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "    logger.debug('dataset loaded')\n",
    "    logger.debug('Load model')\n",
    "\n",
    "    try:\n",
    "        trainer = load_model(dataset, rc, experiment_name)\n",
    "    except Exception as err:\n",
    "        print('Could not load model: ' + str(err))\n",
    "        logger.exception(\"Could not load model\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time\n",
    "        }\n",
    "\n",
    "    logger.debug('model loaded')\n",
    "\n",
    "    logger.debug('Begin training')\n",
    "    model = None\n",
    "    try:\n",
    "        result = trainer.train(use_cuda=rc.use_cuda, perform_evaluation=False)\n",
    "        model = result['model']\n",
    "    except Exception as err:\n",
    "        print('Exception while training: ' + str(err))\n",
    "        logger.exception(\"Could not complete iteration\")\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "\n",
    "    if math.isnan(trainer.get_best_loss()):\n",
    "        print('Loss is nan')\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "\n",
    "    # perform evaluation and log results\n",
    "    result = None\n",
    "    try:\n",
    "        result = trainer.perform_final_evaluation(use_test_set=True, verbose=False)\n",
    "    except Exception as err:\n",
    "        logger.exception(\"Could not complete iteration evaluation.\")\n",
    "        print('Could not complete iteration evaluation: ' + str(err))\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1()\n",
    "        }\n",
    "    print(f'VAL f1\\t{trainer.get_best_f1()} - ({result[1][1]})')\n",
    "    print(f'VAL loss\\t{trainer.get_best_loss()}')\n",
    "    return {\n",
    "            'loss': result[1][0],\n",
    "            'status': STATUS_OK,\n",
    "            'eval_time': time.time() - run_time,\n",
    "            'best_loss': trainer.get_best_loss(),\n",
    "            'best_f1': trainer.get_best_f1(),\n",
    "            'sample_iterations': trainer.get_num_samples_seen(),\n",
    "            'iterations': trainer.get_num_iterations(),\n",
    "            'rc': rc,\n",
    "            'results': {\n",
    "                'train': {\n",
    "                    'loss': result[0][0],\n",
    "                    'f1': result[0][1]\n",
    "                },\n",
    "                'validation': {\n",
    "                    'loss': result[1][0],\n",
    "                    'f1': result[1][1]\n",
    "                },\n",
    "                'test': {\n",
    "                    'loss': result[2][0],\n",
    "                    'f1': result[2][1]\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "\n",
      "Experiment Name: Baseline\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190331\\5\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: Baseline\n",
      "Description: Baseline. Uses good_organic_hp_params without any changes\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'glove', 'learning_ra[...]es'} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                      entities                     |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|      finetune_embedding      |                        True                       |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1444727098ea40a1b11b5d328248eab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.49\t\t0.38\t\t0.038\t\t0.882\t\t0.43m - 0.4m / 0.0m\n",
      "2\t9k\t0.42\t\t0.35\t\t0.184\t\t0.806\t\t0.42m - 0.9m / 15.2m\n",
      "3\t13k\t0.39\t\t0.37\t\t0.215\t\t0.802\t\t0.44m - 1.4m / 14.9m\n",
      "4\t17k\t0.38\t\t0.38\t\t0.222\t\t0.791\t\t0.44m - 1.9m / 15.5m\n",
      "5\t22k\t0.36\t\t0.32\t\t0.271\t\t0.823\t\t0.42m - 2.3m / 15.4m\n",
      "6\t26k\t0.34\t\t0.39\t\t0.227\t\t0.753\t\t0.43m - 2.8m / 15.1m\n",
      "7\t30k\t0.33\t\t0.37\t\t0.270\t\t0.789\t\t0.44m - 3.3m / 15.4m\n",
      "8\t35k\t0.32\t\t0.36\t\t0.256\t\t0.801\t\t0.44m - 3.8m / 15.5m\n",
      "9\t39k\t0.30\t\t0.35\t\t0.259\t\t0.823\t\t0.43m - 4.2m / 15.6m\n",
      "10\t43k\t0.29\t\t0.38\t\t0.252\t\t0.812\t\t0.43m - 4.7m / 15.4m\n",
      "VAL f1\t0.27149321266968324 - (0.27149321266968324)\n",
      "VAL loss\t0.32484197897069594\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: Baseline\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.32484197897069594\n",
      "        | \\     )|_\tf1: 0.27149321266968324\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "Experiment Name: SpellChecker On\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190331\\6\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: SpellChecker On\n",
      "Description: Uses the baseline parameters but with the spellchecker enabled\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'glove', 'learning_ra[...]es'} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                      entities                     |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|      finetune_embedding      |                        True                       |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                        True                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167f1d2a8d5543c2bb2c8d4ff1c5380d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.51\t\t0.40\t\t0.109\t\t0.871\t\t0.44m - 0.4m / 0.0m\n",
      "2\t9k\t0.42\t\t0.36\t\t0.162\t\t0.794\t\t0.45m - 0.9m / 15.4m\n",
      "3\t13k\t0.39\t\t0.36\t\t0.238\t\t0.815\t\t0.43m - 1.4m / 15.7m\n",
      "4\t17k\t0.38\t\t0.39\t\t0.229\t\t0.794\t\t0.44m - 1.9m / 15.3m\n",
      "5\t22k\t0.36\t\t0.33\t\t0.271\t\t0.818\t\t0.43m - 2.3m / 15.4m\n",
      "6\t26k\t0.35\t\t0.36\t\t0.228\t\t0.766\t\t0.43m - 2.8m / 15.3m\n",
      "7\t30k\t0.33\t\t0.36\t\t0.261\t\t0.801\t\t0.42m - 3.3m / 15.4m\n",
      "8\t35k\t0.32\t\t0.36\t\t0.241\t\t0.784\t\t0.42m - 3.7m / 15.1m\n",
      "9\t39k\t0.31\t\t0.39\t\t0.227\t\t0.804\t\t0.42m - 4.2m / 15.1m\n",
      "10\t43k\t0.29\t\t0.36\t\t0.256\t\t0.803\t\t0.42m - 4.7m / 15.3m\n",
      "VAL f1\t0.270996640537514 - (0.270996640537514)\n",
      "VAL loss\t0.3259992901016684\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: SpellChecker On\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.3259992901016684\n",
      "        | \\     )|_\tf1: 0.270996640537514\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "Experiment Name: Fasttext\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190331\\7\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: Fasttext\n",
      "Description: Uses english Fasttext embeddings\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'fasttext', 'learning[...]es'} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                      entities                     |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                      fasttext                     |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|      finetune_embedding      |                        True                       |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ac93b6f113429fa1b953234ea8b243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.49\t\t0.39\t\t0.093\t\t0.875\t\t0.43m - 0.4m / 0.0m\n",
      "2\t9k\t0.41\t\t0.36\t\t0.200\t\t0.798\t\t0.43m - 0.9m / 15.2m\n",
      "3\t13k\t0.37\t\t0.37\t\t0.238\t\t0.782\t\t0.43m - 1.4m / 15.3m\n",
      "4\t17k\t0.36\t\t0.38\t\t0.227\t\t0.782\t\t0.44m - 1.9m / 15.3m\n",
      "5\t22k\t0.34\t\t0.34\t\t0.291\t\t0.834\t\t0.44m - 2.3m / 15.4m\n",
      "6\t26k\t0.32\t\t0.38\t\t0.260\t\t0.777\t\t0.46m - 2.8m / 15.7m\n",
      "7\t30k\t0.30\t\t0.42\t\t0.243\t\t0.758\t\t0.46m - 3.3m / 16.2m\n",
      "8\t35k\t0.28\t\t0.40\t\t0.269\t\t0.790\t\t0.47m - 3.9m / 16.2m\n",
      "9\t39k\t0.25\t\t0.45\t\t0.295\t\t0.825\t\t0.44m - 4.3m / 16.5m\n",
      "10\t43k\t0.23\t\t0.45\t\t0.266\t\t0.804\t\t0.42m - 4.8m / 15.8m\n",
      "11\t48k\t0.22\t\t0.49\t\t0.280\t\t0.807\t\t0.43m - 5.3m / 15.4m\n",
      "12\t52k\t0.20\t\t0.49\t\t0.299\t\t0.826\t\t0.42m - 5.7m / 15.5m\n",
      "13\t56k\t0.19\t\t0.52\t\t0.291\t\t0.820\t\t0.43m - 6.2m / 15.5m\n",
      "14\t60k\t0.19\t\t0.56\t\t0.271\t\t0.818\t\t0.45m - 6.7m / 15.7m\n",
      "15\t65k\t0.18\t\t0.58\t\t0.264\t\t0.816\t\t0.45m - 7.2m / 16.1m\n",
      "16\t69k\t0.16\t\t0.64\t\t0.261\t\t0.800\t\t0.44m - 7.7m / 16.2m\n",
      "17\t73k\t0.17\t\t0.65\t\t0.208\t\t0.800\t\t0.43m - 8.1m / 16.0m\n",
      "VAL f1\t0.29854096520763185 - (0.29854096520763185)\n",
      "VAL loss\t0.3359302548801198\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: Fasttext\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.3359302548801198\n",
      "        | \\     )|_\tf1: 0.29854096520763185\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "Experiment Name: Rolling Sentences\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190331\\8\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: Rolling Sentences\n",
      "Description: Uses a combination of the last and the current sentence instead of just the sentence alone\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'glove', 'learning_ra[...]ne'} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                  entities_combine                 |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|      finetune_embedding      |                        True                       |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                       False                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1187a154059e43eb8ae20a0246c8e31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.49\t\t0.38\t\t0.042\t\t0.880\t\t0.43m - 0.4m / 0.0m\n",
      "2\t9k\t0.42\t\t0.36\t\t0.140\t\t0.805\t\t0.43m - 0.9m / 15.0m\n",
      "3\t13k\t0.39\t\t0.37\t\t0.220\t\t0.782\t\t0.44m - 1.4m / 15.2m\n",
      "4\t17k\t0.38\t\t0.39\t\t0.222\t\t0.789\t\t0.44m - 1.9m / 15.4m\n",
      "5\t22k\t0.36\t\t0.33\t\t0.255\t\t0.826\t\t0.45m - 2.3m / 15.6m\n",
      "6\t26k\t0.35\t\t0.39\t\t0.201\t\t0.741\t\t0.44m - 2.8m / 15.8m\n",
      "7\t30k\t0.34\t\t0.38\t\t0.238\t\t0.791\t\t0.44m - 3.3m / 15.5m\n",
      "8\t35k\t0.33\t\t0.42\t\t0.234\t\t0.772\t\t0.44m - 3.8m / 15.6m\n",
      "9\t39k\t0.31\t\t0.43\t\t0.219\t\t0.782\t\t0.42m - 4.3m / 15.8m\n",
      "10\t43k\t0.29\t\t0.41\t\t0.262\t\t0.814\t\t0.43m - 4.7m / 15.3m\n",
      "11\t48k\t0.28\t\t0.42\t\t0.246\t\t0.832\t\t0.43m - 5.2m / 15.6m\n",
      "12\t52k\t0.27\t\t0.44\t\t0.263\t\t0.779\t\t0.44m - 5.7m / 15.6m\n",
      "13\t56k\t0.26\t\t0.47\t\t0.238\t\t0.801\t\t0.44m - 6.2m / 15.7m\n",
      "14\t60k\t0.25\t\t0.48\t\t0.239\t\t0.816\t\t0.43m - 6.6m / 15.8m\n",
      "15\t65k\t0.25\t\t0.43\t\t0.233\t\t0.802\t\t0.43m - 7.1m / 15.7m\n",
      "16\t69k\t0.23\t\t0.54\t\t0.223\t\t0.805\t\t0.43m - 7.6m / 15.7m\n",
      "17\t73k\t0.23\t\t0.51\t\t0.218\t\t0.793\t\t0.43m - 8.0m / 15.7m\n",
      "VAL f1\t0.26285714285714284 - (0.26285714285714284)\n",
      "VAL loss\t0.33419568398419547\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: Rolling Sentences\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.33419568398419547\n",
      "        | \\     )|_\tf1: 0.26285714285714284\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "Experiment Name: Stop Words\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "Log path is  C:\\Users\\felix\\OneDrive\\Studium\\Studium\\6. Semester\\MA\\Project\\ABSA-Transformer\\logs\\Organic19_Experiments\\20190331\\9\n",
      "\n",
      "\n",
      "#########################################################################\n",
      "Name: Stop Words\n",
      "Description: Uses stop words removal\n",
      "#########################################################################\n",
      "\n",
      "\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                 Hyperparameters                                  |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|          Parameter           |                       Value                       |\n",
      "+------------------------------+---------------------------------------------------+\n",
      "|            kwargs            | {'embedding_type': 'glove', 'learning_ra[...]rue} |\n",
      "|          model_size          |                        300                        |\n",
      "|        early_stopping        |                         5                         |\n",
      "|           use_cuda           |                        True                       |\n",
      "|             task             |                      entities                     |\n",
      "|          batch_size          |                         20                        |\n",
      "| learning_rate_scheduler_type |             LearningSchedulerType.Noam            |\n",
      "|      output_layer_type       |             OutputLayerType.LinearSum             |\n",
      "|        optimizer_type        |                 OptimizerType.Adam                |\n",
      "|        learning_rate         |                       0.001                       |\n",
      "|  noam_learning_rate_warmup   |                        4631                       |\n",
      "|  noam_learning_rate_factor   |                    3.3368149482                   |\n",
      "|          adam_beta1          |                   0.89178641984                   |\n",
      "|          adam_beta2          |                   0.83491754824                   |\n",
      "|           adam_eps           |               8.734158747166484e-09               |\n",
      "|      adam_weight_decay       |                       1e-08                       |\n",
      "|         adam_amsgrad         |                       False                       |\n",
      "|           use_bias           |                       False                       |\n",
      "|         n_enc_blocks         |                         2                         |\n",
      "|           n_heads            |                         3                         |\n",
      "|             d_k              |                        100                        |\n",
      "|             d_v              |                        100                        |\n",
      "|         dropout_rate         |                   0.392996573831                  |\n",
      "|     pointwise_layer_size     |                        195                        |\n",
      "|      last_layer_dropout      |                  0.7608194889605                  |\n",
      "|   log_every_xth_iteration    |                         -1                        |\n",
      "|          num_epochs          |                         35                        |\n",
      "|        embedding_type        |                       glove                       |\n",
      "|        embedding_name        |                         6B                        |\n",
      "|        embedding_dim         |                        300                        |\n",
      "|       clip_comments_to       |                        195                        |\n",
      "|      finetune_embedding      |                        True                       |\n",
      "|           language           |                         en                        |\n",
      "|        use_stop_words        |                        True                       |\n",
      "|         use_stemming         |                       False                       |\n",
      "|        harmonize_bahn        |                       False                       |\n",
      "|      use_spell_checkers      |                       False                       |\n",
      "|      replace_url_tokens      |                        True                       |\n",
      "|       use_text_cleaner       |                       False                       |\n",
      "|             seed             |                         42                        |\n",
      "+------------------------------+---------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5329ab9c9ca34f618e9b079c6bca6dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EP\t# IT\ttr loss\t\tval loss\tf1\t\tacc\t\tduration / total time\n",
      "1\t4k\t0.50\t\t0.37\t\t0.194\t\t0.859\t\t0.42m - 0.4m / 0.0m\n",
      "2\t9k\t0.41\t\t0.34\t\t0.211\t\t0.810\t\t0.43m - 0.9m / 14.8m\n",
      "3\t13k\t0.38\t\t0.35\t\t0.262\t\t0.809\t\t0.44m - 1.4m / 15.3m\n",
      "4\t17k\t0.37\t\t0.36\t\t0.251\t\t0.802\t\t0.44m - 1.9m / 15.4m\n",
      "5\t22k\t0.34\t\t0.35\t\t0.257\t\t0.802\t\t0.44m - 2.3m / 15.6m\n",
      "6\t26k\t0.33\t\t0.41\t\t0.232\t\t0.768\t\t0.43m - 2.8m / 15.7m\n",
      "7\t30k\t0.31\t\t0.39\t\t0.259\t\t0.797\t\t0.44m - 3.3m / 15.4m\n",
      "8\t35k\t0.30\t\t0.37\t\t0.267\t\t0.803\t\t0.47m - 3.8m / 15.5m\n",
      "9\t39k\t0.28\t\t0.41\t\t0.242\t\t0.803\t\t0.46m - 4.3m / 16.6m\n",
      "10\t43k\t0.27\t\t0.44\t\t0.224\t\t0.773\t\t0.42m - 4.8m / 16.2m\n",
      "11\t48k\t0.25\t\t0.40\t\t0.304\t\t0.834\t\t0.42m - 5.2m / 15.4m\n",
      "12\t52k\t0.24\t\t0.47\t\t0.254\t\t0.805\t\t0.43m - 5.7m / 15.4m\n",
      "13\t56k\t0.22\t\t0.57\t\t0.225\t\t0.786\t\t0.42m - 6.2m / 15.6m\n",
      "14\t60k\t0.22\t\t0.58\t\t0.242\t\t0.794\t\t0.42m - 6.6m / 15.5m\n",
      "15\t65k\t0.21\t\t0.57\t\t0.210\t\t0.778\t\t0.42m - 7.1m / 15.6m\n",
      "16\t69k\t0.20\t\t0.62\t\t0.185\t\t0.772\t\t0.42m - 7.6m / 15.6m\n",
      "VAL f1\t0.3042973286875726 - (0.3042973286875726)\n",
      "VAL loss\t0.34349424137788664\n",
      "       .---.\n",
      "          /     \\\n",
      "          \\.@-@./\tExperiment: Stop Words\n",
      "          /`\\_/`\\\tStatus: ok\n",
      "         //  _  \\\\\tLoss: 0.34349424137788664\n",
      "        | \\     )|_\tf1: 0.3042973286875726\n",
      "       /`\\_`>  <_/ \\\n",
      "       \\__/'---'\\__/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in experiments:\n",
    "    name = e['name']\n",
    "    print(f'#########################################################################\\n\\nExperiment Name: {name}\\n')\n",
    "    print('#########################################################################\\n\\n')\n",
    "    \n",
    "    # generate rc\n",
    "    rc = get_default_params(use_cuda=True, overwrite=e['rc'], from_default=baseline)\n",
    "    result = objective(rc, e)\n",
    "    \n",
    "    if result['status'] == STATUS_OK:\n",
    "        print(f\"       .---.\\n \\\n",
    "         /     \\\\\\n\\\n",
    "          \\\\.@-@./\\tExperiment: {e['name']}\\n\\\n",
    "          /`\\\\_/`\\\\\\tStatus: {result['status']}\\n\\\n",
    "         //  _  \\\\\\\\\\tLoss: {result['best_loss']}\\n\\\n",
    "        | \\\\     )|_\\tf1: {result['best_f1']}\\n\\\n",
    "       /`\\\\_`>  <_/ \\\\\\n\\\n",
    "       \\\\__/'---'\\\\__/\\n\")\n",
    "    else:\n",
    "        print(f\"       .---.\\n \\\n",
    "         /     \\\\\\n\\\n",
    "          \\\\.@-@./\\tExperiment: {e['name']} (FAIL)\\n\\\n",
    "          /`\\\\_/`\\\\\\n\\\n",
    "         //  _  \\\\\\\\\\\\n\\\n",
    "        | \\\\     )|_\\n\\\n",
    "       /`\\\\_`>  <_/ \\\\\\n\\\n",
    "       \\\\__/'---'\\\\__/\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
