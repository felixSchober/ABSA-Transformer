\chapter{Method}
\label{ch:method}

The transformer model has shown great potential on a variety challenging \gls{nlp} tasks. Originally, the transformer was created to perform machine translation where it outperformed previous models by a huge margin \cite{Vaswani2017}. In fact, at the time of writing, the transformer is the core of the Google translate engine.
\medskip

OpenAI uses the transformer to perform various \gls{nlp} tasks \cite{Radford2018}. Their \gls{gpt}-model solves classification, entailment, similarity and multiple choice question answering tasks. They use a transformer encoder stack with 12 encoder blocks and task dependent classification heads. 

They show that even though the same base transformer model is used for each task they achieve \gls{sota} results for most of the tasks.
\medskip

One year later, in 2019, OpenAI published \gls{gpt}-2 which is an extension to \gls{gpt} \cite{Radford2019}. With \gls{gpt}-2 Radford et. al. are able to achieve revolutionary results generating text with a transformer architecture\footnote{OpenAI will not release the trained model for the language generation task as they are afraid that it will be used for malicious intents - Source: OpenAI blog \url{https://openai.com/blog/better-language-models/}}. Again, they also achieve \gls{sota} results on other \gls{nlp} task mentioned above.
\bigskip

We propose the \acrfull{absat} which builds on the knowledge that the transformer is a powerful architecture useful for a variety of NLP tasks. Radford et. al. already show that the transformer can be used to predict sentiment for a document \cite{Radford2018}. 

However, sentiment analysis is one of the few tasks where \gls{gpt}-1 is not able to achieve \gls{sota} results. Moreover, \gls{gpt}-1 only classifies standard sentiment and not aspect-based sentiment.
\medskip

This will be the first architecture which uses a transformer to perform multi-task aspect-based sentiment analysis. The next section describes the architecture of the \gls{absat} model. Section \ref{sec:04_multitask} describes how \gls{absat} is used in combination with multi task learning and finally, section \ref{sec:04_transferLearning} explains how we used transfer learning to boost the training performance.

\section{Architecture}

The following section describes the proposed \acrfull{absat} architecture. As the name suggests this design is based on the transformer model \cite{Vaswani2017}. The second model characteristic is influenced by the work of Schmitt et. al. and their concept of separate aspect heads \cite{Schmitt2018}.
\bigskip

\begin{figure}[htp]
	\centering
	\includegraphics[width=\textwidth]{figures/04_method/04_t-absa}
	\caption{Visualization of an exemplary \acrfull{absat} model with one encoder block consisting of three attention heads and four aspect heads. The positional encoding is not visualized in this figure.}
	\label{fig:04_t-absa}
\end{figure}

Figure \ref{fig:04_t-absa} visualizes a simplified \gls{absat} model. This specific instance consists one encoder block which contains 3 attention heads. To the right of the encoder block are four aspect heads. Each aspect head is trained to classifiy the sentiment for one aspect. In figure \ref{fig:04_t-absa} those aspects are \textit{GMOs}, \textit{Quality}, \textit{Price} and \textit{Taste}. Each aspect has four output classes {(visualized in the figure for the "Pesticides"-aspect)}:

\begin{itemize}
	\item negative
	\item neutral
	\item positive
	\item not applicable {(N/A)}
\end{itemize}

Each aspect head is isolated from the rest which allows the transformer to perform multi-label \gls{absa}. When a document or sentence contains a specific aspect the corresponding aspect head outputs either \textit{negative}, \textit{neutral} or \textit{positive}. If this aspect is not part of the sentence, the output is \textit{not applicable}.

We propose two different versions of aspect heads which are described in section \ref{sec:04_aspectHeads} in detail.
\bigskip

The original transformer uses undisclosed word embeddings which output a 512-dimensional vector\footnote{They also experiment with 256 and 1024 dimensional vectors}. It is possible that the original transformer does not use pretrained embeddings. We performed experiments with \gls{absat} and untrained word embeddings but concluded that pretrained word embeddings outperform untrained word embeddings. 

However, the difference was only a few percent and it is conceivable that a transformer with more training data would be able to train its own word embeddings.
\medskip

Considering the smaller datasets that we use for \gls{absa}, the \gls{absat} model uses pretrained embeddings instead of untrained embeddings. Pretrained embeddings for both \gls{glove} and fastText are only available for up to 300 dimensions. As a consequence, the model size of transformer is only 300 instead of 512.
\bigskip

Similar to the vanilla transformer, \gls{absat} also uses an \gls{adam} optimizer \cite{Kingma2014} and a special learning rate decay which is called NOAM\footnote{It is not clear what noam stands for or where this learning rate decay schema came from. It is not mentioned or cited as noam but it is referred to as noam by the authors in discussions: \url{https://github.com/tensorflow/tensor2tensor/issues/280}} \cite{Vaswani2017}

\begin{equation}
	\text{NOAM:} \quad lr = d_\text{model}^{0.5} * min(step\_num^{0.5}, step\_num*warmup\_steps^{-1.5})
\end{equation}
 
Contrary to the transformer model, we do not use label smoothing as a regularization technique. Experiments showed that this impacted the F1-score negatively. Instead, \gls{absat} uses weight decay with a decay value of $\epsilon_w = 1e-8$.

\subsection{Transformer}
\label{sec:04_transformer}

\subsection{Aspect Heads}
\label{sec:04_aspectHeads}

\subsubsection{Linear Mean-Head}

why mean? -> bring loss to similar value regardless of a) word length and b) aspect head choice (linear vs cnn)

\subsubsection{Projection Mean-Head}

\subsubsection{CNN-Head}

\subsubsection*{Weighted Loss}

\begin{equation}
\mathcal{L}_\text{NLL}=-\frac{1}{n}\sum_{i=1}^{n} w_i * (y_i \cdot log(\hat{y}_i))
\label{eq:04_nll}
\end{equation}

where $n$ is the number of classes, $w_i$ is the class weight, $y_i$ is the ground truth and $\hat{y}_i$ is the prediction.

\section{Multi-Task Learning}
\label{sec:04_multitask}
The way the \gls{absa}-Transformer is build, inevitably necessitates multi-task learning since for each aspect-head a separate \gls{nll}-loss is computed. For each of the $m$ classes a loss value is calculated. In the end the mean of all losses is taken as shown in equation \ref{eq:04_multiheadLoss}.

\begin{equation}
\mathcal{L}_\text{MultiTask} = \frac{1}{m}\sum_{j=1}^{m}\mathcal{L}(f(x), y_m)
\label{eq:04_multiheadLoss}
\end{equation}

where $\mathcal{L}(f(x), y_m)$ is the \gls{nll}-loss of the model $f$ with input the $x$.

\subsection*{Multitask Task Data Augmentation}

As described in section \ref{sec:03_mtlAdvantages} it is also possible to augment the data by using an auxiliary task in addition to the regular classification tasks.

There are three possible auxiliary tasks to consider:


\begin{enumerate}
	\item Predict additional label from the source data
	\item Use an additional dataset B in combination with the source dataset A and predict labels for the classes in A and the classes in B simulatanously. This approach is similar to transfer learning but instead of training the models sequentially, this approach would train them together.
	\item Predict additional aspects of the source data which can be trained unsupervised.
\end{enumerate}

This thesis will focus on the first type of auxiliary task where we will try predict an additional label for the source dataset. As the source dataset we choose the GermEval-2017 data since this dataset provides an additional document-wide sentiment label. This label was choosen since the other aspect heads already perform sentiment ananlisis so this task is very similar on the one hand but can provide additional datapoints for the training of the model. In addition the dataset provides a reasonable amnount of training data.

Training with the auxiliary sentiment label is performed by adding an additional sentiment head to the model.

%We also experiment with the weighting of the tasks in the loss


\section{Transfer Learning}
\label{sec:04_transferLearning}
comparison to image first layer features which are very similar regardless of target domain. \cite{Yosinski2014} -> Embedding layer, Transformer

However, last layer usually very dependet on domain and dataset -> good for model because last layer -> heads only domain relevant. So keep Embedding and transformer and exchange heads.

