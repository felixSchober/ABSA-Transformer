\chapter{Method}
\label{ch:method}

\section{Architecture}

\subsection{Transformer}

\subsection{Aspect Heads}

\subsubsection{Linear Mean-Head}

why mean? -> bring loss to similar value regardless of a) word length and b) aspect head choice (linear vs cnn)

\subsubsection{Projection Mean-Head}

\subsubsection{CNN-Head}

\section{Multi-Task Learning}

\section{Transfer Learning}

comparison to image first layer features which are very similar regardless of target domain. \cite{Yosinski2014} -> Embedding layer, Transformer

However, last layer usually very dependet on domain and dataset -> good for model because last layer -> heads only domain relevant. So keep Embedding and transformer and exchange heads.

