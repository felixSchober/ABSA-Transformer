\chapter{Discussion of Results}
\label{ch:discussion}

%TODO: Idee: Attention visualisieren am Anfang und am Ende des Transformers: Hat er wÃ¤hrend des Trainings gelernt?

\section{Hyper Parameter Optimization}

\subsection{Model Parameters}

\subsubsection{Aspect Heads}

see \cite{Ramsundar2015}

\subsubsection{Pointwise Layer Size}

Why smaller than model? -> 

This dimensionality reduction is similar in moti- vation and implementation to the 1x1 convolutions in the GoogLeNet architecture (Szegedy et al., 2014). The wide lower layer allows for complex, expressive features to be learned while the narrow layer limits the parameters spe- cific to each task.

\cite{Ramsundar2015}

\subsubsection{Transformer Architecture}

\subsubsection{Learning Rate Scheduler}

\subsubsection{Optimizer}

\subsubsection{Embedding}


\subsection{Data Preprocessing}
\label{subsec:06_dataPreprocessing}

\subsubsection{Spell Checking}

\subsubsection{Stop Word Removal}

\subsubsection{Comment Clipping}
\ref{subsec:06_CommentClipping}


\section{Results for Named Entity Recognition}

\section{Results for Aspect-Based Sentiment Analysis}

\subsection{GermEval-2017}
\label{sec:06_ResultsGermEval}

\subsection{Organic-2019}

\subsection{Amazon Product Reviews}

\section{Impact of Multitask Learning}
Difference to Multitask learning



\section{Impact of Transfer Learning}

