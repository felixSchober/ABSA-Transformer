\chapter{Discussion of Results}
\label{ch:discussion}

%TODO: Idee: Attention visualisieren am Anfang und am Ende des Transformers: Hat er wÃ¤hrend des Trainings gelernt?

\section{Hyper Parameter Optimization}

The following presents the results of the hyper parameter optimization on GermEval-2017 Task C and the Organic-2019 Coarse category dataset. We evaluate the performance of Hyperopt compared to a random search. Then, the next sections show how certain parameters impact the model performance. 

\subsection{Hyperopt Evaluation}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.49\textwidth]{figures/06_results/06_hp_ge_lm_loss-iteration_validation}
	\includegraphics[width=0.49\textwidth]{figures/06_results/06_hp_ge_lm_loss-iteration_test}
	\caption{Validation- {(blue - left)} and test {(red - right)} losses during 100 Hyperopt iterations on GermEval-2017 dataset}
	\label{fig:06_ValidationLossGermEvalHp}
\end{figure}

To evaluate hyperopt three evaluation runs with 100 iterations were performed.

\begin{enumerate}
	\item GermEval-2017 - \gls{tpe} on validation loss
	\item GermEval-2017 - Random search
	\item Organic Coarse Grained - \gls{tpe} on validation loss
\end{enumerate}

Figure \ref{fig:06_ValidationLossGermEvalHp} visualizes the improvement of the validation- and test losses on the GermEval-2017 dataset after 100 Hyperopt iterations. Is seems as if the regression line is negative in both cases which means that the \gls{tpe} algorithm Hyperopt uses suggests better results as the time moves on.

Unfortunatelly, the \gls{ols} analyises \ref{tab:08_olsLossItVal} and \ref{tab:08_olsLossItTest} in the appendix show that the negative correlation is in fact not significant. This implies that the \gls{tpe} algorithm does not sample paramters from the space which actually improve the loss of the model. This is even more obivous in figure \ref{fig:06_F1GermEvalHp}. This figure shows the development of F1-Score during optimization. While the results on the left for GermEval might look like they improve over the course of the optimization the results\footnote{The improvement is still not significant {(0.340)}} for the optimization of the coarse organic dataset clearly show no improvement.
\medskip

There are several possible explanations which could contribute to this behaviour:

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.49\textwidth]{figures/06_results/06_hp_ge_lm_f1time}
	\includegraphics[width=0.49\textwidth]{figures/06_results/06_hp_og_lm_f1time}
	\caption{F1 Scores of the hyperparamter optimization of GermEval-2017 {(left)} and Coarse Organic 2019 datasets {(right)}.}
	\label{fig:06_F1GermEvalHp}
\end{figure}

\subsubsection*{Iterations}

First, it could be possible that 100 iterations are not enough to provide a stochastic model which is able to make good prdedictions in the hyperparameter space. It is worth noting that Bergstra et. al. show that hyperopt outperforms random search within 200 trials \cite{Bergstra2013}. However, for most architectures it is not feasible to run an optimization search for much longer than 200 iterations let alone the 1000 iterations they claim as the point where hyperopt converges. 
\medskip

It is also worth menioning that the Hyperopt module uses a random sampler for the first 10 iterations to get data points to initialize the \glspl{tpe}. Decreasing this number could yield to better results for computational expensive models since the algorithm is forced to suggest values earlier.

\subsubsection*{Hyperparamter Search Space}

It is possible that the hyperparameter search space which Hyperopt uses to generate new paramters is too large. Table \ref{tab:08_hpSpace} shows the hyperparameter search space for the optimization of the GermEval-2017 dataset. There are paramters which do not change the outcome by a huge margin and then there are parameters which decide whether or not the model trains at all. However, finding those parameters is a challenging task.

\subsubsection*{Comparison with a Random Search}

To confirm the findings above a completely random search was performed by hyperopt on the same number of iterations. 

\subsection{Model Parameters}

\subsubsection{Aspect Heads}

see \cite{Ramsundar2015}

\subsubsection{Pointwise Layer Size}

Why smaller than model? -> 

This dimensionality reduction is similar in moti- vation and implementation to the 1x1 convolutions in the GoogLeNet architecture (Szegedy et al., 2014). The wide lower layer allows for complex, expressive features to be learned while the narrow layer limits the parameters spe- cific to each task.

\cite{Ramsundar2015}

\subsubsection{Transformer Architecture}

\subsubsection{Learning Rate Scheduler}

\subsubsection{Optimizer}

\subsubsection{Embedding}


\subsection{Data Preprocessing}
\label{subsec:06_dataPreprocessing}

\subsubsection{Spell Checking}

\subsubsection{Stop Word Removal}

\subsubsection{Comment Clipping}
\ref{subsec:06_CommentClipping}


\section{Results for Named Entity Recognition}

\section{Results for Aspect-Based Sentiment Analysis}

\subsection{GermEval-2017}
\label{sec:06_ResultsGermEval}

\subsection{Organic-2019}

\subsection{Amazon Product Reviews}

\section{Impact of Multitask Learning}
Difference to Multitask learning



\section{Impact of Transfer Learning}

