\chapter{Discussion of Results}
\label{ch:discussion}

\section{Hyper Parameter Optimization}

\subsection{Pointwise Layer Size}

Why smaller than model? -> 

This dimensionality reduction is similar in moti- vation and implementation to the 1x1 convolutions in the GoogLeNet architecture (Szegedy et al., 2014). The wide lower layer allows for complex, expressive features to be learned while the narrow layer limits the parameters spe- cific to each task.

\cite{Ramsundar2015}

\subsection{Spell Checker}


\section{Results for Named Entity Recognition}

\section{Results for Aspect-Based Sentiment Analysis}

\subsection{GermEval-2017}


\subsection{GermEval-2017}

\subsection{Organic-2019}

\section{Impact of Multitask Learning}
Difference to Multitask learning

\subsection{Impact of Aspect heads on performance}

see \cite{Ramsundar2015}

\section{Impact of Transfer Learning}

