\chapter{\abstractname}

In this master's thesis, we propose a novel neural network architecture for \acrfull{absa}. This architecture is based on the Google transformer introduced in 2017 by Vaswani et al.~\cite{Vaswani2017} and inspired by the works of Schmitt et al.~\cite{Schmitt2018}.
\medskip

The model we propose classifies multi-label aspect-based sentiment end-to-end. To the best of our knowledge, this is the first model which uses a transformer for aspect-based sentiment analysis.
\medskip

Furthermore, this thesis explores transfer- and multitask learning. We show that multitask learning is capable of augmenting data with auxiliary tasks, thereby boosting model performance. For transfer learning, we develop a new large scale amazon review dataset with almost 1.2 million reviews. We use this dataset to transfer knowledge to a much smaller organic dataset. We achieve significant performance increases using this technique.
\medskip

We evaluate and benchmark our proposed architecture on four datasets and show that the proposed \acrfull{absat} model achieves very competitive results on all datasets.

