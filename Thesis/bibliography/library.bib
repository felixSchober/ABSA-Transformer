Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Mikolov2013e,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
address = {Lake Tahoe, USA},
archivePrefix = {arXiv},
arxivId = {1310.4546v1},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {NIPS'13 Proceedings of the 26th International Conference on Neural Information Processing Systems},
eprint = {1310.4546v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
keywords = {()},
pages = {3111--3119},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {https://arxiv.org/pdf/1310.4546.pdf},
year = {2013}
}
@article{Zong2010,
abstract = {This paper proposes a food image classification method using local textural patterns and their global structure to describe the food image. In this paper, a visual codebook of local textural patterns is created by employing Scale Invariant Feature Transformation (SIFT) interest point detector with the Local Binary Pattern (LBP) feature. In addition to describing the food image using local texture, the global structure of the food object is represented as the spatial distribution of the local textural structures and encoded using shape context. We evaluated the proposed method on the Pittsburgh Fast-Food Image (PFI) dataset. Experimental results showed that the proposed method could obtain better performance than the baseline experiment on the PFI dataset.},
author = {Zong, Zhimin and Nguyen, Duc Thanh and Ogunbona, Philip and Li, Wanqing},
doi = {10.1109/ISM.2010.37},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Zong et al. - 2010 - On the combination of local texture and global structure for food classification.pdf:pdf},
isbn = {9780769542171},
journal = {Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010},
keywords = {Bhattacharyya distance,Codeword filtering,Food classification,Local binary pattern,PFID,Shape context},
mendeley-tags = {Bhattacharyya distance,Codeword filtering,PFID},
pages = {204--211},
title = {{On the combination of local texture and global structure for food classification}},
year = {2010}
}
@book{Theodoridis2009s,
abstract = {This chapter explores the template matching. Template matching involves defining a measure or a cost to find the “similarity” between the (known) reference patterns and the (unknown) test pattern by performing the matching operation. It finds its application in speech recognition, in automation using robot vision, in motion estimation for video coding, and in image database retrieval systems. The chapter explores the problem of string pattern matching and then deals with the scene analysis and shape recognition problems. The tasks, although they share the same goal, require different tools because of their different nature. Measures based on optimal path searching techniques and measures based on correlation are discussed. One section discusses a category of template matching, where the involved patterns consist of strings of identified symbols or feature vectors (string patterns). Topics of deformable template model and content-based information retrieval are discussed. Dynamic programming and the Viterbi algorithm are presented in the chapter and then applied to speech recognition. Correlation matching and the basic philosophy behind deformable template matching are also presented.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50010-4},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(8).pdf:pdf},
isbn = {9781597492720},
pages = {481--519},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500104},
year = {2009}
}
@article{scikit-image,
author = {van der Walt, St{\'{e}}fan and Sch{\"{o}}nberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony and The scikit-image contributors},
doi = {10.7717/peerj.453},
issn = {2167-8359},
journal = {PeerJ},
keywords = {Education,Image processing,Open source,Python,Reproducible research,Scientific programming,Visualization},
pages = {e453},
title = {{scikit-image: image processing in {\{}P{\}}ython}},
url = {http://dx.doi.org/10.7717/peerj.453},
volume = {2},
year = {2014}
}
@inproceedings{Villalobos2012,
abstract = {Obesity in the world has spread to epidemic proportions. In 2008 the World Health Organization (WHO) reported that 1.5 billion adults were suffering from some sort of overweightness. Obesity treatment requires constant monitoring and a rigorous control and diet to measure daily calorie intake. These controls are expensive for the health care system, and the patient regularly rejects the treatment because of the excessive control over the user. Recently, studies have suggested that the usage of technology such as smartphones may enhance the treatments of obesity and overweight patients; this will generate a degree of comfort for the patient, while the dietitian can count on a better option to record the food intake for the patient. In this paper we propose a smart system that takes advantage of the technologies available for the Smartphones, to build an application to measure and monitor the daily calorie intake for obese and overweight patients. Via a special technique, the system records a photo of the food before and after eating in order to estimate the consumption calorie of the selected food and its nutrient components. Our system presents a new instrument in food intake measuring which can be more useful and effective.},
author = {Villalobos, Gregorio and Almaghrabi, Rana and Pouladzadeh, Parisa and Shirmohammadi, Shervin},
booktitle = {2012 IEEE International Symposium on Medical Measurements and Applications Proceedings},
doi = {10.1109/MeMeA.2012.6226636},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Villalobos et al. - 2012 - An image procesing approach for calorie intake measurement.pdf:pdf},
isbn = {978-1-4673-0882-3},
keywords = {Calorie intake measurement,Calories measurement,Food intake measurement,Image color analysis,Image processing,Image segmentation,Obesity,Shape,Shape recognition,Support vector machines,Thumb,World Health Organization,adults,biomedical measurement,calorie intake,constant monitoring,consumption calorie,dietitian,diseases,epidemic proportions,food intake,geriatrics,health care,health care system,image procesing approach,medical image processing,nutrient components,obesity treatment,patient treatment,rigorous control,smart system,smartphones},
language = {English},
month = {may},
pages = {1--5},
publisher = {IEEE},
title = {{An image procesing approach for calorie intake measurement}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6226636},
year = {2012}
}
@article{Tolstikhin2017,
archivePrefix = {arXiv},
arxivId = {1711.01558},
author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
eprint = {1711.01558},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Tolstikhin et al. - 2017 - Wasserstein Auto-Encoders.pdf:pdf},
month = {nov},
title = {{Wasserstein Auto-Encoders}},
url = {https://arxiv.org/abs/1711.01558},
year = {2017}
}
@techreport{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4{\%} (7.6{\%} absolute improvement), MultiNLI accuracy to 86.7{\%} (5.6{\%} absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.},
annote = {BERT is designed to pre-train
deep bidirectional representations by jointly
conditioning on both left and right context in
all layers.


BERT's model architecture is a multi-layer bidi-rectional Transformer encoder based on the orig-inal implementation},
archivePrefix = {arXiv},
arxivId = {1810.04805v1},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Google, Kristina Toutanova and Language, A I},
eprint = {1810.04805v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
keywords = {Transformer},
mendeley-tags = {Transformer},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {https://github.com/tensorflow/tensor2tensor},
year = {2018}
}
@article{Matsuda2012,
abstract = {In this paper, we propose a two-step method to recognize multiple-food images by detecting candidate regions with several methods and classifying them with various kinds of features. In the first step, we detect several candidate re- gions by fusing outputs of several region detectors including Felzenszwalb's deformable part model (DPM) [1], a circle de- tector and the JSEG region segmentation. In the second step, we apply a feature-fusion-based food recognition method for bounding boxes of the candidate regions with various kinds of visual features including bag-of-features of SIFT and CSIFT with spatial pyramid (SP-BoF), histogram of oriented gradi- ent (HoG), and Gabor texture features. In the experiments, we estimated ten food candidates for multiple-food images in the descending order of the confi- dence scores. As results, we have achieved the 55.8{\%} classi- fication rate, which improved the baseline result in case of us- ing only DPM by 14.3 points, for a multiple-food image data set. This demonstrates that the proposed two-step method is effective for recognition of multiple-food images.},
annote = {UEC FOOD 100},
author = {Matsuda, Yuji and Hoashi, Hajime and Yanai, Keiji},
doi = {10.1109/ICME.2012.157},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Matsuda, Hoashi, Yanai - 2012 - Recognition of multiple-food images by detecting candidate regions.pdf:pdf},
isbn = {978-1-4673-1659-0},
issn = {19457871},
journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
keywords = {multiple kernel learning,multiple-food image,region detection,window search},
pages = {25--30},
title = {{Recognition of multiple-food images by detecting candidate regions}},
year = {2012}
}
@article{Horton1997,
abstract = {We have compared four classifiers on the problem of predicting the cellular localization sites of proteins in yeast and E. coli. A set of sequence derived features, such as regions of high hydrophobicity, were used for each classifier. The methods compared were a structured probabilistic model specifically designed for the localization problem, the k nearest neighbors classifier, the binary decision tree classifier, and the na{\"{i}}ve Bayes classifier. The result of tests using stratified cross validation shows the k nearest neighbors classifier to perform better than the other methods. In the case of yeast this difference was statistically significant using a cross-validated paired t test. The result is an accuracy of approximately 60{\%} for 10 yeast classes and 86{\%} for 8 E. coli classes. The best previously reported accuracies for these datasets were 55{\%} and 81{\%} respectively.},
author = {Horton, P and Nakai, K},
isbn = {1553-0833 (Print)},
issn = {1553-0833},
journal = {Proceedings / ... International Conference on Intelligent Systems for Molecular Biology ; ISMB. International Conference on Intelligent Systems for Molecular Biology},
keywords = {classification,classifier,colz,e,k nearest neighbor,protein localization,yeast},
pages = {147--152},
pmid = {9322029},
title = {{Better prediction of protein cellular localization sites with the k nearest neighbors classifier.}},
volume = {5},
year = {1997}
}
@article{Kawano2015,
abstract = {In this paper, we propose a novel effective framework to ex-pand an existing image dataset automatically leveraging existing cat-egories and crowdsourcing. Especially, in this paper, we focus on ex-pansion on food image data set. The number of food categories is un-countable, since foods are different from a place to a place. If we have a Japanese food dataset, it does not help build a French food recognition system directly. That is why food data sets for different food cultures have been built independently category so far. Then, in this paper, we propose to leverage existing knowledge on food of other cultures by a generic " foodness " classifier and domain adaptation. This can enable us not only to built other-cultured food datasets based on an original food image dataset automatically, but also to save as much crowd-sourcing costs as possible. In the experiments, we show the effectiveness of the proposed method over the baselines.},
annote = {UEC FOOD 256},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1007/978-3-319-16199-0_1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kawano, Yanai - 2015 - Automatic expansion of a food image dataset leveraging existing categories with domain adaptation.pdf:pdf},
isbn = {9783319161983},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Adaptive SVM,Crowd-sourcing,Dataset expansion,Domain adaptation,Food image,Foodness},
pages = {3--17},
title = {{Automatic expansion of a food image dataset leveraging existing categories with domain adaptation}},
volume = {8927},
year = {2015}
}
@misc{DeFreitas2015,
annote = {Sehr gute Einf{\"{u}}hrung},
author = {de Freitas, Nando},
keywords = {AI,Artificial Intelligence (Field Of Study),convolutional networks,deep learning,machine learning,nando de freitas,neural networks},
title = {{Deep Learning Lecture 1: Introduction}},
url = {https://www.youtube.com/watch?v=PlhFWT7vAEw},
urldate = {2015-11-06},
year = {2015}
}
@article{Khanna2010,
abstract = {In this paper, we describe the Technology Assisted Dietary Assessment (TADA) project at Purdue University. Dietary intake, what someone eats during the course of a day, provides valuable insights for mounting intervention programs for prevention of many chronic diseases such as obesity and cancer. Accurate methods and tools to assess food and nutrient intake are essential for research on the association between diet and health. An overview of our methods used in the TADA project is presented. Our approach includes the use of image analysis tools for identification and quantification of food that is consumed at a meal. Images obtained before and after foods are eaten are used to estimate the amount and type of food consumed.},
author = {Khanna, Nitin and Boushey, Carol J. and Kerr, Deborah and Okos, Martin and Ebert, David S. and Delp, Edward J.},
doi = {10.1109/ISM.2010.50},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Khanna et al. - 2010 - An overview of the Technology Assisted Dietary Assessment project at Purdue University.pdf:pdf},
isbn = {9780769542171},
journal = {Proceedings - 2010 IEEE International Symposium on Multimedia, ISM 2010},
keywords = {Classification,Diet record method,Dietary assessment,Feature extraction,Image texture,Mobile device,Mobile telephone,Pattern recognition,Volume estimation},
pages = {290--295},
pmid = {22020443},
title = {{An overview of the Technology Assisted Dietary Assessment project at Purdue University}},
year = {2010}
}
@article{Ponrani2014,
abstract = {Now a day's obesity is a major problem in human life. So the people are very eager to measuring their weight, healthy eating and also avoiding obesity, so they were need a system to measure the calorie and nutrition from the daily in taking food. A new System is proposed to measure the calorie and nutrition from the food image and it helps patients and dieticians for managing their daily in taking food. In this generation the usage of Personal mobile technology such as smart phones or tablets usage has been increased and also the users can carry with them periodically all the time, so by using a built-in camera of mobile the snapshot of food is taken to measure the consumption of calorie and nutrient components. The results have acceptable accuracy in calorie measurement technique can be done by using MATLAB.},
author = {Ponrani, D Seles and Suveka, S Nirmal and Brabha, S Kiran},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Ponrani, Suveka, Brabha - 2014 - Performance Analysis of SVM to Measure Calorie and Nutrition from Food Images.pdf:pdf},
journal = {International Journal of Advanced Research Trends in Engineering and Technology},
keywords = {Calorie and Nutrition measurement,obesity management},
number = {3},
pages = {93--98},
title = {{Performance Analysis of SVM to Measure Calorie and Nutrition from Food Images}},
volume = {1},
year = {2014}
}
@article{Le2011,
abstract = {We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8{\%} accu- racy in recognizing 20,000 object categories from ImageNet, a leap of 70{\%} relative im- provement over the previous state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1112.6209},
author = {Le, Quoc V and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S and Dean, Jeff and Ng, Andrew Y},
doi = {10.1109/MSP.2011.940881},
eprint = {1112.6209},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Le et al. - 2011 - Building high-level features using large scale unsupervised learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {10535888},
journal = {International Conference in Machine Learning},
keywords = {deep learning,unsupervised learning},
pages = {38115},
title = {{Building high-level features using large scale unsupervised learning}},
url = {http://arxiv.org/abs/1112.6209},
year = {2011}
}
@article{Batuwita2013,
abstract = {Support Vector Machines is a very popular machine learning technique. De- spite of all its theoretical and practical advantages, SVMs could produce sub- optimal results with imbalanced datasets. That is, an SVM classifier trained on an imbalanced dataset can produce suboptimal models which are biased towards the majority class and have low performance on the minority class, like most of the other classification paradigms. There have been various data preprocessing and algorithmic techniques proposed in the literature to allevi- ate this problem for SVMs. This chapter aims to review these techniques.},
author = {Batuwita, Rukshan and Palade, Vasile},
doi = {10.1002/9781118646106},
isbn = {9781118074626},
journal = {Imbalanced Learning: Foundations, Algorithms, Applications},
pages = {83--100},
title = {{Class Imbalance Learning Methods for Support Vector}},
year = {2013}
}
@article{Nguyen2014,
abstract = {This paper proposes food image classification methods exploiting both local appearance and global structural information of food objects. The contribution of the paper is threefold. First, non-redundant local binary pattern (NRLBP) is used to describe the local appearance information of food objects. Second, the structural information of food objects is represented by the spatial relationship between interest points and encoded using a shape context descriptor formed from those interest points. Third, we propose two methods of integrating appearance and structural information for the description and classification of food images. We evaluated the proposed methods on two datasets. Experimental results verified that the combination of local appearance and structural features can improve classification performance.},
author = {Nguyen, Duc Thanh and Zong, Zhimin and Ogunbona, Philip O. and Probst, Yasmine and Li, Wanqing},
doi = {10.1016/j.neucom.2014.03.017},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2014 - Food image classification using local appearance and global structural information.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Food image classification,Local binary pattern,Non-redundant local binary pattern,Shape context},
month = {sep},
pages = {242--251},
title = {{Food image classification using local appearance and global structural information}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231214004317},
volume = {140},
year = {2014}
}
@article{Allamanis2014,
abstract = {Every programmer has a characteristic style, ranging from preferences about identifier naming to preferences about object relationships and design patterns. Coding conventions define a consistent syntactic style, fostering readability and hence maintainability. When collaborating, programmers strive to obey a project{\&}{\#}8217;s coding conventions. However, one third of reviews of changes contain feedback about coding conventions, indicating that programmers do not always follow them and that project members care deeply about adherence. Unfortunately, programmers are often unaware of coding conventions because inferring them requires a global view, one that aggregates the many local decisions programmers make and identifies emergent consensus on style. We present NATURALIZE, a framework that learns the style of a codebase, and suggests revisions to improve stylistic consistency. NATURALIZE builds on recent work in applying statistical natural language processing to source code. We apply NATURALIZE to suggest natural identifier names and formatting conventions. We present four tools focused on ensuring natural code during development and release management, including code review. NATURALIZE achieves 94 {\%} accuracy in its top suggestions for identifier names. We used NATURALIZE to generate 18 patches for 5 open source projects: 14 were accepted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.4182v3},
author = {Allamanis, Miltiadis and Barr, Earl T and Bird, Christian and Sutton, Charles},
doi = {10.1145/2635868.2635883},
eprint = {arXiv:1402.4182v3},
isbn = {978-1-4503-3056-5},
journal = {Fse},
keywords = {Coding conventions,naturalness of software},
pages = {281--293},
title = {{Learning Natural Coding Conventions}},
url = {http://doi.acm.org/10.1145/2635868.2635883},
year = {2014}
}
@article{Bowman2015,
abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
archivePrefix = {arXiv},
arxivId = {1511.06349},
author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
eprint = {1511.06349},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:pdf},
month = {nov},
title = {{Generating Sentences from a Continuous Space}},
url = {http://arxiv.org/abs/1511.06349},
year = {2015}
}
@phdthesis{Baxter,
abstract = {Food recognition is a difficult problem, because unlike objects like cars, faces, or pedestrians, food is deformable and exhibits high intra-class variation. This paper con- siders the approach of analyzing a food item at the pixel- level by classifying each pixel as a certain ingredient, and then using statistics and spatial relationships between those pixel ingredient labels as features in an SVM classifier. We experimented with multiple variations on past methods, and found that using pixel ingredient labels to identify food greatly increases classification accuracy, but at the expense of higher computational cost.},
author = {Baxter, Jay},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Baxter - Unknown - Food Recognition using Ingredient-Level Features.pdf:pdf},
keywords = {PFID,pairwise statistics of local features},
mendeley-tags = {PFID,pairwise statistics of local features},
school = {Massachusetts Institute of Technology},
title = {{Food Recognition using Ingredient-Level Features}}
}
@misc{ReboucasdeOliveira2012,
abstract = {The present invention relates to a method for automatic food recognition by means of portable devices equipped with digital cameras. With this system, it is possible to identify a previously established food menu. To this purpose, a semi-automated method of segmentation is applied to delineate the regions in which each type of food in an image of a plate of food, captured by a user. Pattern recognition techniques are used in images, integrated into a system whose goal is to label each type of food contained in the photo of a plate of food. No type of preprocessing is performed to correct deficiencies in capturing the image, just using the auto-focus component present in the portable device to capture a clear image.},
annote = {- Erkennung mit Color und Texture.

- Segmentierung mit Color

- Klassifizierung mit SVMs (radial based kernel-type function)
- Multithreading auf Handy.
- "To prevent the image from the plate of being captured in a distorted or unfocused, auto-focus of the device is triggered, preventing this problem." {\textless}- Wahnsinn. Was f{\"{u}}r eine gute Idee},
author = {{Reboucas de Oliveira}, Luciano and {Manuel de Freitas Jorge}, Eduardo and {Almeida de Azevedo Filho}, Alberto and {Franco Costa}, Victor},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Reboucas de Oliveira et al. - 2012 - System for Food Recognition Method Using Portable Devices Having Digital Cameras.pdf:pdf},
keywords = {Gaussian difference,coefficient of spatial variation},
mendeley-tags = {Gaussian difference,coefficient of spatial variation},
month = {jul},
title = {{System for Food Recognition Method Using Portable Devices Having Digital Cameras}},
url = {http://www.google.com/patents/US20120170801},
year = {2012}
}
@article{Wang2016,
abstract = {Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7{\%} in precision, 11.5{\%} in recall, and 14.2{\%} in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9{\%} in F1. {\&}copy; 2016 ACM.},
author = {Wang, Song and Liu, Taiyue and Tan, Lin},
doi = {10.1145/2884781.2884804},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Liu, Tan - 2016 - Automatically learning semantic features for defect prediction.pdf:pdf},
isbn = {9781450339001},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Artificial intelligence;Defects;Forecasting;Learni},
pages = {297--308},
title = {{Automatically learning semantic features for defect prediction}},
url = {http://dx.doi.org/10.1145/2884781.2884804},
volume = {14-22-May-},
year = {2016}
}
@article{Falkenauer1998,
author = {Falkenauer, E},
issn = {13811231},
journal = {Journal of Heuristics},
keywords = {benchmarking,general and targeted methods,method applicability,optimization,overfitting},
pages = {1--6},
title = {{On method overfitting}},
url = {http://link.springer.com/article/10.1023/A:1009617801681},
volume = {287},
year = {1998}
}
@misc{Lowe2004a,
abstract = {A method and apparatus for identifying scale invariant features in an image and a further method and apparatus for using such scale invariant features to locate an object in an image are disclosed. The method and apparatus for identifying scale invariant features may involve the use of a processor circuit for producing a plurality of component subregion descriptors for each subregion of a pixel region about pixel amplitude extrema in a plurality of difference images produced from the image. This may involve producing a plurality of difference images by blurring an initial image to produce a blurred image and by subtracting the blurred image from the initial image to produce the difference image. For each difference image, pixel amplitude extrema are located and a corresponding pixel region is defined about each pixel amplitude extremum. Each pixel region is divided into subregions and a plurality of component subregion descriptors are produced for each subregion. These component subregion descriptors are correlated with component subregion descriptors of an image under consideration and an object is indicated as being detected when a sufficient number of component subregion descriptors (scale invariant features) define an aggregate correlation exceeding a threshold correlation with component subregion descriptors (scale invariant features) associated with the object.},
author = {Lowe, David G.},
number = {12},
title = {{Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image}},
volume = {1},
year = {2004}
}
@book{Theodoridis2009c,
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50022-0},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(20).pdf:pdf},
isbn = {9781597492720},
keywords = {Appendix},
mendeley-tags = {Appendix},
pages = {946--948},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500220},
year = {2009}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G.},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Lowe - 2004 - Distinctive image features from scale-invariant keypoints.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {Image matching,Invariant features,Object recognition,Scale invariance},
number = {2},
pages = {91--110},
title = {{Distinctive image features from scale-invariant keypoints}},
volume = {60},
year = {2004}
}
@article{Rosten2006,
abstract = {Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7{\%} of the available processing time. By comparison neither the Harris detector (120{\%}) nor the detection stage of SIFT (300{\%}) can operate at full frame rate. Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.},
author = {Rosten, Edward and Drummond, Tom},
doi = {10.1007/11744023_34},
isbn = {3540338322},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {430--443},
pmid = {18684738},
title = {{Machine learning for high-speed corner detection}},
volume = {3951 LNCS},
year = {2006}
}
@article{Gujjar2014,
abstract = {This paper deals with the classification of bulk food grain samples and detection of foreign bodies in food grains. A new method for inspecting food samples is presented, using ANN and segmentation to classify grain samples and detect foreign bodies that are not detectable using conventional methods easily. A BPNN based classifier is designed to classify the unknown grain samples. The algorithms are developed to extract color, texture and combined features are extracted from grains and after normalization presented to neural network for training purpose. The trained network is then used to identify the unknown grain type and it's quality in terms of pure/impure type. A Segmentation based detection model is developed to detect the foreign body in the impure grain samples. This model accepts an impure grain samples, pre-processes and segments the image using two different thresholds T1 and T2 to detect the foreign body in impure image. Finally the success rates are observed from both classification and foreign body detection models and are recorded.},
author = {Gujjar, Harish S and Siddappa, M.},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Gujjar, Siddappa - 2014 - Recognition and Classification of Different Types of Food Grains and Detection of Foreign Bodies using Neural.pdf:pdf;:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Gujjar, Siddappa - 2014 - Recognition and Classification of Different Types of Food Grains and Detection of Foreign Bodies using Neur(2).pdf:pdf},
journal = {IJCA Proceedings on International Conference on Information and Communication Technologies},
month = {aug},
number = {1},
pages = {12--17},
publisher = {Foundation of Computer Science (FCS)},
title = {{Recognition and Classification of Different Types of Food Grains and Detection of Foreign Bodies using Neural Networks}},
url = {http://www.ijcaonline.org/proceedings/icict/number1/17959-1403},
volume = {ICICT},
year = {2014}
}
@book{Theodoridis2009u,
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50002-5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition.pdf:pdf},
isbn = {9781597492720},
pages = {xv--xvii},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500025},
year = {2009}
}
@article{Nagappan2006,
abstract = {What is it that makes software fail? In an empirical study of the post-release defect history of five Microsoft software systems, we found that failure-prone software entities are statistically correlated with code complexity measures. However, there is no single set of complexity metrics that could act as a universally best defect predictor. Using principal component analysis on the code metrics, we built regression models that accurately predict the likelihood of post-release defects for new entities. The approach can easily be generalized to arbitrary projects; in particular, predictors obtained from one project can also be significant for new, similar projects.},
author = {Nagappan, Nachiappan and Ball, Thomas and Zeller, Andreas},
doi = {10.1145/1134285.1134349},
isbn = {1-59593-375-1},
issn = {02705257},
journal = {Proceedings of the 28th international conference on Software engineering},
keywords = {bug database,complexity metrics,empirical study,principal component analysis,regression model},
pages = {452--461},
title = {{Mining metrics to predict component failures}},
url = {http://wwwipd.ira.uka.de/Tichy/uploads/folien/122/nagappanICSE06.pdf},
year = {2006}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge(2).pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://dx.doi.org/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@article{Surya2015,
abstract = {Obesity is the major cause of overweight this leads to the type II diabetes, heart disease and cancer. Measuring the food is very important for a successful healthy diet. Measuring calorie and nutrition in daily food is one of the challenge methods. Smartphone plays a vital role in today's technological world using this technique will enhance the issue in intake of dietary consumption .In this project an food image recognition system for measuring the calorie and nutrition values was developed .the user has to take the picture of the food image this system will classify the image to detect the type of food and portion size and the recognition information will estimate the number of calories in the food. In this system the food area, size and volume will be used to calculate the calorie and nutrition in accurate way.},
annote = {what a bad paper},
author = {Surya, R and Priya, S Saru},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Surya, Priya - 2015 - Food Image Recognition Using Svm Classifier for Measuring Calorie and Nutrition Values.pdf:pdf},
journal = {International Journal of Scientific {\&} Engineering Research},
keywords = {Feature extraction.,Food image,Pre-processing,Support vector machine (SVM),and,announcement cost,feature extraction,food image,image recognition smartphone is,in terms of obtainability,interruption,much more capable,other hand,pre-processing,support vector machine,svm},
number = {4},
pages = {324--328},
title = {{Food Image Recognition Using Svm Classifier for Measuring Calorie and Nutrition Values}},
url = {http://www.ijser.org/researchpaper{\%}5CFood-Image-Recognition-Using-Svm-Classifier-for-Measuring-Calorie-and-Nutrition-Values.pdf},
volume = {6},
year = {2015}
}
@article{Boehm1988a,
abstract = {A discussion is presented of the two primary ways of understanding$\backslash$nsoftware costs. The black-box or influence-function approach provides$\backslash$nuseful experimental and observational insights on the relative software$\backslash$nproductivity and quality leverage of various management, technical,$\backslash$nenvironmental, and personnel options. The glass-box or cost distribution$\backslash$napproach helps identify strategies for integrated software productivity$\backslash$nand quality improvement programs using such structures as the value$\backslash$nchain and the software productivity opportunity tree. The individual$\backslash$nstrategies for improving software productivity are identified. Issues$\backslash$nrelated to software costs and controlling them are examined and$\backslash$ndiscussed. It is pointed out that a good framework of techniques exists$\backslash$nfor controlling software budgets, schedules, and work completed, but$\backslash$nthat a great deal of further progress is needed to provide an overall$\backslash$nset of planning and control techniques covering software product$\backslash$nqualities and end-user system objectives},
author = {Boehm, Barry W. and Papaccio, Philip N.},
doi = {10.1109/32.6191},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Boehm, Papaccio - 1988 - Understanding and Controlling Software Costs(2).pdf:pdf},
isbn = {0098-558},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Programming productivity,software costs,software engineering economics,software management,software metrics,software productivity},
number = {10},
pages = {1462--1477},
title = {{Understanding and Controlling Software Costs}},
volume = {14},
year = {1988}
}
@techreport{Conneau2018,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an un-supervised manner on large corpora, as base features. Efforts to obtain embed-dings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsu-pervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsu-pervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features , which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available 1 .},
archivePrefix = {arXiv},
arxivId = {1705.02364v5},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and {Lo¨ıc Barrault}, Lo¨ıc and Bordes, Antoine},
eprint = {1705.02364v5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Conneau et al. - 2018 - Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.pdf:pdf},
keywords = {Infersent,LSTM,NLI,Natural language inference,facebook,sentence embeddings,sentence encoding,supervised learning},
mendeley-tags = {Infersent,LSTM,NLI,Natural language inference,facebook,sentence embeddings,sentence encoding,supervised learning},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
url = {https://www.github.com/facebookresearch/InferSent},
year = {2018}
}
@inproceedings{Vaswani2017b,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
annote = {Summary

Instead of RNN approach uses self attention to focus on important words


state of the art (is used in google translate (see https://youtu.be/rBCqOTEfxvg?t=1575 and https://translate.google.de/{\#}en/fr/The{\%}20city{\%}20councilmen{\%}20refused{\%}20the{\%}20female{\%}20demonstrators{\%}20a{\%}20permit{\%}20because{\%}20they{\%}20feared{\%}20violence) -{\textgreater} makes the same mistakes

training time: 3.5 days on 8 GPUs
state of the art translation 12h on 8 P100 GPUs (eq 7 days on one 1080Ti)

Problem with CNNs:
https://youtu.be/rBCqOTEfxvg?t=578

It's very positional. The last layer does not see every word but instead sees a subset of the sentence. For example word 13, 15, ... 
How do we make sure, that word 13 is actually important. It could also be word 12 that is important. This scheme is referencing by position while referencing by content would be much more natural

With attention one can look in the past and focus on the important parts. With CNNs this is also possible but it requires that you get the exact position you need by chance},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Advances in neural information processing systems},
doi = {10.1017/S0952523813000308},
eprint = {1706.03762},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
isbn = {1469-8714},
issn = {1469-8714},
keywords = {attention},
mendeley-tags = {attention},
month = {jun},
pages = {5998--6008},
pmid = {24016424},
title = {{Attention Is All You Need}},
url = {https://arxiv.org/abs/1706.03762 http://arxiv.org/abs/1706.03762},
year = {2017}
}
@article{Pham2013,
abstract = {We describe FoodBoard, an instrumented chopping board that uses optical fibers and embedded camera imaging to identify unpackaged ingredients during food preparation on its surface. By embedding the sensing directly, and robust- ly, in the surface of a chopping board we also demonstrate how surface contact optical sensing can be used to realize the portability and privacy required of technology used in a setting such as a domestic kitchen. FoodBoard was subject- ed to a close to real-world evaluation in which 12 users prepared actual meals. FoodBoard compared favourably with existing unpackaged food recognition systems, classi- fying a larger number of distinct food ingredients (12 incl. meat, fruit, vegetables) with an average accuracy of 82.8{\%}.},
author = {Pham, Cuong and Jackson, Daniel and Sch{\"{o}}ning, Johannes and Bartindale, Tom and Plotz, Thomas and Olivier, Patrick},
doi = {10.1145/2493432.2493522},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pham et al. - 2013 - FoodBoard surface contact imaging for food recognition.pdf:pdf},
isbn = {9781450317702},
journal = {Proceedings of the {\ldots}},
keywords = {Food Recognition,H.1.2 User/Machine Systems I.5 Pattern Recognition,Sensing Surfaces,UbiComp},
mendeley-tags = {H.1.2 User/Machine Systems I.5 Pattern Recognition},
pages = {749--752},
title = {{FoodBoard: surface contact imaging for food recognition}},
url = {http://dl.acm.org/citation.cfm?id=2493522},
year = {2013}
}
@misc{Kim2014,
abstract = {Recognition of food images is emerging as an impotant research topic in image classification problem because of the demand for dietary assessment tools. In this paper, we propose an automatic food image recognition system for 100 food categories by fusing various kinds of handcrafted image features including bag-of-features (BoF) and Fisher Vectors with SIFT and HOG descriptors. In addition, we used extracted features from Deep Convolutional Neural Network. Our experiments achieved 40.34{\%} as the top 1 accuracy and 75.34{\%} as the top 5 accuracy for the 100 class food image dataset, UEC-FOOD100, which out- performs the best classification accuracy of this dataset reported so far, 72.26{\%}, greatly.},
annote = {GitHub Repository CNN -{\textgreater} https://github.com/carpedm20/FoodClassifier},
author = {Kim, Taehoon},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kim - 2014 - Food Image Recognition Boosting with Shallow Handcrafted Feature and Deep Convolutional Feature.pdf:pdf},
keywords = {deep convolutional,fisher vector,food recognition},
title = {{Food Image Recognition : Boosting with Shallow Handcrafted Feature and Deep Convolutional Feature}},
url = {https://drive.google.com/file/d/0ByTS2HBKYvZxeHNhbUN1UkhGWjd2RTJYRkphb3dkSjVBbjJn/view https://github.com/carpedm20/FoodClassifier https://github.com/carpedm20/FoodClas},
year = {2014}
}
@article{Matsuda2013,
abstract = {In this paper, we propose a method to recog- nize food images which include multiple food items considering co-occurrence statistics of food items. Theproposedmethodemploys amanifold rank- ing method which has been applied to image re- trieval successfully in the literature. In the exper- iments, we prepared co-occurrence matrices of 100 food items using various kinds of data sources in- cludingWeb texts, Web food blogs and our own food database, and evaluated the final results obtained by applying manifold ranking. As results, it has been proved that co-occurrence statistics obtained from a food photo database is very helpful to improve the classification rate within the top ten candidates.},
author = {Matsuda, Yuji and Yanai, Keiji},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Matsuda, Yanai - 2013 - Multiple-Food Image Recognition Considering Co-occurrence.pdf:pdf},
isbn = {9784990644116},
journal = {International Conference on Pattern Recognition},
number = {Icpr},
pages = {1724--1730},
title = {{Multiple-Food Image Recognition Considering Co-occurrence}},
year = {2013}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G.},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Lowe - 1999 - Object recognition from local scale-invariant features.pdf:pdf},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
number = {[8},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
publisher = {IEEE},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
volume = {2},
year = {1999}
}
@book{Theodoridis2009e,
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50021-9},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(19).pdf:pdf},
isbn = {9781597492720},
keywords = {appendix},
mendeley-tags = {appendix},
pages = {930--945},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500219},
year = {2009}
}
@book{Theodoridis2009r,
abstract = {This chapter presents clustering algorithms based on different ideas, which cannot be grouped under a single philosophy. The chapter discuses clustering algorithms based on graph theory concepts, such as the minimum spanning tree, the directed tree, and spectral clustering. The second category of competitive learning algorithms is also explained. The third category includes branch and bound algorithms, and guarantees to provide globally optimal clustering, in terms of a prespecified optimality criterion, at the cost of increased computational requirements. The fourth category discussed in the chapter contains algorithms that are based on morphological transformations. The fifth category contains algorithms that are not based on cluster representatives but, instead, seek to place boundaries between clusters. Algorithms of the sixth category treat clusters as dense in data regions of the feature space separated by regions sparse in data. The seventh category includes additional algorithms that are based on function optimization, such as simulated annealing and deterministic annealing. This category also includes genetic algorithms modified suitably for clustering tasks. The final eighth category of algorithms discussed in this chapter includes algorithms that combine clustering in order to produce a final one.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50017-7},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(15).pdf:pdf},
isbn = {9781597492720},
pages = {765--862},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500177},
year = {2009}
}
@article{Foroni2013,
abstract = {In recent years we have witnessed an increasing interest in food processing and eating behaviors. This is probably due to several reasons. The biological relevance of food choices, the complexity of the food-rich environment in which we presently live (making food-intake regulation difficult), and the increasing health care cost due to illness associated with food (food hazards, food contamination, and aberrant food-intake). Despite the importance of the issues and the relevance of this research, comprehensive and validated databases of stimuli are rather limited, outdated, or not available for non-commercial purposes to independent researchers who aim at developing their own research program. The FoodCast Research Image Database (FRIDa) we present here includes 877 images belonging to eight different categories: natural-food (e.g., strawberry), transformed-food (e.g., french fries), rotten-food (e.g., moldy banana), natural-non-food items (e.g., pinecone), artificial food-related objects (e.g., teacup), artificial objects (e.g., guitar), animals (e.g., camel), and scenes (e.g., airport). FRIDa has been validated on a sample of healthy participants (N = 73) on standard variables (e.g., valence, familiarity, etc.) as well as on other variables specifically related to food items (e.g., perceived calorie content); it also includes data on the visual features of the stimuli (e.g., brightness, high frequency power, etc.). FRIDa is a well-controlled, flexible, validated, and freely available (http://foodcast.sissa.it/neuroscience/) tool for researchers in a wide range of academic fields and industry.},
author = {Foroni, Francesco and Pergola, Giulio and Argiris, Georgette and Rumiati, Raffaella I},
doi = {10.3389/fnhum.2013.00051},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Foroni et al. - 2013 - The FoodCast research image database (FRIDa).pdf:pdf},
isbn = {1662-5161 (Electronic)$\backslash$r1662-5161 (Linking)},
issn = {1662-5161},
journal = {Frontiers in human neuroscience},
keywords = {and to avoid poisoned,category specificity,database,food,food processing,frida,humans need to process,information about,like other animals,or uneat-,possible sources of nutrition,the foodcast research image,validated images database},
number = {March},
pages = {51},
pmid = {23459781},
title = {{The FoodCast research image database (FRIDa).}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3585434{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2013}
}
@inproceedings{Beijbom2015,
abstract = {Logging food and calorie intake has been shown to facilitate weight management. Unfortunately, current food logging methods are time-consuming and cumbersome, which limits their effectiveness. To address this limitation, we present an automated computer vision system for logging food and calorie intake using images. We focus on the "restaurant" scenario, which is often a challenging aspect of diet management. We introduce a key insight that addresses this problem specifically: restaurant plates are often both nutritionally and visually consistent across many servings. This insight provides a path to robust calorie estimation from a single RGB photograph: using a database of known food items together with restaurant-specific classifiers, calorie estimation can be achieved through identification followed by calorie lookup. As demonstrated on a challenging Menu-Match dataset and an existing third party dataset, our approach outperforms previous computer vision methods and a commercial calorie estimation app. Our Menu-Match dataset of realistic restaurant meals is made publicly available.},
author = {Beijbom, Oscar and Joshi, Neel and Morris, Dan and Saponas, Scott and Khullar, Siddharth},
booktitle = {2015 IEEE Winter Conference on Applications of Computer Vision},
doi = {10.1109/WACV.2015.117},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Beijbom et al. - 2015 - Menu-Match Restaurant-Specific Food Logging from Images.pdf:pdf},
isbn = {978-1-4799-6683-7},
keywords = {Computer vision,Databases,Estimation,Feature extraction,Image color analysis,Menu-Match dataset,Standards,Visualization,automated computer vision system,calorie estimation,calorie intake logging,catering industry,computer vision,diet management,image classification,image colour analysis,restaurant meals,restaurant plates,restaurant-specific classifiers,restaurant-specific food logging,single RGB photograph},
month = {jan},
pages = {844--851},
publisher = {IEEE},
shorttitle = {Applications of Computer Vision (WACV), 2015 IEEE},
title = {{Menu-Match: Restaurant-Specific Food Logging from Images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7045971},
year = {2015}
}
@misc{Tamrakar2013,
abstract = {A computer-implemented method for estimating a volume of at least one food item on a food plate is disclosed. A first and second plurality of images are received from different positions above a food plate, wherein angular spacing between the positions of the first plurality of images is greater than angular spacing between the positions of the second plurality of images. A first set of poses of each of the first plurality of images is estimated. A second set of poses of each of the second plurality of images is estimated based on at least the first set of poses. A pair of images taken from each of the first and second plurality of images is rectified based on at least the first and second set of poses. A 3D point cloud is reconstructed based on at least the rectified pair of images. At least one surface of the at least one food item above the food plate is estimated based on at least the reconstructed 3D point cloud. The volume of the at least one food item is estimated based on the at least one surface.},
annote = {- Erkennung des Volumens anhand von Stereo Bildern, die mittels Video erkannt werden.{\textless}m:linebreak/{\textgreater}{\textless}m:linebreak/{\textgreater}- Ansonsten alles Standard-Kost (Feature Extraction SVMs)},
author = {Tamrakar, Amir and Sawhney, Harpreet Singh and {Yu, Qian, Divakaran}, Ajay},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Tamrakar, Sawhney, Yu, Qian, Divakaran - 2013 - Method for computing food volume in a method for analyzing food.pdf:pdf},
keywords = {SFIT,SRI,Stereo Images,Volume Estimation,k-Nearest-Neighbors,k-means},
mendeley-tags = {SFIT,SRI,Stereo Images,Volume Estimation,k-Nearest-Neighbors,k-means},
month = {jan},
title = {{Method for computing food volume in a method for analyzing food}},
url = {https://www.google.com.ar/patents/US8345930},
year = {2013}
}
@incollection{Shimoda2015,
abstract = {We propose a CNN-based food image segmentation which requires no pixel-wise annotation. The proposed method consists of food region proposals by selective search and bounding box clustering, back propagation based saliency map estimation with the CNN model fine-tuned with the UEC-FOOD100 dataset, GrabCut guided by the estimated saliency maps and region integration by non-maximum suppression. In the experiments, the proposed method outperformed RCNN regarding food region detection as well as the PASCAL VOC detection task.},
address = {Cham},
author = {Shimoda, Wataru and Yanai, Keiji},
booktitle = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
doi = {10.1007/978-3-319-23222-5},
editor = {Murino, Vittorio and Puppo, Enrico and Sona, Diego and Cristani, Marco and Sansone, Carlo},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Shimoda, Yanai - 2015 - CNN-based Food Image Segmentation without Pixel-Wise Annotation.pdf:pdf;:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Shimoda, Yanai - 2015 - CNN-based Food Image Segmentation without Pixel-Wise Annotation(2).pdf:pdf},
isbn = {978-3-319-23221-8},
keywords = {Food segmentation Convolutional neural network Dee},
mendeley-tags = {Food segmentation Convolutional neural network Dee},
pages = {449--457},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{CNN-based Food Image Segmentation without Pixel-Wise Annotation}},
url = {http://link.springer.com/10.1007/978-3-319-23222-5},
volume = {9281},
year = {2015}
}
@inproceedings{Pouladzadeh2012,
abstract = {Emerging food classification methods play an important role in nowadays food recognition applications. For this purpose, a new recognition algorithm for food is presented, considering its shape, color, size, and texture characteristics. Using various combinations of these features, a better classification will be achieved. Based on our simulation results, the proposed algorithm recognizes food categories with an approval recognition rate of 92.6{\%}, in average.},
author = {Pouladzadeh, Parisa and Villalobos, Gregorio and Almaghrabi, Rana and Shirmohammadi, Shervin},
booktitle = {2012 IEEE International Conference on Multimedia and Expo Workshops},
doi = {10.1109/ICMEW.2012.92},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pouladzadeh et al. - 2012 - A Novel SVM Based Food Recognition Method for Calorie Measurement Applications.pdf:pdf},
isbn = {978-1-4673-2027-6},
keywords = {Calories measurement,Feature extraction,Food recognition,Image color analysis,Image segmentation,SVM based food recognition,Shape,Support vector machine (SVM),Support vector machines,Thumb,Training,calorie measurement applications,classification method,color,color characteristics,food recognition applications,image colour analysis,image recognition,image texture,shape characteristics,size and texture detection,size characteristics,support vector machines,texture characteristics},
month = {jul},
pages = {495--498},
publisher = {IEEE},
shorttitle = {Multimedia and Expo Workshops (ICMEW), 2012 IEEE I},
title = {{A Novel SVM Based Food Recognition Method for Calorie Measurement Applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6266433},
year = {2012}
}
@inproceedings{Bhagyalaksluni2014a,
author = {Bhagyalaksluni, Ivis A},
booktitle = {2014 IEEE International Conference on Computer Communication and Systems(ICCCS '14)},
isbn = {9781479936717},
keywords = {content based image retrieval,local binary patterns,local derivative,local ternary patterns,local tetra patterns,patterns},
pages = {18--23},
title = {{A Survey on Content Based Image Retrieval Using Various Operators}},
year = {2014}
}
@book{Theodoridis2009j,
abstract = {This chapter presents basic concepts and definitions related to clustering. The various types of data encountered in clustering applications are reviewed. The chapter explains the basic steps that an expert must follow in order to develop a clustering task. Clustering is one of the most primitive mental activities of humans, used to handle the huge amount of information they receive every day. Humans tend to categorize different entities into clusters. Each cluster is then characterized by the common attributes of the entities it contains. Clustering may be found under different names in different contexts, such as unsupervised learning and learning without a teacher (in pattern recognition), numerical taxonomy (in biology, ecology), typology (in social sciences), and partition (in graph theory). In the chapter, examples are presented to show that the process of assigning objects to clusters may lead to very different results depending on the specific criterion used for clustering. The chapter explains the concepts of proximity measures between two points and the proximity functions between two sets. Proximity measures that are commonly encountered in various applications are also discussed.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50013-X},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(11).pdf:pdf},
isbn = {9781597492720},
pages = {595--625},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B978159749272050013X},
year = {2009}
}
@inproceedings{Bettadapura2015,
author = {Bettadapura, Vinay and Thomaz, Edison and Parnami, Aman and Abowd, Gregory D. and Essa, Irfan},
booktitle = {2015 IEEE Winter Conference on Applications of Computer Vision},
doi = {10.1109/WACV.2015.83},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bettadapura et al. - 2015 - Leveraging Context to Support Automated Food Recognition in Restaurants.pdf:pdf},
isbn = {978-1-4799-6683-7},
keywords = {Cameras,Feature extraction,Google,Image color analysis,Image recognition,Image segmentation,Training,automated food recognition,classifier training,computer vision,computer vision techniques,food journaling automation,food photos,image classification,image-based recognition,mobile camera pervasiveness,mobile computing,object recognition,performance evaluation,restaurant online menu databases,restaurants},
language = {English},
month = {jan},
pages = {580--587},
publisher = {IEEE},
title = {{Leveraging Context to Support Automated Food Recognition in Restaurants}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7045937 http://www.cc.gatech.edu/{~}irfan/p/2015-Bettadapura-LCSAFRR.pdf},
year = {2015}
}
@article{Kloft2011,
author = {Kloft, Marius and Brefeld, Ulf and Sonnenburg, S{\"{o}}ren and Zien, Alexander},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kloft et al. - 2011 - l p -Norm Multiple Kernel Learning.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
month = {feb},
pages = {953--997},
publisher = {JMLR.org},
title = {{l p -Norm Multiple Kernel Learning}},
url = {http://dl.acm.org/citation.cfm?id=1953048.2021033},
volume = {12},
year = {2011}
}
@article{Prabu2015,
abstract = {Computer vision-based food recognition could be used to estimate a meal's carbohydrate content for diabetic patients. This study proposes a methodology for automatic food recognition, based on the bag-of-features (BoF) model,GLCM and LBP features. Moreover, the enhancement of the visual dataset with more images will improve the classification rates, especially for the classes with high diversity. The final system will additionally include a food segmentation stage before applying the proposed recognition module, so that images with multiple food types can also be addressed. The optimized system computes dense local features, using the scaleinvariant feature transform on the HSV color space and texture features and these extracted features are trained and classified using SVM classifier. The system achieved classification accuracy of the order of 90{\%}, thus proving the feasibility of the proposed approach in a very challenging image dataset.},
author = {Prabu, K Ganesh},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Prabu - 2015 - A Food Recognition System for Diabetic Patients using SVM Classifier.pdf:pdf},
journal = {International Journal of Advanced Technology in Engineering and Science},
keywords = {diabetic patients,optimized system,recognition module,texture features},
number = {2},
pages = {371--378},
title = {{A Food Recognition System for Diabetic Patients using SVM Classifier}},
url = {http://ijates.com/images/short{\_}pdf/1422098038{\_}401.pdf},
volume = {3},
year = {2015}
}
@phdthesis{Zhang,
abstract = {Ingredients are the core components that make a dish what it is, besides the preparation process. Using the idea of attribute-based classification, we seek to classify plates of food to the correct cuisine by the country, using the in- gredients as attributes for a plate of food. Because of the important role that ingredients have in any plate of food, this method can be generalized to any type of dishes with any type of ingredients, and can learn new dishes not seen in the training, as long as the ingredients can be speci- fied to fit an attribute descriptor. Our dataset came from online sources and includes three cuisines, each with two dishes represented by 76 images. Even though our dataset is limited, reasonable results and a mean accuracy of 82.9{\%} show that the method could be generalized to more cate- gories.},
author = {Zhang, Mabel Mengzi},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Zhang - Unknown - Identifying the Cuisine of a Plate of Food.pdf:pdf},
school = {University of California San Diego},
title = {{Identifying the Cuisine of a Plate of Food}}
}
@article{Zheng2010,
abstract = {Software defect predictors which classify the software modules into defect-prone and not-defect-prone classes are effective tools to maintain the high quality of software products. The early prediction of defect-proneness of the modules can allow software developers to allocate the limited resources on those defect-prone modules such that high quality software can be produced on time and within budget. In the process of software defect prediction, the misclassification of defect-prone modules generally incurs much higher cost than the misclassification of not-defect-prone ones. Most of the previously developed predication models do not consider this cost issue. In this paper, three cost-sensitive boosting algorithms are studied to boost neural networks for software defect prediction. The first algorithm based on threshold-moving tries to move the classification threshold towards the not-fault-prone modules such that more fault-prone modules can be classified correctly. The other two weight-updating based algorithms incorporate the misclassification costs into the weight-update rule of boosting procedure such that the algorithms boost more weights on the samples associated with misclassified defect-prone modules. The performances of the three algorithms are evaluated by using four datasets from NASA projects in terms of a singular measure, the Normalized Expected Cost of Misclassification (NECM). The experimental results suggest that threshold-moving is the best choice to build cost-sensitive software defect prediction models with boosted neural networks among the three algorithms studied, especially for the datasets from projects developed by object-oriented language. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Zheng, Jun},
doi = {10.1016/j.eswa.2009.12.056},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Zheng - 2010 - Cost-sensitive boosting neural networks for software defect prediction.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Adaboost,Cost-sensitive,Neural networks,Software defect},
number = {6},
pages = {4537--4543},
publisher = {Elsevier Ltd},
title = {{Cost-sensitive boosting neural networks for software defect prediction}},
url = {http://dx.doi.org/10.1016/j.eswa.2009.12.056},
volume = {37},
year = {2010}
}
@article{Blechert2014,
abstract = {Our current environment is characterized by the omnipresence of food cues. The sight and smell of real foods, but also graphically depictions of appetizing foods, can guide our eating behavior, for example, by eliciting food craving and influencing food choice. The relevance of visual food cues on human information processing has been demonstrated by a growing body of studies employing food images across the disciplines of psychology, medicine, and neuroscience. However, currently used food image sets vary considerably across laboratories and image characteristics (contrast, brightness, etc.) and food composition (calories, macronutrients, etc.) are often unspecified. These factors might have contributed to some of the inconsistencies of this research. To remedy this, we developed food-pics, a picture database comprising 568 food images and 315 non-food images along with detailed meta-data. A total of N = 1988 individuals with large variance in age and weight from German speaking countries and North America provided normative ratings of valence, arousal, palatability, desire to eat, recognizability and visual complexity. Furthermore, data on macronutrients (g), energy density (kcal), and physical image characteristics (color composition, contrast, brightness, size, complexity) are provided. The food-pics image database is freely available under the creative commons license with the hope that the set will facilitate standardization and comparability across studies and advance experimental research on the determinants of eating behavior.},
author = {Blechert, Jens and Meule, Adrian and Busch, Niko A. and Ohla, Kathrin},
doi = {10.3389/fpsyg.2014.00617},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Blechert et al. - 2014 - Food-pics An image database for experimental research on eating and appetite.pdf:pdf},
isbn = {1664-1078 (Electronic)},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {ERP,Eating behavior,FMRI,Food pictures,Food-cues,Image properties,Obesity,Standardized food images},
number = {JUN},
pages = {1--10},
pmid = {25009514},
title = {{Food-pics: An image database for experimental research on eating and appetite}},
volume = {5},
year = {2014}
}
@article{He2018a,
abstract = {Attention-based long short-term memory (LSTM) networks have proven to be useful in aspect-level sentiment classification. However, due to the difficulties in annotating aspect-level data, existing public datasets for this task are all relatively small, which largely limits the effectiveness of those neural models. In this paper, we explore two approaches that transfer knowledge from document- level data, which is much less expensive to obtain, to improve the performance of aspect-level sentiment classification. We demonstrate the effectiveness of our approaches on 4 public datasets from SemEval 2014, 2015, and 2016, and we show that attention-based LSTM benefits from document-level knowledge in multiple ways.},
archivePrefix = {arXiv},
arxivId = {1806.04346},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
eprint = {1806.04346},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2018 - Exploiting Document Knowledge for Aspect-level Sentiment Classification.pdf:pdf},
title = {{Exploiting Document Knowledge for Aspect-level Sentiment Classification}},
url = {http://arxiv.org/abs/1806.04346},
year = {2018}
}
@article{Krizhevsky2012,
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
eprint = {1102.0183},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - Imagenet classification with deep convolutional neural networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@article{Bacchelli2012,
abstract = {Emails related to the development of a software system contain information about design choices and issues encountered during the development process. Exploiting the knowledge embedded in emails with automatic tools is challenging, due to the unstructured, noisy, and mixed language nature of this communication medium. Natural language text is often not well-formed and is interleaved with languages with other syntaxes, such as code or stack traces. We present an approach to classify email content at line level. Our technique classifies email lines in five categories (i.e., text, junk, code, patch, and stack trace) to allow one to subsequently apply ad hoc analysis techniques for each category. We evaluated our approach on a statistically significant set of emails gathered from mailing lists of four unrelated open source systems.},
author = {Bacchelli, Alberto and {Dal Sasso}, Tommaso and D'Ambros, Marco and Lanza, Michele},
doi = {10.1109/ICSE.2012.6227177},
isbn = {9781467310673},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Emails,Empirical software engineering,Unstructured Data Mining},
pages = {375--385},
title = {{Content classification of development emails}},
year = {2012}
}
@misc{WHO,
abstract = {Obesity fact sheet from WHO providing key facts and information on causes, health consequences, double burden of disease, prevention, WHO response.},
author = {{World Health Organization}},
booktitle = {Fact sheet N°311},
publisher = {World Health Organization},
title = {{WHO | Obesity and overweight}},
url = {http://www.who.int/mediacentre/factsheets/fs311/en/},
urldate = {2016-03-30},
year = {2015}
}
@article{Schmidt1999,
author = {Schmidt, Douglas C.},
journal = {C++ Report},
pages = {1--10},
title = {{Wrapper facade: a structural pattern for encapsulated functions within classes}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.5282},
year = {1999}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
volume = {7},
year = {2015}
}
@article{Fei-Fei2005,
abstract = {We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a "theme". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.},
author = {Fei-Fei, Li and Perona, P.},
doi = {10.1109/CVPR.2005.16},
isbn = {0-7695-2372-2},
issn = {1063-6919},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
keywords = {Animals,Bag of Words,Bayesian hierarchical model,Bayesian methods,BoF,BoW,Cities and towns,Dictionaries,Frequency,Histograms,Humans,Layout,Unsupervised learning,Vehicles,belief networks,codeword distribution,image classification,image representation,learning natural scene category,natural scenes,training set,unsupervised learning},
mendeley-tags = {Bag of Words,BoF,BoW},
pages = {524--531},
title = {{A Bayesian hierarchical model for learning natural scene categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467486},
volume = {2},
year = {2005}
}
@article{Akhundov2018,
abstract = {We take a practical approach to solving sequence labeling problem assuming unavailability of domain expertise and scarcity of informational and computational resources. To this end, we utilize a universal end-to-end Bi-LSTM-based neural sequence labeling model applicable to a wide range of NLP tasks and languages. The model combines morphological, semantic, and structural cues extracted from data to arrive at informed predictions. The model's performance is evaluated on eight benchmark datasets (covering three tasks: POS-tagging, NER, and Chunking, and four languages: English, German, Dutch, and Spanish). We observe state-of-the-art results on four of them: CoNLL-2012 (English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus (German POS-tagging), and competitive performance on the rest.},
annote = {Summary

Sequence labeling with bi-lstms using byte- and word embeddings together},
archivePrefix = {arXiv},
arxivId = {1808.03926},
author = {Akhundov, Adnan and Trautmann, Dietrich and Groh, Georg},
eprint = {1808.03926},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Akhundov, Trautmann, Groh - 2018 - Sequence Labeling A Practical Approach.pdf:pdf},
keywords = {NER,POS,chunking,lstm,named entity recognition,part-of-speech,shallow parsing,supervised learning},
mendeley-tags = {NER,POS,chunking,lstm,named entity recognition,part-of-speech,shallow parsing,supervised learning},
title = {{Sequence Labeling: A Practical Approach}},
url = {http://arxiv.org/abs/1808.03926},
year = {2018}
}
@article{Lawrence1996,
abstract = {One of the most important aspects of any machine learning paradigm is how it scales according to problem size and complexity. Using a task with known optimal training error, and a pre-specified maximum number of training updates, we investigate the convergence of the backpropagation algorithm with respect to a) the complexity of the required function approximation, b) the size of the network in relation to the size required for an optimal solution, and c) the degree of noise in the training data. In general, for a) the solution found is worse when the function to be approximated is more complex, for b) oversize networks can result in lower training and generalization error, and for c) the use of committee or ensemble techniques can be more beneficial as the amount of noise in the training data is increased. For the experiments we performed, we do not obtain the optimal solution in any case. We further support the observation that larger networks can produce better training and generalization error using a face recognition example where a network with many more parameters than training points generalizes better than smaller networks.},
author = {Lawrence, Steve and Giles, C Lee and Tsoi, Ah Chung},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Lawrence, Giles, Tsoi - 1996 - What Size Neural Network Gives Optimal Generalization Convergence Properties of Backpropagation.pdf:pdf},
journal = {Networks},
keywords = {backpropagation,committees,convergence,curse of dimensionality,ensembles,function approximation,generalization,local minima,network size,problem complexity,smoothness},
number = {UMIACS-TR-96-22 and CS-TR-3617},
pages = {1--37},
title = {{What Size Neural Network Gives Optimal Generalization ? Convergence Properties of Backpropagation}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:What+Size+Neural+Network+Gives+Optimal+Generalization?+Convergence+Properties+of+Backpropagation{\#}0},
year = {1996}
}
@inproceedings{Hoashi2010,
author = {Hoashi, Hajime and Joutou, Taichi and Yanai, Keiji},
booktitle = {2010 IEEE International Symposium on Multimedia},
doi = {10.1109/ISM.2010.51},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hoashi, Joutou, Yanai - 2010 - Image Recognition of 85 Food Categories by Feature Fusion.pdf:pdf},
isbn = {978-1-4244-8672-4},
keywords = {Gabor feature,automatic food image recognition system,bag-of-features,cellular phone camera,color histogram,feature extraction,feature fusion,food categories,food image recognition,food technology,gradient histogram,image fusion,image recognition,learning (artificial intelligence),multiple kernel learning,prototype system},
language = {English},
month = {dec},
pages = {296--301},
publisher = {IEEE},
title = {{Image Recognition of 85 Food Categories by Feature Fusion}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5693856},
year = {2010}
}
@article{Viola2001,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, P and Jones, M},
doi = {10.1109/CVPR.2001.990517},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Viola, Jones - 2001 - Rapid object detection using a boosted cascade of simple features.pdf:pdf},
isbn = {0-7695-1272-0},
issn = {1063-6919},
journal = {Conference on Computer Vision and Pattern Recognition (CVPR)},
pmid = {7143246},
title = {{Rapid object detection using a boosted cascade of simple features}},
year = {2001}
}
@misc{Cox2003,
abstract = {Two separate white light illuminated images are acquired of a plate of food. The image data is processed, and the two images are compared to determine volume of particular food zones. In parallel to that, the food type in each zone is identified by a food recognition processor nd reference to a stored nutritional data bank. These two values are combined with the foods' nutritional value in the data bank to provide zone-by-zone nutrient content information. These can be individually displayed, and/or the total displayed so that the user knows the nutritional value of the food on his plate in terms of total calories, percent fat, percent protein, and percent carbohydrate. In addition, the approximate milligrams each of principal vitamin, mineral, fiber, enzyme and phytonutrient on the plate can be displayed sequentially. Provision is made to download data into a PDA or PC.},
annote = {- Kamera mit zwei Lampen nimmt zwei Bilder auf.{\textless}m:linebreak/{\textgreater}- Schatten werden vermessen -{\textgreater} Volumen{\textless}m:linebreak/{\textgreater}- Bild wird anhand von Farbe und Shape klassifiziert{\textless}m:linebreak/{\textgreater}{\textless}m:linebreak/{\textgreater}Wahrscheinlich nicht sehr robust.},
author = {Cox, Dale},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Cox - 2003 - Personal food analyzer.pdf:pdf},
keywords = {Descission Tree},
mendeley-tags = {Descission Tree},
month = {apr},
title = {{Personal food analyzer}},
url = {https://www.google.com.ar/patents/US20030076983},
year = {2003}
}
@article{Yang2010,
abstract = {Food recognition is difficult because food items are de-formable objects that exhibit significant variations in appearance. We believe the key to recognizing food is to exploit the spatial relationships between different ingredients (such as meat and bread in a sandwich). We propose a new representation for food items that calculates pairwise statistics between local features computed over a soft pixel-level segmentation of the image into eight ingredient types. We accumulate these statistics in a multi-dimensional histogram, which is then used as a feature vector for a discriminative classifier. Our experiments show that the proposed representation is significantly more accurate at identifying food than existing methods.},
author = {Yang, Shulin and Chen, Mei and Pomerleau, Dean and Sukthankar, Rahul},
doi = {10.1109/CVPR.2010.5539907},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2010 - Food recognition using statistics of pairwise local features.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2249--2256},
title = {{Food recognition using statistics of pairwise local features}},
year = {2010}
}
@inproceedings{Sutskever2013,
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever et al. - 2013 - On the importance of initialization and momentum in deep learning.pdf:pdf},
pages = {1139--1147},
title = {{On the importance of initialization and momentum in deep learning}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013{\_}sutskever13},
year = {2013}
}
@misc{Inc.,
author = {Foo.log inc.},
title = {{Introduction to FoodLog}},
url = {http://www.foodlog.jp/introduction/log},
urldate = {2016-04-04},
year = {2013}
}
@article{Wang2014a,
abstract = {Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images.It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.4661v1},
author = {Wang, Jiang and Song, Yang and Leung, Thomas and Rosenberg, Chuck and Wang, Jingbin and Philbin, James and Chen, Bo and Wu, Ying},
doi = {10.1109/CVPR.2014.180},
eprint = {arXiv:1404.4661v1},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1386--1393},
title = {{Learning fine-grained image similarity with deep ranking}},
year = {2014}
}
@article{Lin2016,
abstract = {Conventionally, the maximum likelihood (ML) criterion is applied to train a deep belief network (DBN). We present a maximum entropy (ME) learning algorithm for DBNs, designed specifically to handle limited training data. Maximizing only the entropy of parameters in the DBN allows more effective generalization capability, less bias towards data distributions, and robustness to over-fitting compared to ML learning. Results of text classification and object recognition tasks demonstrate ME-trained DBN outperforms ML-trained DBN when training data is limited.},
author = {Lin, Payton and Fu, Szu-Wei and Wang, Syu-Siang and Lai, Ying-Hui and Tsao, Yu},
doi = {10.3390/e18070251},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2016 - Maximum Entropy Learning with Deep Belief Networks.pdf:pdf},
issn = {1099-4300},
journal = {Entropy},
keywords = {deep belief networks,deep learning,deep neural networks,low-resource tasks,machine learning,maximum entropy,restricted Boltzmann machine},
language = {en},
month = {jul},
number = {7},
pages = {251},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Maximum Entropy Learning with Deep Belief Networks}},
url = {http://www.mdpi.com/1099-4300/18/7/251/htm},
volume = {18},
year = {2016}
}
@techreport{Iyyer,
author = {Iyyer, Mohit and Boyd-Graber, Jordan and Iii, Hal Daum{\'{e}}},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Iyyer, Boyd-Graber, Iii - Unknown - Generating Sentences from Semantic Vector Space Representations.pdf:pdf},
title = {{Generating Sentences from Semantic Vector Space Representations}},
url = {https://people.cs.umass.edu/{~}miyyer/pubs/2014{\_}nips{\_}generation.pdf}
}
@article{Zepeda2008,
abstract = {This paper examines the use of photographic and written food diaries as interventions to raise awareness of and change dietary habits. Weinstein's precaution adoption theory and Guagnano, Stern and Dietz's Attitude Behaviour Context theory provide the theoretical basis to explain why nutrition knowledge does not result in healthy eating behaviour and why an intervention may be necessary to change attitudes and behaviours. A pilot study using written and photographic food diaries was conducted with 43 participants. Qualitative analysis of participant interviews revealed that photographic food diaries can alter attitudes and behaviours associated with food choices, and they are more likely to do so than written diaries because they serve as an intervention at the point when decisions regarding what to eat are being made.},
author = {Zepeda, Lydia and Deal, David},
doi = {10.1111/j.1470-6431.2008.00725.x},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Zepeda, Deal - 2008 - Think before you eat photographic food diaries as intervention tools to change dietary decision making and attitud.pdf:pdf},
isbn = {1470-6423},
issn = {14706423},
journal = {International Journal of Consumer Studies},
keywords = {abc theory,dietary habits,food diaries,precaution adoption theory},
number = {6},
pages = {692--698},
title = {{Think before you eat: photographic food diaries as intervention tools to change dietary decision making and attitudes}},
url = {http://doi.wiley.com/10.1111/j.1470-6431.2008.00725.x},
volume = {32},
year = {2008}
}
@article{Weiß2007,
abstract = {Predicting the time and effort for a software problem has long been a difficult task. We present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. Our technique leverages existing issue tracking systems: given a new issue report, we use the Lucene framework to search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach using effort data from the JBoss project. Given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating naive predictions by a factor of four.},
author = {Wei{\ss}, Cathrin and Premraj, Rahul and Zimmermann, Thomas and Zeller, Andreas},
doi = {10.1109/MSR.2007.13},
isbn = {076952950X},
journal = {Proceedings - ICSE 2007 Workshops: Fourth International Workshop on Mining Software Repositories, MSR 2007},
number = {1},
title = {{How long will it take to fix this bug?}},
year = {2007}
}
@article{Chen2001,
abstract = {With multiresolution decomposition and forest representation of wavelet transforms, we implemented a “from presence to classification” object-detection model. Three aspects of this model are studied. First, the presence of an object is quickly detected with fewer data manipulations at the coarsest resolution; secondly, object classification with high accuracy is fulfilled at the full resolution; and thirdly, the propagation in the coarse-to-fine process is studied in terms of coefficient propagation within a coefficient tree. We applied this model to internal deboned poultry inspection. As soon as the presence of a hazardous object was detected at a coarse resolution, a signal was actuated to reject the chicken fillet containing foreign inclusions before packing. Only with small foreign inclusions did we need to resort to finer resolution analysis.},
author = {Chen, Zikuan and Tao, Yang},
doi = {10.1016/S0031-3203(00)00169-2},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Tao - 2001 - Food safety inspection using “from presence to classification” object-detection model.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Food safety,Forest representation,Multiresolution analysis,Object recognition,Wavelet transform},
month = {dec},
number = {12},
pages = {2331--2338},
title = {{Food safety inspection using “from presence to classification” object-detection model}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320300001692},
volume = {34},
year = {2001}
}
@article{Dalal2005,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Dalal, Navneet and Triggs, Bill},
doi = {10.1109/CVPR.2005.177},
eprint = {9411012},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Dalal, Triggs - 2005 - Histograms of Oriented Gradients for Human Detection.pdf:pdf},
isbn = {0-7695-2372-2},
issn = {1063-6919},
journal = {CVPR '05: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1},
keywords = {human-detection,local-feature,object-detection},
pages = {886--893},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {citeulike-article-id:3047126{\%}5Cnhttp://dx.doi.org/10.1109/CVPR.2005.177},
year = {2005}
}
@article{Burke2011,
abstract = {Self-monitoring is the centerpiece of behavioral weight loss intervention programs. This article presents a systematic review of the literature on three components of self-monitoring in behavioral weight loss studies: diet, exercise, and self-weighing. This review included articles that were published between 1993 and 2009 that reported on the relationship between weight loss and these self-monitoring strategies. Of the 22 studies identified, 15 focused on dietary self-monitoring, one on self-monitoring exercise, and six on self-weighing. A wide array of methods was used to perform self-monitoring; the paper diary was used most often. Adherence to self-monitoring was reported most frequently as the number of diaries completed or the frequency of log-ins or reported weights. The use of technology, which included the Internet, personal digital assistants, and electronic digital scales were reported in five studies. Descriptive designs were used in the earlier studies whereas more recent reports involved prospective studies and randomized trials that examined the effect of self-monitoring on weight loss. A significant association between self-monitoring and weight loss was consistently found; however, the level of evidence was weak because of methodologic limitations. The most significant limitations of the reviewed studies were the homogenous samples and reliance on self-report. In all but two studies, the samples were predominantly white and women. This review highlights the need for studies in more diverse populations, for objective measures of adherence to self-monitoring, and for studies that establish the required dose of self-monitoring for successful outcomes.},
author = {Burke, Lora E. and Wang, Jing and Sevick, Mary Ann},
doi = {10.1016/j.jada.2010.10.008},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Burke, Wang, Sevick - 2011 - Self-Monitoring in Weight Loss A Systematic Review of the Literature.pdf:pdf},
issn = {00028223},
journal = {Journal of the American Dietetic Association},
keywords = {Behavior Therapy,Combined Modality Therapy,Computers, Handheld,Diet, Reducing,Diet, Reducing: psychology,Exercise,Exercise: psychology,Humans,Internet,Obesity,Obesity: psychology,Obesity: therapy,Patient Compliance,Patient Compliance: psychology,Self Care,Self Care: psychology,Weight Loss},
month = {jan},
number = {1},
pages = {92--102},
pmid = {21185970},
title = {{Self-Monitoring in Weight Loss: A Systematic Review of the Literature}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3268700{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {111},
year = {2011}
}
@book{Theodoridis2009f,
abstract = {This chapter discusses feature selection and most of the well-known techniques used for it. Feature selection or feature reduction can be described as selecting the most important feature so as to reduce their number and at the same time retain as much as possible of their class discriminatory information. The first step in feature selection is to look at each of the generated features independently and test their discriminatory capability for the problem at hand. Treating features individually, that is, as scalars, has the advantage of computational simplicity. The chapter explores techniques measuring classification capabilities of feature vectors. The chapter also discusses the filter approach and the Wrapper approach for feature selection. In the filter approach, the optimality rule for feature selection is independent of the classifier, which is used in the classifier design stage. In Wrapper approach, for each feature, vector combination the classification error probability of the classifier has to be estimated and the combination resulting in the minimum error probability is selected. The chapter gives emphasis on the t-test. Depending on time constraints, the concepts of divergence, Bhattacharrya distance, and scattered matrices are presented and commented on. Emphasis is also given to Fisher's linear discriminant method (LDA) for the two-class case.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50007-4},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(5).pdf:pdf},
isbn = {9781597492720},
pages = {261--322},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500074},
year = {2009}
}
@article{Ojala1994,
abstract = {This paper evaluates the performance both of some texture measures which have been successfully used in various applications and of some new promising approaches. For classification a method based on Kullback discrimination of sample and prototype distributions is used. The classification results for single features with one-dimensional feature value distributions and for pairs of complementary features with two-dimensional distributions are presented},
author = {Ojala, T. and Pietikainen, M. and Harwood, D.},
doi = {10.1109/ICPR.1994.576366},
isbn = {0-8186-6265-4},
issn = {00313203},
journal = {Proceedings of the 12th IAPR International Conference on Pattern Recognition (ICPR)},
keywords = {Analysis of variance,Autocorrelation,Automation,Distributed computing,Electric variables measurement,Histograms,Image texture analysis,Kullback discrimination,Performance evaluation,Prototypes,Rotation measurement,classification,complementary features,image texture,one-dimensional feature value distributions,performance evaluation,texture measures},
pages = {582--585},
title = {{Performance evaluation of texture measures with classification based on Kullback discrimination of distributions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=576366},
volume = {1},
year = {1994}
}
@article{LeCunJackelB.BoserJ.S.DenkerD.HendersonR.E.HowardW.Hubbard1990,
abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1{\%} error rate and about a 9{\%} reject rate on zipcode digits provided by the U.S. Postal Service.},
archivePrefix = {arXiv},
arxivId = {1004.3732},
author = {{Le Cun Jackel, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard}, L. D. and Cun, Bb Le and Denker, Js and Henderson, D.},
doi = {10.1111/dsu.12130},
eprint = {1004.3732},
isbn = {1-55860-100-7},
issn = {1524-4725},
journal = {Advances in Neural Information Processing Systems},
pages = {396--404},
pmid = {23301817},
title = {{Handwritten Digit Recognition with a Back-Propagation Network}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.5076{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.5076{\&}rep=rep1{\&}type=pdf},
year = {1990}
}
@article{Gehler2009,
abstract = {A key ingredient in the design of visual object classification systems is the identification of relevant class specific aspects while being robust to intra-class variations. While this is a necessity in order to generalize beyond a given set of training images, it is also a very difficult problem due to the high variability of visual appearance within each class. In the last years substantial performance gains on challenging benchmark datasets have been reported in the literature. This progress can be attributed to two developments: the design of highly discriminative and robust image features and the combination of multiple complementary features based on different aspects such as shape, color or texture. In this paper we study several models that aim at learning the correct weighting of different features from training data. These include multiple kernel learning as well as simple baseline methods. Furthermore we derive ensemble methods inspired by Boosting which are easily extendable to several multiclass setting. All methods are thoroughly evaluated on object classification datasets using a multitude of feature descriptors. The key results are that even very simple baseline methods, that are orders of magnitude faster than learning techniques are highly competitive with multiple kernel learning. Furthermore the Boosting type methods are found to produce consistently better results in all experiments. We provide insight of when combination methods can be expected to work and how the benefit of complementary features can be exploited most efficiently.},
author = {Gehler, Peter and Nowozin, Sebastian},
doi = {10.1109/ICCV.2009.5459169},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Gehler, Nowozin - 2009 - On feature combination for multiclass object classification(2).pdf:pdf},
isbn = {978-1-4244-4420-5},
issn = {1550-5499},
journal = {Computer Vision, 2009 IEEE 12th International Conference on},
number = {Iccv},
pages = {221--228},
title = {{On feature combination for multiclass object classification}},
year = {2009}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
chapter = {Graphical},
doi = {10.1117/1.2819119},
editor = {Jordan, M and Kleinberg, J and Sch{\"{o}}lkopf, B},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Yang2009,
archivePrefix = {arXiv},
arxivId = {1504.06897},
author = {Yang, J and Yu, K and Gong, Y and Huang, T},
doi = {10.1109/CVPR.2009.5206757},
eprint = {1504.06897},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2009 - Linear spatial pyramid matching using sparse coding for image classification.pdf:pdf},
isbn = {1063-6919 VO -},
issn = {1063-6919},
journal = {Computer Vision and Pattern  {\ldots}},
title = {{Linear spatial pyramid matching using sparse coding for image classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5206757},
year = {2009}
}
@book{Goodfellow2015,
abstract = {www.deeplearningbook.org},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {Nature Methods},
doi = {10.1038/nmeth.3707},
eprint = {arXiv:1312.6184v5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow, Bengio, Courville - 2015 - Deep Learning.pdf:pdf},
isbn = {9780521835688},
issn = {1548-7091},
number = {1},
pages = {35--35},
pmid = {10463930},
title = {{Deep Learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539{\%}5Cnhttp://www.nature.com/doifinder/10.1038/nmeth.3707},
volume = {13},
year = {2015}
}
@phdthesis{Xia,
author = {Xia, Yinghui},
title = {{Cuisine and Flavor Classification Using Recipes and Food Images}},
url = {http://web.stanford.edu/{~}yinghui/files/cuisine-flavor-classification.pdf},
year = {2016}
}
@article{Leung2001,
abstract = {We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons . Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector , which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions.},
author = {Leung, Thomas and Malik, Jitendra},
doi = {10.1023/A:1011126920638},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {3D texture,Natural material recognition,Texture recognition,Texture synthesis},
number = {1},
pages = {29--44},
pmid = {21126719},
title = {{Representing and recognizing the visual appearance of materials using three-dimensional textons}},
volume = {43},
year = {2001}
}
@article{Chapelle1999,
author = {Chapelle, O. and Haffner, P. and Vapnik, V.N.},
doi = {10.1109/72.788646},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Chapelle, Haffner, Vapnik - 1999 - Support vector machines for histogram-based image classification.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Classification tree analysis,Corel stock photo collection,Histograms,Image classification,Image databases,Image recognition,Kernel,Polynomials,Support vector machine classification,Support vector machines,Web pages,feature space dimensionality,heavy-tailed RBF kernels,high-dimensional histograms,histogram-based image classification,image classification,learning (artificial intelligence),linear SVM,radial basis function networks,remapping,support vector machines},
language = {English},
number = {5},
pages = {1055--1064},
publisher = {IEEE},
title = {{Support vector machines for histogram-based image classification}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=788646},
volume = {10},
year = {1999}
}
@article{Tola2010,
abstract = {In this paper, we introduce a local image descriptor, DAISY, which is very efficient to compute densely. We also present an EM-based algorithm to compute dense depth and occlusion maps from wide-baseline image pairs using this descriptor. This yields much better results in wide-baseline situations than the pixel and correlation-based algorithms that are commonly used in narrow-baseline stereo. Also, using a descriptor makes our algorithm robust against many photometric and geometric transformations. Our descriptor is inspired from earlier ones such as SIFT and GLOH but can be computed much faster for our purposes. Unlike SURF, which can also be computed efficiently at every pixel, it does not introduce artifacts that degrade the matching performance when used densely. It is important to note that our approach is the first algorithm that attempts to estimate dense depth maps from wide-baseline image pairs, and we show that it is a good one at that with many experiments for depth estimation accuracy, occlusion detection, and comparing it against other descriptors on laser-scanned ground truth scenes. We also tested our approach on a variety of indoor and outdoor scenes with different photometric and geometric transformations and our experiments support our claim to being robust against these.},
author = {Tola, Engin and Lepetit, Vincent and Fua, Pascal},
doi = {10.1109/TPAMI.2009.77},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Tola, Lepetit, Fua - 2010 - DAISY an efficient dense descriptor applied to wide-baseline stereo.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Photogrammetry,Photogrammetry: methods,Subtraction Technique},
language = {English},
month = {may},
number = {5},
pages = {815--30},
pmid = {20299707},
publisher = {IEEE},
title = {{DAISY: an efficient dense descriptor applied to wide-baseline stereo.}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4815264},
volume = {32},
year = {2010}
}
@article{Svetkey2008,
abstract = {CONTEXT: Behavioral weight loss interventions achieve short-term success, but re-gain is common.

OBJECTIVE: To compare 2 weight loss maintenance interventions with a self-directed control group.

DESIGN, SETTING, AND PARTICIPANTS: Two-phase trial in which 1032 overweight or obese adults (38{\%} African American, 63{\%} women) with hypertension, dyslipidemia, or both who had lost at least 4 kg during a 6-month weight loss program (phase 1) were randomized to a weight-loss maintenance intervention (phase 2). Enrollment at 4 academic centers occurred August 2003-July 2004 and randomization, February-December 2004. Data collection was completed in June 2007.

INTERVENTIONS: After the phase 1 weight-loss program, participants were randomized to one of the following groups for 30 months: monthly personal contact, unlimited access to an interactive technology-based intervention, or self-directed control. Main Outcome Changes in weight from randomization.

RESULTS: Mean entry weight was 96.7 kg. During the initial 6-month program, mean weight loss was 8.5 kg. After randomization, weight regain occurred. Participants in the personal-contact group regained less weight (4.0 kg) than those in the self-directed group (5.5 kg; mean difference at 30 months, -1.5 kg; 95{\%} confidence interval [CI], -2.4 to -0.6 kg; P = .001). At 30 months, weight regain did not differ between the interactive technology-based (5.2 kg) and self-directed groups (5.5 kg; mean difference -0.3 kg; 95{\%} CI, -1.2 to 0.6 kg; P = .51); however, weight regain was lower in the interactive technology-based than in the self-directed group at 18 months (mean difference, -1.1 kg; 95{\%} CI, -1.9 to -0.4 kg; P = .003) and at 24 months (mean difference, -0.9 kg; 95{\%} CI, -1.7 to -0.02 kg; P = .04). At 30 months, the difference between the personal-contact and interactive technology-based group was -1.2 kg (95{\%} CI -2.1 to -0.3; P = .008). Effects did not differ significantly by sex, race, age, and body mass index subgroups. Overall, 71{\%} of study participants remained below entry weight.

CONCLUSIONS: The majority of individuals who successfully completed an initial behavioral weight loss program maintained a weight below their initial level. Monthly brief personal contact provided modest benefit in sustaining weight loss, whereas an interactive technology-based intervention provided early but transient benefit.

TRIAL REGISTRATION: clinicaltrials.gov Identifier: NCT00054925.},
author = {Svetkey, Laura P and Stevens, Victor J and Brantley, Phillip J and Appel, Lawrence J and Hollis, Jack F and Loria, Catherine M and Vollmer, William M and Gullion, Christina M and Funk, Kristine and Smith, Patti and Samuel-Hodge, Carmen and Myers, Valerie and Lien, Lillian F and Laferriere, Daniel and Kennedy, Betty and Jerome, Gerald J and Heinith, Fran and Harsha, David W and Evans, Pamela and Erlinger, Thomas P and Dalcin, Arline T and Coughlin, Janelle and Charleston, Jeanne and Champagne, Catherine M and Bauck, Alan and Ard, Jamy D and Aicher, Kathleen},
doi = {10.1001/jama.299.10.1139},
issn = {1538-3598},
journal = {JAMA},
keywords = {Adult,Aged,Aged, 80 and over,Communication,Continuity of Patient Care,Energy Intake,Energy Metabolism,Female,Humans,Internet,Male,Middle Aged,Obesity,Obesity: prevention {\&} control,Risk Reduction Behavior,Weight Loss},
month = {mar},
number = {10},
pages = {1139--48},
pmid = {18334689},
title = {{Comparison of strategies for sustaining weight loss: the weight loss maintenance randomized controlled trial.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18334689},
volume = {299},
year = {2008}
}
@book{Theodoridis2009p,
abstract = {This chapter introduces pattern recognition as the scientific discipline with the goal of classification of objects into a number of categories or classes. The chapter discusses the basic philosophy and methodological directions in which the various pattern recognition approaches have evolved and developed. Pattern recognition is an integral part of most machine intelligence systems built for decision making. Machine vision is an area in which pattern recognition is of importance. A typical application of a machine vision system is in the manufacturing industry, either for automated visual inspection or for automation in the assembly line. Character recognition is another important area of pattern recognition, with major implications in automation and information handling. Computer-aided diagnosis is an application of pattern recognition, aimed at assisting doctors in making diagnostic decisions. The chapter outlines various other areas in which pattern recognition finds its use. The chapter also explains the concept of supervised, unsupervised, and semisupervised learning, and concludes with a brief discussion on the contents of other chapters.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50003-7},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(2).pdf:pdf},
isbn = {9781597492720},
pages = {1--12},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500037},
year = {2009}
}
@article{Calonder2010,
author = {Calonder, M. and Lepetit, V. and Strecha, C. and Fua, P.},
doi = {10.1007/978-3-642-15561-1_56},
isbn = {3-642-15560-X, 978-3-642-15560-4},
issn = {978-3-642-15560-4},
journal = {European Conference on Computer Vision (ECCV)},
pages = {778--792},
title = {{BRIEF : Binary Robust Independent Elementary Features}},
year = {2010}
}
@book{Theodoridis2009l,
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50019-0},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(17).pdf:pdf},
isbn = {9781597492720},
pages = {915--926},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500190},
year = {2009}
}
@article{Mens2016,
annote = {Paper on maintaining and evolving software systems.
Mentions the paper as an example for efforts in defect predictions.},
author = {Mens, Tom},
doi = {10.1109/ICSME.2016.19},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Mens - 2016 - An Ecosystemic and Socio-Technical View on Software Maintenance and Evolution.pdf:pdf},
isbn = {9781509038060},
keywords = {-software ecosystem,collaborative,disciplinary research,empirical software engineering,inter-,mixed methods research,socio-technical network,software engineering},
number = {August},
pages = {1--8},
title = {{An Ecosystemic and Socio-Technical View on Software Maintenance and Evolution}},
year = {2016}
}
@book{Theodoridis2009,
abstract = {This chapter explains the basic concepts of system evaluation and semisupervised learning. The beginning sections of the chapter focus on the last stage of the design procedure of a classification system. It is assumed that an optimal classifier has been designed, based on a selected set of training feature vectors. The goal is to evaluate its performance with respect to the probability of classification error associated with the designed system. Methodologies are developed for the estimation of the classification error probability, using the available, hence finite, set of data. Techniques for feature generation, feature selection, classifier design, and system evaluation are mobilized in order to develop a realistic computer-aided diagnosis medical system to assist a doctor reaching a decision. The chapter aims to introduce semisupervised learning basics and to indicate the possible performance improvement that unlabeled data may offer if used properly. The various error rate estimation techniques are discussed, and a case study with real data is treated. The leave-one-out method and the resubstitution methods are also discussed in the chapter.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50012-8},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(10).pdf:pdf},
isbn = {9781597492720},
pages = {567--594},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500128},
year = {2009}
}
@article{Ackley1985,
abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
author = {Ackley, D and Hinton, G and Sejnowski, T},
doi = {10.1016/S0364-0213(85)80012-4},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Ackley, Hinton, Sejnowski - 1985 - A learning algorithm for boltzmann machines.pdf:pdf},
isbn = {0934613338},
issn = {03640213},
journal = {Cognitive Science},
number = {1},
pages = {147--169},
title = {{A learning algorithm for boltzmann machines}},
url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
volume = {9},
year = {1985}
}
@misc{Boushey2010,
abstract = {The present system and method provides a more precise way to record food and beverage intake than traditional methods. The present disclosure provides custom software for use in mobile computing devices that include a digital camera. Photos captured by mobile digital devices are analyzed with image processing and comparisons to certain databases to allow a user to discretely record foods eaten. Specifically, the user captures images of the meal or snack before and after eating. The foods pictured are identified. Image processing software may identify the food or provide choices for the user. Once a food is identified and volume of the food is estimated, nutrient databases are used for calculating final portion sizes and nutrient totals.},
annote = {- Bestimmung von Kalorien durch Handy.
- Aufnahme davor und danach.
- Auswahl durch automatisch erkannte Listen von Zutaten.
- Recognition auf Server
- Gabor filter
- 50 Bilder 
- 17 f{\"{u}}r Training
- 3D Shape Reconstruction
- Volume Estimation},
author = {Boushey, Carol and Edward, Delp and Ebert, David Scott and Lutes, Kyle DelMar and Kerr, Deborah},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Boushey et al. - 2010 - Dietary Assessment System and Method.pdf:pdf},
keywords = {Color histogram,Gabor filter,SVM,Volume Estimation},
mendeley-tags = {Color histogram,Gabor filter,SVM,Volume Estimation},
month = {may},
title = {{Dietary Assessment System and Method}},
url = {http://www.google.com/patents/US20100111383},
year = {2010}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.},
annote = {second version of SURF.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bay et al. - 2008 - Speeded-Up Robust Features (SURF).pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
month = {jun},
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314207001555},
volume = {110},
year = {2008}
}
@article{Martin2014,
abstract = {The digital photography of foods method accurately estimates the food intake of adults and children in cafeterias. When using this method, images of food selection and leftovers are quickly captured in the cafeteria. These images are later compared with images of 'standard' portions of food using computer software. The amount of food selected and discarded is estimated based upon this comparison, and the application automatically calculates energy and nutrient intake. In the present review, we describe this method, as well as a related method called the Remote Food Photography Method (RFPM), which relies on smartphones to estimate food intake in near real-time in free-living conditions. When using the RFPM, participants capture images of food selection and leftovers using a smartphone and these images are wirelessly transmitted in near real-time to a server for analysis. Because data are transferred and analysed in near real-time, the RFPM provides a platform for participants to quickly receive feedback about their food intake behaviour and to receive dietary recommendations for achieving weight loss and health promotion goals. The reliability and validity of measuring food intake with the RFPM in adults and children is also reviewed. In sum, the body of research reviewed demonstrates that digital imaging accurately estimates food intake in many environments and it has many advantages over other methods, including reduced participant burden, elimination of the need for participants to estimate portion size, and the incorporation of computer automation to improve the accuracy, efficiency and cost-effectiveness of the method.},
author = {Martin, C K and Nicklas, T and Gunturk, B and Correa, J B and Allen, H R and Champagne, C},
doi = {10.1111/jhn.12014},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Martin et al. - 2014 - Measuring food intake with digital photography.pdf:pdf},
issn = {1365-277X},
journal = {Journal of human nutrition and dietetics : the official journal of the British Dietetic Association},
keywords = {Cell Phones,Computers,Diet,Diet Records,Diet Surveys,Energy Intake,Food Habits,Food Preferences,Goals,Health Promotion,Humans,Mental Recall,Molecular Sequence Data,Nutrition Assessment,Photography,Portion Size,Recommended Dietary Allowances,Remote Sensing Technology,Reproducibility of Results,Software,Surveys and Questionnaires,Weight Loss},
month = {jan},
pages = {72--81},
pmid = {23848588},
title = {{Measuring food intake with digital photography.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4138603{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {27 Suppl 1},
year = {2014}
}
@article{Rosten,
abstract = {This paper addresses the problem of real-time 3D model-based tracking by combining point-based and edge-based tracking systems. We present a careful analysis of the prop-erties of these two sensor systems and show that this leads to some non-trivial design choices that collectively yield extremely high performance. In particular, we present a method for integrating the two systems and robustly com-bining the pose estimates they produce. Further we show how on-line learning can be used to improve the perfor-mance of feature tracking. Finally, to aid real-time perfor-mance, we introduce the FAST feature detector which can perform full-frame feature detection at 400Hz. The combi-nation of these techniques results in a system which is capa-ble of tracking average prediction errors of 200 pixels. This level of robustness allows us to track very rapid motions, such as 50 • camera shake at 6Hz.},
author = {Rosten, Edward and Drummond, Tom},
title = {{Fusing Points and Lines for High Performance Tracking}}
}
@article{Farinella2014,
abstract = {It is well-known that people love food. However, an insane diet can cause problems in the general health of the people. Since health is strictly linked to the diet, advanced computer vision tools to recognize food images (e.g. acquired with mobile/wearable cameras), as well as their properties (e.g., calories), can help the diet monitoring by providing useful information to the experts (e.g., nutritionists) to assess the food intake of patients (e.g., to combat obesity). The food recognition is a challenging task since the food is intrinsically deformable and presents high variability in appearance. Image representation plays a fundamental role. To properly study the peculiarities of the image representation in the food application context, a benchmark dataset is needed. These facts motivate the work presented in this paper. In this work we introduce the UNICT-FD889 dataset. It is the first food image dataset composed by over 800 distinct plates of food which can be used as benchmark to design and compare representation models of food images. We exploit the UNICT-FD889 dataset for Near Duplicate Image Retrieval (NDIR) purposes by comparing three standard state-of-the-art image descriptors: Bag of Textons, PRICoLBP and SIFT. Results confirm that both textures and colors are fundamental properties in food representation. Moreover the experiments point out that the Bag of Textons representation obtained considering the color domain is more accurate than the other two approaches for NDIR.},
author = {Farinella, Giovanni Maria and Allegra, Dario and Stanco, Filippo},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Farinella, Allegra, Stanco - 2014 - A Benchmark Dataset to Study the Representation of Food Images.pdf:pdf},
journal = {European Conference Computer Vision Workshops},
keywords = {food dataset,food recognition,near duplicate image re-,pricolbp,sift,textons,trieval},
title = {{A Benchmark Dataset to Study the Representation of Food Images}},
year = {2014}
}
@misc{Funayama2012,
abstract = {Methods and apparatus for operating on images are described, in particular methods and apparatus for interest point detection and/or description working under different scales and with different rotations, e. g. for scale-invariant and rotation-invariant interest point detection and/ or description. The present invention can provide improved or alternative apparatus and methods for matching interest points either in the same image or in a different image. The present invention can provide alternative or improved software for implement ing any of the methods of the invention. The present invention can provide alternative or improved data structures created by multiple ?ltering operations to generate a plurality of ?ltered images as well as data structures for storing the ?ltered images themselves, eg as stored in memory or transmitted through a network. The present invention can provide alter native or improved data structures including descriptors of interest points in images, eg as stored in memory or trans mitted through a network as well as data structures associat ing such descriptors with an original copy of the image or an image derived therefrom, eg a thumbnail image.},
author = {Funayama, Ryuji and Yanagihara, Hiromichi and {Van Gool}, Luc and Tuytelaars, Tinne and Bay, Herbert},
isbn = {3540631674},
number = {12},
title = {{ROBUST INTEREST POINT DETECTOR AND DESCRIPTOR}},
volume = {2},
year = {2012}
}
@article{He2018,
abstract = {Aspect-level sentiment classification aims to determine the sentiment polarity of a review sentence towards an opinion target. A sentence could contain multiple sentiment-target pairs; thus the main challenge of this task is to separate different opinion contexts for different targets. To this end, attention mechanism has played an important role in previous state-of-the-art neural models. The mechanism is able to capture the importance of each context word towards a target by modeling their semantic associations. We build upon this line of research and propose two novel approaches for improving the effectiveness of attention. First, we propose a method for target representation that better captures the semantic meaning of the opinion target. Second, we introduce an attention model that incorporates syntactic information into the attention mechanism. We experiment on attention-based LSTM (Long Short-Term Memory) models using the datasets from SemEval 2014, 2015, and 2016. The experimental results show that the conventional attention-based LSTM can be substantially improved by incorporating the two approaches.},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2018 - Effective Attention Modeling for Aspect-Level Sentiment Classification.pdf:pdf},
pages = {1121--1131},
title = {{Effective Attention Modeling for Aspect-Level Sentiment Classification}},
url = {http://aclweb.org/anthology/C18-1096},
year = {2018}
}
@article{Vedaldi2009,
abstract = {Our objective is to obtain a state-of-the art object category detector by employing a state-of-the-art image classifier to search for the object in all possible image sub-windows. We use multiple kernel learning of Varma and Ray (ICCV 2007) to learn an optimal combination of exponential {\&}{\#}x03C7;{\textless}sup{\textgreater}2{\textless}/sup{\textgreater} kernels, each of which captures a different feature channel. Our features include the distribution of edges, dense and sparse visual words, and feature descriptors at different levels of spatial organization. Such a powerful classifier cannot be tested on all image sub-windows in a reasonable amount of time. Thus we propose a novel three-stage classifier, which combines linear, quasi-linear, and non-linear kernel SVMs. We show that increasing the non-linearity of the kernels increases their discriminative power, at the cost of an increased computational complexity. Our contributions include (i) showing that a linear classifier can be evaluated with a complexity proportional to the number of sub-windows (independent of the sub-window area and descriptor dimension); (ii) a comparison of three efficient methods of proposing candidate regions (including the jumping window classifier of Chum and Zisserman (CVPR 2007) based on proposing windows from scale invariant features); and (Hi) introducing overlap-recall curves as a mean to compare and optimize the performance of the intermediate pipeline stages. The method is evaluated on the PASCAL Visual Object Detection Challenge, and exceeds the performances of previously published methods for most of the classes.},
author = {Vedaldi, Andrea and Gulshan, Varun and Varma, Manik and Zisserman, Andrew},
doi = {10.1109/ICCV.2009.5459183},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Vedaldi et al. - 2009 - Multiple Kernels for object detection.pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {606--613},
title = {{Multiple Kernels for object detection}},
year = {2009}
}
@book{Theodoridis2009m,
abstract = {This chapter explains clustering algorithms based on function optimization, using tools from differential calculus. Hard clustering and fuzzy and possibilistic schemes are considered, based on various types of cluster representatives, including point representatives, hyperplane representatives, and shell-shaped representatives. A distinct characteristic of most of the algorithms of this chapter is that the cluster representatives are computed using all the available vectors of X, and not only the vectors that have been assigned to the respective cluster. The chapter focuses on four major categories of algorithms that include the mixture decomposition, the fuzzy, the possibilistic and the hard clustering algorithms. In mixture decomposition algorithms, the cost function is constructed on the basis of random vectors, and assignment to clusters follows probabilistic arguments, in the spirit of the Bayesian classification. The conditional probabilities here result from the optimization process. In the fuzzy approach a proximity function between a vector and a cluster is defined. Values of the membership functions of a vector in the various clusters are interrelated. This constraint is removed in the case of the possibilistic approach. Hard clustering may be viewed as a special case of the fuzzy clustering approach, where each vector belongs exclusively to a cluster. This category includes the celebrated k-means clustering algorithm.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50016-5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(14).pdf:pdf},
isbn = {9781597492720},
pages = {701--763},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500165},
year = {2009}
}
@article{Jureczko2010,
abstract = {Background: This paper describes an analysisthat was conducted on newly collected repository with 92 versions of 38 proprietary, open-source and academic projects. A preliminary perfomed before showed the need for a further in-depth analysis study in order to identify project clusters. Aims: The goal of this research is to perform clustering on software projects in order to identify groups of software projects with similar characteristic from the defect prediction point of view. One defect prediction model should work well for all projects that belong to such group. The existence of those groups was investigated with statistical tests and by comparing the mean value of prediction efficiency. Method: Hierarchical and k-means clustering, as well obtained clusters were investigated with as Kohonen's neural network was used to find groups of similar projects. The the discriminant analysis. For each of the identified group a statistical analysis has been conducted in order to distinguish whether this group really exists. Two defect prediction models were created for each of the identified groups. The first one was based on the projects that belong to a given group, and the second one - on all the projects. Then, both models were applied to all versions of projects from the investigated group. If the predictions from the model based on projects that belong to the identified group are significantly better than the all-projects model (the mean values were compared and statistical tests were used), we conclude that the group really exists. Results: Six different clusters were identified and the existence of two of them was statistically proven: 1) cluster proprietary B – T=19, p=0.035, r=0.40; 2) cluster proprietary/open – t(17)=3.18, p=0.05, r=0.59. The obtained effect sizes (r) represent large effects according to Cohen's benchmark, which is a substantial finding. Conclusions: The two identified clusters were described and compared with results obtained by other researchers. The results of this work makes next step towards defining formal methods of reuse defect prediction models by identifying groups of projects within which the same defect prediction model may be used. Furthermore, a method of clustering was suggested and applied.},
author = {Jureczko, Marian and Madeyski, Lech},
doi = {10.1145/1868328.1868342},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Jureczko, Madeyski - 2010 - Towards identifying software project clusters with regard to defect prediction.pdf:pdf},
isbn = {9781450304047},
journal = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering - PROMISE '10},
keywords = {clustering,defect prediction,design metrics,size metrics},
pages = {1},
title = {{Towards identifying software project clusters with regard to defect prediction}},
year = {2010}
}
@techreport{Bahuleyan2018,
abstract = {Probabilistic generation of natural language sentences is an important task in NLP. Existing models such as variational autoencoders (VAE) for sequence generation are extremely difficult to train due to the issues associated with the Kullback-Leibler (KL) loss collapsing to zero. One has to implement various heuristics such as KL weight annealing and word dropout in a carefully engineered manner to successfully train a text VAE. In this paper, we propose the use of Wasserstein au-toencoders (WAE) for probabilistic natural language sentence generation. We show that sequence-to-sequence WAEs are more robust towards hyperparameters and can be trained in a straightforward manner without the need for any weight an-nealing. Empirical evidence shows that the latent space learned by WAEs exhibits properties of continuity and smoothness as in VAEs, while simultaneously achieving much higher BLEU scores for sentence reconstruction .},
archivePrefix = {arXiv},
arxivId = {1806.08462v1},
author = {Bahuleyan, Hareesh and Mou, Lili and Vamaraju, Kartik and Zhou, Hao and Vechtomova, Olga},
eprint = {1806.08462v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bahuleyan et al. - 2018 - Probabilistic Natural Language Generation with Wasserstein Autoencoders.pdf:pdf},
keywords = {Language Generation,VAR,Variational Autoencoder,WAE,Wasserstein Autoencoder},
mendeley-tags = {Language Generation,VAR,Variational Autoencoder,WAE,Wasserstein Autoencoder},
title = {{Probabilistic Natural Language Generation with Wasserstein Autoencoders}},
url = {https://arxiv.org/pdf/1806.08462.pdf},
year = {2018}
}
@phdthesis{Schober,
abstract = {As obesity becomes more and more of a problem in developed countries, food logging is frequently used to help overweight people to balance their energy intake. Unfortunately, food logging is a tedious and inaccurate process. Computer vision and machine learning can help the user with this process. By taking an image of the meal, algorithms are able to make automated food intake assessments by detecting food items and their size. The goal of this thesis is the evaluation and implementation of a proof of concept application that can be used to facilitate and extend future food logging applications. For the task of food classification seven approaches were evaluated including feature classifiers like SIFT and SURF and convolutional neural networks. To enable the classi- fication, segmentation, training and evaluation of classifiers, an extensive application was implemented using Python. The application supports data preprocessing and is designed so that it can be extended with additional image recognition concepts. With the aforementioned algorithms it was shown that algorithms are able to achieve a classification accuracy of 75{\%} on 50 different food item classes if the algorithm suggests five possible candidates.},
author = {Schober, Felix},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Schober - 2016 - Evaluation and Implementation of Computer Vision Concepts for Image Recognition of Food Items Using Deep Learning and S.pdf:pdf},
pages = {103},
school = {Technische Universit{\"{a}}t M{\"{u}}nchen},
title = {{Evaluation and Implementation of Computer Vision Concepts for Image Recognition of Food Items Using Deep Learning and SVMs}},
type = {Bachelor's Thesis},
year = {2016}
}
@article{Mou2016,
abstract = {Deep neural networks have made significant breakthroughs in many fields of artificial intelligence. However, it has not been applied in the field of programming language processing. In this paper, we propose the tree-based convolutional neural network (TBCNN) to model programming languages, which contain rich and explicit tree structural information. In our model, program vector representations are learned by the "coding" pretraining criterion based on abstract syntax trees (ASTs); the convolutional layer explicitly captures neighboring features on the tree; with the "binary continuous tree" and "3-way pooling," our model can deal with ASTs of different shapes and sizes.We evaluate the program vector representations empirically, showing that the coding criterion successfully captures underlying features of AST nodes, and that program vector representations significantly speed up supervised learning. We also compare TBCNN to baseline methods; our model achieves better accuracy in the task of program classification. To our best knowledge, this paper is the first to analyze programs with deep neural networks; we extend the scope of deep learning to the field of programming language processing. The experimental results validate its feasibility; they also show a promising future of this new research area.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.5718v2},
author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
eprint = {arXiv:1409.5718v2},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Mou et al. - 2016 - Convolutional neural networks over tree structures for programming language processing.pdf:pdf},
isbn = {9781577357605},
journal = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI '16)},
keywords = {Technical Papers: Machine Learning Applications},
pages = {1287--1293},
title = {{Convolutional neural networks over tree structures for programming language processing}},
url = {http://sei.pku.edu.cn/{~}zhanglu/Download/AAAI16.pdf},
year = {2016}
}
@article{Shotton2008,
author = {Shotton, J and Johnson, M and Cipolla, R},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Shotton, Johnson, Cipolla - 2008 - Semantic Texton Forest for Image Categorization and Segmentation.pdf:pdf},
journal = {Proceedings of the conference on Computer Vision and Pattern Recognition},
keywords = {Semantic Segmentation; Image Parsing; Random Fores},
pages = {1--8},
title = {{Semantic Texton Forest for Image Categorization and Segmentation}},
year = {2008}
}
@misc{Puri2010,
abstract = {A method and system for analyzing at least one food item on a food plate is disclosed. A plurality of images of the food plate is received by an image capturing device. A description of the at least one food item on the food plate is received by a recognition device. The description is at least one of a voice description and a text description. At least one processor extracts a list of food items from the description; classifies and segments the at least one food item from the list using color and texture features derived from the plurality of images; and estimates the volume of the classified and segmented at least one food item. The processor is also configured to estimate the caloric content of the at least one food item.},
annote = {Kombination Stimme und Visual Patterns{\textless}m:linebreak/{\textgreater}{\textless}m:linebreak/{\textgreater}- Colors {\&} Textures mit SVM{\textless}m:linebreak/{\textgreater}{\textless}m:linebreak/{\textgreater}Vermutlich nicht sehr zuverl{\"{a}}ssig, da nur Erkennung durch Histogramme und Gradienten.},
author = {Puri, Manika and Zhiwei, Zhu and Lubin, Jeffrey and Pschar, Tom and Divakaran, Ajay and Sawheney, Harpeet S.},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Puri et al. - 2010 - Food recognition using visual analysis and speech recognition.pdf:pdf},
keywords = {Volume Estimation},
mendeley-tags = {Volume Estimation},
month = {jul},
title = {{Food recognition using visual analysis and speech recognition}},
url = {http://www.google.com/patents/US20100173269},
year = {2010}
}
@article{Weinberger2005,
author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence K},
journal = {{\{}A{\}}dvances in {\{}N{\}}eural {\{}I{\}}nformation {\{}P{\}}rocessing {\{}S{\}}ystems (NIPS)},
keywords = {metric learning},
pages = {1473--1480},
title = {{{\{}D{\}}istance {\{}M{\}}etric {\{}L{\}}earning for {\{}L{\}}arge {\{}M{\}}argin {\{}N{\}}earest {\{}N{\}}eighbor {\{}C{\}}lassification}},
volume = {18},
year = {2005}
}
@article{Kawano2014a,
abstract = {We propose a mobile food recognition system the purposes of which are estimating calorie and nutritious of foods and recording a user's eating habits. Since all the processes on image recognition performed on a smartphone, the system does not need to send images to a server and runs on an ordinary smartphone in a real-time way. To recognize food items, a user draws bounding boxes by touching the screen first, and then the system starts food item recognition within the indicated bounding boxes. To recognize them more accurately, we segment each food item region by GrubCut, extract a color histogram and SURF-based bag-of-features, and finally classify it into one of the fifty food categories with linear SVM and fast chi(2) kernel. In addition, the system estimates the direction of food regions where the higher SVM output score is expected to be obtained, show it as an arrow on the screen in order to ask a user to move a smartphone camera. This recognition process is performed repeatedly about once a second. We implemented this system as an Android smartphone application so as to use multiple CPU cores effectively for real-time recognition. In the experiments, we have achieved the 81.55{\%} classification rate for the top 5 category candidates when the ground-truth bounding boxes are given. In addition, we obtained positive evaluation by user study compared to the food recording system without object recognition.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1007/978-3-319-04117-9_38},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kawano, Yanai - 2014 - FoodCam A real-time mobile food recognition system employing Fisher Vector.pdf:pdf},
isbn = {9783319041162},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {369--373},
title = {{FoodCam: A real-time mobile food recognition system employing Fisher Vector}},
volume = {8326 LNCS},
year = {2014}
}
@article{JanErikSolem2012,
abstract = {If you want a basic understanding of computer vision's underlying theory and algorithms, this hands-on introduction is the ideal place to start. You'll learn techniques for object recognition, {\{}3D{\}} reconstruction, stereo imaging, augmented ...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Jan Erik Solem}},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {1-4493-1653-0},
issn = {1098-6596},
journal = {Programming Computer Vision with Python},
pages = {264},
pmid = {25246403},
title = {{Programming Computer Vision with Python}},
year = {2012}
}
@article{Boehm1988,
abstract = {A discussion is presented of the two primary ways of understanding$\backslash$nsoftware costs. The black-box or influence-function approach provides$\backslash$nuseful experimental and observational insights on the relative software$\backslash$nproductivity and quality leverage of various management, technical,$\backslash$nenvironmental, and personnel options. The glass-box or cost distribution$\backslash$napproach helps identify strategies for integrated software productivity$\backslash$nand quality improvement programs using such structures as the value$\backslash$nchain and the software productivity opportunity tree. The individual$\backslash$nstrategies for improving software productivity are identified. Issues$\backslash$nrelated to software costs and controlling them are examined and$\backslash$ndiscussed. It is pointed out that a good framework of techniques exists$\backslash$nfor controlling software budgets, schedules, and work completed, but$\backslash$nthat a great deal of further progress is needed to provide an overall$\backslash$nset of planning and control techniques covering software product$\backslash$nqualities and end-user system objectives},
annote = {A significant related insight is that the cost of fixing or reworking software is much smaller (factors of 50 to 200) in the early phases than in the later phases (page 1466)},
author = {Boehm, Barry W. and Papaccio, Philip N.},
doi = {10.1109/32.6191},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Boehm, Papaccio - 1988 - Understanding and Controlling Software Costs.pdf:pdf},
isbn = {0098-558},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Programming productivity,software costs,software engineering economics,software management,software metrics,software productivity},
number = {10},
pages = {1462--1477},
title = {{Understanding and Controlling Software Costs}},
volume = {14},
year = {1988}
}
@article{Kawano2014,
abstract = {Abstract In this paper, we report the feature obtained from the Deep Convolutional Neural Network boosts food recognition accuracy greatly by integrating it with conventional hand-crafted image features, Fisher Vectors with HoG and Color patches. In the experiments, we have achieved 72.26{\%} as the top-1 accuracy and 92.00{\%} as the top-5 accuracy for the 100-class food dataset, UEC-FOOD100, which outperforms the best classification accuracy of this dataset reported so far, 59.6{\%}, greatly.},
author = {Kawano, Yoshiyuki and Yanai, Keiji},
doi = {10.1145/2638728.2641339},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kawano, Yanai - 2014 - Food Image Recognition with Deep Convolutional Features.pdf:pdf},
isbn = {9781450330473},
journal = {ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp)},
pages = {589--593},
title = {{Food Image Recognition with Deep Convolutional Features}},
year = {2014}
}
@article{Pielke2011,
author = {Pielke, Roger and Byerly, Radford},
doi = {10.1038/472038d},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pielke, Byerly - 2011 - Shuttle Programme Lifetime Cost.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
pages = {38},
pmid = {21475182},
title = {{Shuttle Programme Lifetime Cost}},
volume = {472},
year = {2011}
}
@article{Rodriguez2004,
author = {Rodriguez, Carlos C (Albany)},
isbn = {0262122413},
journal = {October},
number = {x},
pages = {1--5},
title = {{The Kernel Trick}},
year = {2004}
}
@article{bradski2000opencv,
author = {Bradski, Gary and Others},
journal = {Doctor Dobbs Journal},
number = {11},
pages = {120--126},
publisher = {M AND T PUBLISHING INC},
title = {{The opencv library}},
volume = {25},
year = {2000}
}
@article{Press2018,
abstract = {In NMT, how far can we get without attention and without separate encoding and decoding? To answer that question, we introduce a recurrent neural translation model that does not use attention and does not have a separate encoder and decoder. Our eager translation model is low-latency, writing target tokens as soon as it reads the first source token, and uses constant memory during decoding. It performs on par with the standard attention-based model of Bahdanau et al. (2014), and better on long sentences. 1},
annote = {Translation without attention and encoder-decoder. 

Instead eager translation starting with the output of the first token after the frist input token

--{\textgreater} eager translation model},
archivePrefix = {arXiv},
arxivId = {1810.13409v1},
author = {Press, Ofir and Smith, Noah A},
eprint = {1810.13409v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Press, Smith - 2018 - You May Not Need Attention.pdf:pdf},
keywords = {LSTM,supervised learning},
mendeley-tags = {LSTM,supervised learning},
number = {December},
title = {{You May Not Need Attention}},
url = {http://www.statmt.org/wmt14/},
year = {2018}
}
@article{Schmitt2018,
abstract = {In this work, we propose a new model for aspect-based sentiment analysis. In contrast to previous approaches, we jointly model the detection of aspects and the classification of their polarity in an end-to-end trainable neural network. We conduct experiments with different neural architectures and word representations on the recent GermEval 2017 dataset. We were able to show considerable performance gains by using the joint modeling approach in all settings compared to pipeline approaches. The combination of a convolutional neural network and fasttext embeddings outperformed the best submission of the shared task in 2017, establishing a new state of the art.},
archivePrefix = {arXiv},
arxivId = {1808.09238},
author = {Schmitt, Martin and Steinheber, Simon and Schreiber, Konrad and Roth, Benjamin},
eprint = {1808.09238},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Schmitt et al. - 2018 - Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks.pdf:pdf},
month = {aug},
title = {{Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks}},
url = {http://arxiv.org/abs/1808.09238},
year = {2018}
}
@phdthesis{Mezgec2015,
abstract = {As people become more and more aware of the importance of a healthy diet, a need for automatic food and drink recognition systems has arisen. Such a system can not only provide recognition of the food or drink item, but can also estimate its nutritional value, making it useful for diet planning. In this paper, we describe the problems of food and drink recognition and take a look at the research that has been published so far in this research field. We focus on popular and influential solutions and in particular, we give an overview of three interesting papers: two for food recognition and one for drink recognition, each with its own approach to the problem. Finally, we give our critical review of the research in the field and suggest what the future research work should focus on.},
author = {Mezgec, Simon},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Mezgec - 2015 - Modern Methods and Algorithms for Food and Drink Recognition Using Average Quality Photograph.pdf:pdf},
keywords = {Computer Vision,drink recognition,electronic nose,electronic tongue.,food recognition,image dataset},
school = {Ljubjana},
title = {{Modern Methods and Algorithms for Food and Drink Recognition Using Average Quality Photograph}},
year = {2015}
}
@article{Abadi2015,
abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Shlens, Jon and Steiner, Benoit and Sutskever, Ilya and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vinyals, Oriol and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
journal = {None},
pages = {19},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {0500581},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1},
pages = {1--127},
pmid = {17348934},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Dosovitskiy2013,
abstract = {When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial 'seed' patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101).},
archivePrefix = {arXiv},
arxivId = {1312.5242},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Brox, Thomas},
eprint = {1312.5242},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Dosovitskiy, Springenberg, Brox - 2013 - Unsupervised feature learning by augmenting single images.pdf:pdf},
month = {dec},
pages = {7},
title = {{Unsupervised feature learning by augmenting single images}},
url = {http://arxiv.org/abs/1312.5242},
year = {2013}
}
@article{Chen2014,
abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6{\%} IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
archivePrefix = {arXiv},
arxivId = {1412.7062},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
eprint = {1412.7062},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs.pdf:pdf},
month = {dec},
title = {{Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1412.7062},
year = {2014}
}
@article{Wang2009,
abstract = {By combining Histograms of Oriented Gradients (HOG) and Local Binary Pattern (LBP) as the feature set, we propose a novel human detection approach capable of handling partial occlusion. Two kinds of detectors, i.e., global detector for whole scanning windows and part detectors for local regions, are learned from the training data using linear SVM. For each ambiguous scanning window, we construct an occlusion likelihood map by using the response of each block of the HOG feature to the global detector. The occlusion likelihood map is then segmented by Mean-shift approach. The segmented portion of the window with a majority of negative response is inferred as an occluded region. If partial occlusion is indicated with high likelihood in a certain scanning window, part detectors are applied on the unoccluded regions to achieve the final classification on the current scanning window. With the help of the augmented HOG-LBP feature and the global-part occlusion handling method, we achieve a detection rate of 91.3{\%} with FPPW= 10{\textless}sup{\textgreater}{\&}{\#}x2212;6{\textless}/sup{\textgreater}, 94.7{\%} with FPPW= 10{\textless}sup{\textgreater}{\&}{\#}x2212;5{\textless}/sup{\textgreater}, and 97.9{\%} with FPPW= 10{\textless}sup{\textgreater}{\&}{\#}x2212;4{\textless}/sup{\textgreater} on the INRIA dataset, which, to our best knowledge, is the best human detection performance on the INRIA dataset. The global-part occlusion handling method is further validated using synthesized occlusion data constructed from the INRIA and Pascal dataset.},
author = {Wang, Xiaoyu and Han, Tony X. and Yan, Shuicheng},
doi = {10.1109/ICCV.2009.5459207},
isbn = {978-1-4244-4420-5},
issn = {1550-5499},
journal = {Computer Vision, 2009 IEEE 12th International Conference on},
number = {Iccv},
pages = {32--39},
title = {{An HOG-LBP human detector with partial occlusion handling}},
year = {2009}
}
@book{Theodoridis2009i,
abstract = {This chapter discusses the concept of hierarchical clustering algorithms. These algorithms produce a hierarchy of clustering and are usually found in the social sciences and biological taxonomy. They have also been used in many other fields, including modern biology, medicine, and archaeology. Applications of the hierarchical algorithms may also be found in computer science and engineering. Hierarchical clustering algorithms produce a hierarchy of nested clustering. These algorithms involve N steps, as many as the number of data vectors. A hierarchical algorithm can be viewed as a mapping of the data proximity matrix into a cophenetic matrix. The chapter explains in detail two main categories of hierarchical algorithms: the agglomerative and the divisive hierarchical algorithms. While explaining the general agglomerative scheme, the emphasis is on single link and complete link algorithms based on matrix theory. A special type of hierarchical algorithms that are most appropriate for handling large data sets is discussed. The need for such algorithms stems from a number of applications, such as Web mining and bioinformatics.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50015-3},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(13).pdf:pdf},
isbn = {9781597492720},
pages = {653--700},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500153},
year = {2009}
}
@article{Kim2010,
abstract = {There is a growing concern about chronic diseases and other health problems related to diet including obesity and cancer. The need to accurately measure diet (what foods a person consumes) becomes imperative. Dietary intake provides valuable insights for mounting intervention programs for prevention of chronic diseases. Measuring accurate dietary intake is considered to be an open research problem in the nutrition and health fields. In this paper, we describe a novel mobile telephone food record that will provide an accurate account of daily food and nutrient intake. Our approach includes the use of image analysis tools for identification and quantification of food that is consumed at a meal. Images obtained before and after foods are eaten are used to estimate the amount and type of food consumed. The mobile device provides a unique vehicle for collecting dietary information that reduces the burden on respondents that are obtained using more classical approaches for dietary assessment. We describe our approach to image analysis that includes the segmentation of food items, features used to identify foods, a method for automatic portion estimation, and our overall system architecture for collecting the food intake information. Color},
author = {Kim, Sungye},
doi = {10.1109/JSTSP.2010.2051471.The},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kim - 2010 - The Use of Mobile Devices in Aiding Dietary Assessment and Evaluation.pdf:pdf},
journal = {IEEE Journal of Selected Topics in Signal Processing},
keywords = {Volume Estimation},
mendeley-tags = {Volume Estimation},
number = {4},
pages = {756--766},
title = {{The Use of Mobile Devices in Aiding Dietary Assessment and Evaluation}},
volume = {4},
year = {2010}
}
@article{Csurka2004,
abstract = {We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Na{\"{i}}ve Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.},
author = {Csurka, Gabriella and Dance, Christopher R. and Fan, Lixin and Willamowski, Jutta and Bray, C{\'{e}}dric},
doi = {10.1234/12345678},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Csurka et al. - 2004 - Visual categorization with bags of keypoints.pdf:pdf},
isbn = {9780335226375},
issn = {18703453},
journal = {Proceedings of the ECCV International Workshop on Statistical Learning in Computer Vision},
pages = {59--74},
title = {{Visual categorization with bags of keypoints}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.604},
year = {2004}
}
@book{Theodoridis2009d,
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50020-7},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(18).pdf:pdf},
isbn = {9781597492720},
pages = {927--929},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500207},
year = {2009}
}
@article{Pedregosa2011,
abstract = {Abstract Scikit - learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high- ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1201.0490v2},
author = {Pedregosa, Fabian and Varoquaux, G},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1201.0490v2},
isbn = {9781783281930},
issn = {15324435},
journal = {{\ldots} of Machine Learning {\ldots}},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195},
volume = {12},
year = {2011}
}
@book{Theodoridis2009o,
abstract = {This chapter discusses the concepts of sequential clustering algorithms and the clustering schemes and criteria that are available to the analyst. This chapter begins with a general overview of the various clustering algorithmic schemes and then focuses on one category, known as sequential algorithms. Clustering algorithms may be viewed as schemes that provide with sensible clustering by considering only a small fraction of the set containing all possible partitions of X. The result depends on the specific algorithm and the criteria used. A clustering algorithm is a learning procedure that tries to identify the specific characteristics of the clusters underlying the data set. Sequential clustering algorithms produce a single clustering and are quite straightforward and fast methods. In most of them, all the feature vectors are presented to the algorithm once or a few times (typically no more than five or six times). The final result is, usually, dependent on the order in which the vectors are presented to the algorithm. These schemes tend to produce compact and hyperspherically or hyperellipsoidally shaped clusters, depending on the distance metric used. The chapter also discusses different categories of clustering algorithms including hierarchical clustering algorithms, clustering algorithms based on cost function optimization, branch and bound clustering algorithms, genetic clustering algorithms, and stochastic relaxation methods.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50014-1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(12).pdf:pdf},
isbn = {9781597492720},
pages = {627--652},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500141},
year = {2009}
}
@book{Theodoridis2009a,
abstract = {This chapter discusses the basic concepts of context-dependent classification. Context-dependent classification can also be emancipated from its Bayesian root. This can be achieved by adopting different transition costs, which are not necessarily related to probability densities. The chapter introduces hidden Markov models (HMM) which are one of the most widely used models describing the underlying class dependence. The Markov chain models are applied to communications and speech recognition. Concepts of continuous observation HMM, HMM with state duration modeling, HMM with duration modeling, best path method, segment modeling, and Markov random fields are explained. The chapter also focuses on training Markov models via neural networks. Feature vectors in this chapter are referred as observations occurring in sequence, one after the other. Two typical application areas of the Viterbi algorithm are presented in the chapter. A simple example is presented to reveal how the equalization problem comes under the umbrella of a Markovian context-dependent classification task. Examples in the chapter show that equalization problem can be defined as a context-dependent classification task. The chapter explains the context-dependent classification in one-dimensional and related two-dimensional generalizations.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50011-6},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(9).pdf:pdf},
isbn = {9781597492720},
pages = {521--565},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500116},
year = {2009}
}
@book{Swaroop,
author = {Swaroop, C H},
title = {{A Byte of Python}},
url = {http://files.swaroopch.com/python/byte{\_}of{\_}python.pdf}
}
@article{Neelamegam2013,
abstract = {In foodstuff trade, grading of coarse food resources is essential because samples of stuffs are subjected to adulteration. In the precedent, foodstuffs in the appearance of granules were conceded through sieves or supplementary mechanical way for grading purposes. In this manuscript, investigation is performed on basmati rice granules; to appraise the act via image processing and Neural Network Pattern Recognition Tool which is implemented based on the features extracted from rice granules for categorization grades of granules. Digital imaging is acknowledged as a proficient system, to haul out the features from rice granules in a non-contact mode. Images are acquired for rice using camera. Image Pre-processing techniques, Adaptive thresholding, Canny edge detection, Feature extraction are the checks that are performed on the acquired image using image processing method through Open source Computer Vision (Open CV) which is a library of functions that aids image processing in real time. The morphological features extracted from the image are given to Neural Network Pattern Recognition Tool. This effort has been prepared to categorize the appropriate quality category for a specified rice sample based on its parameters. The performance of image processing condensed the time of action and enhanced the crop identification significantly.},
author = {Neelamegam, P and Abirami, S and {Vishnu Priya}, K and {Rubalya Valantina}, S},
doi = {10.1109/CICT.2013.6558219},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Neelamegam et al. - 2013 - Analysis of rice granules using image processing and neural network.pdf:pdf},
journal = {Information {\&} Communication Technologies (ICT), 2013 IEEE Conference on},
keywords = {Biological neural networks,Canny edge detection,Detectors,Feature extraction,Histograms,Image edge detection,Open CV,Sobel edge detection,adaptive thresholding,basmati rice granules,classification grades,crop recognition,digital imaging,edge detection,food handling industry,food processing industry,granular food materials,gray scale,image classification,image colour analysis,image processing,image segmentation,median smoothing,neural nets,neural network,open source computer vision},
number = {7},
pages = {879--884},
title = {{Analysis of rice granules using image processing and neural network}},
volume = {96},
year = {2013}
}
@article{Boser1992,
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms. 1 INTRODUCTION Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
doi = {10.1.1.21.3818},
eprint = {arXiv:1011.1669v3},
isbn = {089791497X},
issn = {0-89791-497-X},
journal = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory},
pages = {144--152},
pmid = {25246403},
title = {{A Training Algorithm for Optimal Margin Classifiers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818},
year = {1992}
}
@book{Theodoridis2009n,
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50001-3},
isbn = {9781597492720},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500013},
year = {2009}
}
@inproceedings{Eigen2015,
author = {Eigen, David and Fergus, Rob},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Eigen, Fergus - 2015 - Predicting Depth, Surface Normals and Semantic Labels With a Common Multi-Scale Convolutional Architecture.pdf:pdf},
pages = {2650--2658},
title = {{Predicting Depth, Surface Normals and Semantic Labels With a Common Multi-Scale Convolutional Architecture}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Eigen{\_}Predicting{\_}Depth{\_}Surface{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@article{Koru2005,
abstract = {The article focuses on building effective defect-prediction models in practice. Defective software modules cause software failures, increase development and maintenance costs, and decrease customer satisfaction. Effective defect prediction models can help developers focus quality assurance activities on defect-prone modules and thus improve software quality by using resources more efficiently. Successfully predicting defect-prone software modules can help developers improve product quality by focusing quality assurance activities on those modules. Emerging repositories of publicly available software engineering data sets support research in this area by providing static measures and defect data that developers can use to build prediction models and test their effectiveness. Stratifying the U.S. National Aeronautics and Space Administration data sets from the predictor models in software engineering repository according to module size showed improved prediction performance in the sub-sets that included larger modules.},
author = {Koru, a G{\"{u}}neş and Liu, Hongfang},
doi = {10.1109/MS.2005.149},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Koru, Liu - 2005 - Building Effective diction Models in Practice.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {APPLICATION software,CUSTOMER satisfaction,ELECTRONIC data processing,PREDICTION models,QUALITY assurance,SOFTWARE engineering},
number = {6},
pages = {23--29},
title = {{Building Effective diction Models in Practice.}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\%}7B{\&}{\%}7Ddb=bth{\%}7B{\&}{\%}7DAN=18794543{\%}7B{\&}{\%}7Dsite=ehost-live},
volume = {22},
year = {2005}
}
@article{Steidl2013,
abstract = {A significant amount of source code in software systems consists of comments, i. e., parts of the code which are ignored by the compiler. Comments in code represent a main source for system documentation and are hence key for source code understanding with respect to development and maintenance. Although many software developers consider comments to be crucial for program understanding, existing approaches for software quality analysis ignore system commenting or make only quantitative claims. Hence, current quality analyzes do not take a significant part of the software into account. In this work, we present a first detailed approach for quality analysis and assessment of code comments. The approach provides a model for comment quality which is based on different comment categories. To categorize comments, we use machine learning on Java and C/C++ programs. The model comprises different quality aspects: by providing metrics tailored to suit specific categories, we show how quality aspects of the model can be assessed. The validity of the metrics is evaluated with a survey among 16 experienced software developers, a case study demonstrates the relevance of the metrics in practice.},
author = {Steidl, Daniela and Hummel, Benjamin and Juergens, Elmar},
doi = {10.1109/ICPC.2013.6613836},
isbn = {978-1-4673-3092-3},
issn = {1063-6897},
journal = {Program Comprehension (ICPC), 2013 IEEE 21st International Conference on},
keywords = {C++ language,C/C++ programs,Coherence,Computer bugs,Documentation,Java,Java programs,Measurement,Software,Training data,code comment assessment,comment categories,learning (artificial intelligence),machine learning,program compilers,program diagnostics,program understanding,reverse engineering,software metrics,software quality,software quality analysis,software systems,source code comment quality analysis,system documentation},
pages = {83--92},
title = {{Quality analysis of source code comments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6613836},
year = {2013}
}
@article{Thome2012,
author = {Thome, Antonio Carlos Gay},
doi = {10.5772/2575},
isbn = {978-953-51-0823-8},
journal = {Advances in Character Recognition},
pages = {25--49},
title = {{SVM Classifiers – Concepts and Applications to Character Recognition}},
url = {http://www.intechopen.com/books/advances-in-character-recognition},
year = {2012}
}
@article{Hinton2002,
abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual "expert" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called "contrastive divergence" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
author = {Hinton, Geoffrey E.},
doi = {10.1162/089976602760128018},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hinton - 2002 - Training products of experts by minimizing contrastive divergence.pdf:pdf},
isbn = {10.1162/089976602760128018},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1771--1800},
pmid = {12180402},
title = {{Training products of experts by minimizing contrastive divergence}},
volume = {14},
year = {2002}
}
@article{Bengio2009a,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
eprint = {0500581},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Bossard2014,
abstract = {In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve eﬃciency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101'000 images. With an average accuracy of 50.76{\%}, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88{\%} and 8.13{\%}, respectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classification methods.},
annote = {ETHZ Food 101},
archivePrefix = {arXiv},
arxivId = {10.1007/978-3-319-10599-4{\_}29},
author = {Bossard, Lukas and Guillaumin, Matthieu and {Van Gool}, Luc},
doi = {10.1007/978-3-319-10599-4_29},
eprint = {978-3-319-10599-4{\_}29},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bossard, Guillaumin - Unknown - Food-101 -- Mining Discriminative Components with Random Forests.pdf:pdf},
isbn = {978-3-319-10598-7},
issn = {978-3-319-10598-7},
journal = {Computer Vision - ECCV 2014},
keywords = {Discriminative part mining,Food recognition,Image classiﬁcation,Random Forest,discriminative part mining,food recognition,forest,image classification,random},
pages = {446--461},
primaryClass = {10.1007},
publisher = {Springer International Publishing},
title = {{Food-101 -- Mining Discriminative Components with Random Forests}},
url = {https://www.vision.ee.ethz.ch/datasets{\_}extra/food-101/},
year = {2014}
}
@article{Muller2016,
author = {M{\"{u}}ller, Sebastian C and Fritz, Thomas},
doi = {10.1145/2884781.2884803},
isbn = {9781450339001},
journal = {ICSE '16 Proceedings of the 38th International Conference on Software Engineering},
pages = {452--463},
title = {{Using ( Bio ) Metrics to Predict Code Quality Online}},
year = {2016}
}
@techreport{Perone,
abstract = {Despite the fast developmental pace of new sentence embedding methods, it is still challenging to find comprehensive evaluations of these different techniques. In the past years, we saw significant improvements in the field of sentence embeddings and especially towards the development of universal sentence encoders that could provide inductive transfer to a wide variety of downstream tasks. In this work, we perform a comprehensive evaluation of recent methods using a wide variety of downstream and linguistic feature probing tasks. We show that a simple approach using bag-of-words with a recently introduced language model for deep context-dependent word embeddings proved to yield better results in many tasks when compared to sentence encoders trained on entailment datasets. We also show, however, that we are still far away from a universal encoder that can perform consistently across several downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1806.06259v1},
author = {Perone, Christian S and Silveira, Roberto and Paula, Thomas S},
eprint = {1806.06259v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Perone, Silveira, Paula - Unknown - Evaluation of sentence embeddings in downstream and linguistic probing tasks(2).pdf:pdf},
title = {{Evaluation of sentence embeddings in downstream and linguistic probing tasks}},
url = {https://arxiv.org/pdf/1806.06259.pdf}
}
@misc{Dykeman2016,
abstract = {When I started working on understanding generative models, I didn't find any resources that gave a good, high level, intuitive overview of variational autoencoders. Some great resources exist for understanding them in detail and seeing the math behind them. In particular “Tutorial on Variational Autoencoders” by Carl Doersch covers the same topics as this post, but as the author notes, there is some abuse of notation in that article, and the treatment is more abstract then what I'll go for here. Here, I'll carry the example of a variational autoencoder for the MNIST digits dataset throughout, using concrete examples for each concept. Hopefully by reading this article you can get a general idea of how Variational Autoencoders work before tackling them in detail.},
author = {Dykeman, Isaac},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Dykeman - 2016 - Conditional Variational Autoencoders.pdf:pdf},
keywords = {VAR,Variational Autoencoder,unsuppervised},
mendeley-tags = {VAR,Variational Autoencoder,unsuppervised},
title = {{Conditional Variational Autoencoders}},
url = {http://ijdykeman.github.io/ml/2016/12/21/cvae.html},
urldate = {2018-11-08},
year = {2016}
}
@inproceedings{Hu2012,
abstract = {Dietary treatment is the basic therapy for diabetes, but how to do the right food intake is the biggest discouraging problem. We present a proposal for mobile phone diabetes food information display which can help determine the food composition and calories automatically from the clinical point of view with the mature communication technology. We analyzed the composite of the device, especially the key technical method, which is digital image recognition, three-dimensional image analysis, standard tables of food composition database and composite meals energy calculation auxiliary knowledge base. The device has technical feasibility and broad application prospects. It's combination of dietary treatment with smart mobile technology, which can provide great support to solve the problem of food energy assessment and analysis, not only for the single-ingredient food but also the composite meals. The proposal also provides further thinking about how to improve adherence to medical nutrition treatment in diabetes.},
author = {Hu, Jing-sheng and Jiang, Chen},
booktitle = {National Conference on Information Technology and Computer Science},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hu, Jiang - 2012 - A Proposal for Automatic Diabetes Food Information Display with Mobile Phone.pdf:pdf},
isbn = {9789491216381},
keywords = {-dietary assessment,3D REconstruction,Volume Estimation,also known as,analysis,device,dietary treatment,image,is the base treatment,medical nutrition therapy,over the world,prevalence of diabetes all,smart mobile phone},
mendeley-tags = {3D REconstruction,Volume Estimation},
number = {Citcs},
pages = {43--46},
title = {{A Proposal for Automatic Diabetes Food Information Display with Mobile Phone}},
year = {2012}
}
@article{Chen2012,
abstract = {Computer-aided food identification and quantity estimation have caught more attention than before due to the growing concern of health and obesity. The identification problem is usually defined as an image categorization or classification problem and several researches on this topic have been proposed. In this paper, we address the issues of feature descriptors in the food identification problem and introduce a preliminary approach for the quantity es- timation using depth information. Sparse coding is utilized in the SIFT and Local binary pattern feature descriptors, and these fea- tures combined with Gabor and color features are used to represent food items. A multi-label SVM classifier is trained for each fea- ture, and these classifiers are combined with multi-class Adaboost algorithm. For evaluation, 50 major categories of worldwide food are used, and each category contains 100 photographs from differ- ent sources, such as photos taken manually or from Internet web albums. An overall accuracy of 68.3{\%} is achieved, and success at top-N candidates achieved 80.6{\%}, 84.8{\%}, and 90.9{\%} accuracy accordingly when N equals 2, 3, and 5, thus making mobile appli- cation practical. The experimental results show that the proposed methods greatly improve the performance of original SIFT and LBP feature descriptors. On the other hand, for quantity estimation us- ing depth information, a straight forward method is proposed for certain food, while transparent food ingredients such as pure water and},
annote = {50-data},
author = {Chen, MY and Yang, YH and Ho, CJ and Wang, SH},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2012 - Automatic chinese food identification and quantity estimation.pdf:pdf},
journal = {SIGGRAPH Asia 2012 {\ldots}},
keywords = {Adaboost algorithm},
mendeley-tags = {Adaboost algorithm},
title = {{Automatic chinese food identification and quantity estimation}},
url = {http://dl.acm.org/citation.cfm?id=2407775},
year = {2012}
}
@inproceedings{Snoek2005,
address = {New York, New York, USA},
annote = {Kombinieren von mehreren Features um die Rate zu verbessern.},
author = {Snoek, Cees G. M. and Worring, Marcel and Smeulders, Arnold W. M.},
booktitle = {Proceedings of the 13th annual ACM international conference on Multimedia - MULTIMEDIA '05},
doi = {10.1145/1101149.1101236},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Snoek, Worring, Smeulders - 2005 - Early versus late fusion in semantic video analysis.pdf:pdf},
isbn = {1595930442},
keywords = {Early Fusion,Feature Fusion,Late Fusion,early fusion,late fusion,multimedia understanding,semantic concept detection},
mendeley-tags = {Early Fusion,Feature Fusion,Late Fusion},
month = {nov},
pages = {399},
publisher = {ACM Press},
title = {{Early versus late fusion in semantic video analysis}},
url = {http://dl.acm.org/citation.cfm?id=1101149.1101236},
year = {2005}
}
@article{Wong2009,
author = {Wong, W. Eric and Qi, Yu},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Wong, Qi - 2009 - Bp neural network-based effective fault localization.pdf:pdf},
journal = {International Journal of Software Engineering and Knowledge Engineering},
keywords = {back-propagation,bp,failed test,fault localization,network,neural,program debugging,successful test,suspiciousness of code},
number = {4},
pages = {573--597},
title = {{Bp neural network-based effective fault localization}},
volume = {19},
year = {2009}
}
@article{Bosch2014,
abstract = {Many chronic diseases, such as heart diseases, diabetes, and obesity, can be related to diet. Hence, the need to accurately measure diet becomes imperative. We are developing methods to use image analysis tools for the identification and quantification of food consumed at a meal. In this paper we describe a new approach to food identification using several features based on local and global measures and a “voting” based late decision fusion classifier to identify the food items. Experimental results on a wide variety of food items are presented. Index},
annote = {Of the 10 leading causes of death in the U.S., 6 are related to diet.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bosch, Marc and Zhu, Fengqing and Khanna, Nitin and Boushey, Carol and Edward, Delp},
doi = {10.1016/j.drugalcdep.2008.02.002.A},
eprint = {NIHMS150003},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bosch et al. - 2014 - Combining global and local features for food identification in dietary assessment.pdf:pdf},
isbn = {2156623929},
issn = {08966273},
journal = {IEEE Trans Image Process. 2011 ; 2011: 1789–1792.},
keywords = {Feature extraction,image analysis,image texture,object recognition,supervised learning},
mendeley-tags = {Feature extraction,image analysis,image texture,object recognition,supervised learning},
number = {10},
pages = {1203--1214},
pmid = {1000000221},
title = {{Combining global and local features for food identification in dietary assessment}},
volume = {15},
year = {2014}
}
@article{VandeSande2010,
abstract = {Image category recognition is important to access visual information on the level of objects and scene types. So far, intensity-based descriptors have been widely used for feature extraction at salient points. To increase illumination invariance and discriminative power, color descriptors have been proposed. Because many different descriptors exist, a structured overview is required of color invariant descriptors in the context of image category recognition. Therefore, this paper studies the invariance properties and the distinctiveness of color descriptors (software to compute the color descriptors from this paper is available from http://www.colordescriptors.com) in a structured way. The analytical invariance properties of color descriptors are explored, using a taxonomy based on invariance properties with respect to photometric transformations, and tested experimentally using a data set with known illumination conditions. In addition, the distinctiveness of color descriptors is assessed experimentally using two benchmarks, one from the image domain and one from the video domain. From the theoretical and experimental results, it can be derived that invariance to light intensity changes and light color changes affects category recognition. The results further reveal that, for light intensity shifts, the usefulness of invariance is category-specific. Overall, when choosing a single descriptor and no prior knowledge about the data set and object and scene categories is available, the OpponentSIFT is recommended. Furthermore, a combined set of color descriptors outperforms intensity-based SIFT and improves category recognition by 8 percent on the PASCAL VOC 2007 and by 7 percent on the Mediamill Challenge.},
author = {van de Sande, Koen E A and Gevers, Theo and Snoek, Cees G M},
doi = {10.1109/TPAMI.2009.154},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/van de Sande, Gevers, Snoek - 2010 - Evaluating color descriptors for object and scene recognition.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Color,Colorimetry,Colorimetry: methods,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Three-Dimensional,Three-Dimensional: methods},
language = {English},
month = {sep},
number = {9},
pages = {1582--96},
pmid = {20634554},
publisher = {IEEE},
title = {{Evaluating color descriptors for object and scene recognition.}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5204091},
volume = {32},
year = {2010}
}
@article{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
author = {Krizhevsky, Alex},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky - 2009 - Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
journal = {{\ldots} Science Department, University of Toronto, Tech. {\ldots}},
pages = {1--60},
title = {{Learning Multiple Layers of Features from Tiny Images}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Multiple+Layers+of+Features+from+Tiny+Images{\#}0},
year = {2009}
}
@book{Theodoridis2009b,
abstract = {This chapter explores classifiers based on Bayes Decision Theory. The chapter primarily focuses on Bayesian classification and techniques for estimating unknown probability density functions based on the available experimental evidence. The chapter also deals with the design of the classifier in a pattern recognition system. While discussing the concept of minimizing the classification error probability, it is shown that the Bayesian classifier is optimal with respect to minimizing the classification error probability. Special focus is put on the Bayesian classification, the minimum distance (Euclidean and Mahalanobis), the nearest neighbor classifiers, and the naive Bayes classifier. The chapter approaches the classification problem via Bayesian probabilistic arguments with a goal to minimize the classification error probability or the risk. Examples are presented to show that not all problems are well suited to such approaches, and in those cases, it may be preferable to compute decision surfaces directly by means of alternative costs. The chapter also focuses on a particular family of decision surfaces associated with the Bayesian classification for the specific case of Gaussian density functions.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50004-9},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(3).pdf:pdf},
isbn = {9781597492720},
pages = {13--89},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500049},
year = {2009}
}
@article{Rublee2011,
author = {Rublee, Ethan and Bradski, Gary},
doi = {10.1109/ICCV.2011.6126544},
isbn = {978-1-4577-1102-2},
issn = {1550-5499},
keywords = {Ethan Rublee Vincent Rabaud Kurt Konolige Gary Bra},
pmid = {20033598},
title = {{ORB - an efficient alternative to SIFT or SURF}},
url = {http://www.willowgarage.com/sites/default/files/orb{\_}final.pdf},
year = {2011}
}
@article{Duan2005,
abstract = {Multiclass SVMs are usually implemented by combining sev- eral two-class SVMs. The one-versus-all method using winner-takes-all strategy and the one-versus-one method implemented by max-wins vot- ing are popularly used for this purpose. In this paper we give empirical evidence to show that these methods are inferior to another one-versus- one method: one that uses Platt's posterior probabilities together with the pairwise coupling idea of Hastie and Tibshirani. The evidence is par- ticularly strong when the training dataset is sparse.},
author = {Duan, Kai-Bo and Keerthi, S. Sathiya},
doi = {10.1007/b136985},
isbn = {978-3-540-26306-7},
issn = {0302-9743 (Print) 1611-3349 (Online)},
journal = {Multiple Classifier Systems},
pages = {278--285},
pmid = {124},
title = {{Which Is the Best Multiclass SVM Method? An Empirical Study}},
url = {http://dl.acm.org/citation.cfm?id=2134810.2134843},
volume = {3541},
year = {2005}
}
@article{Nie2013,
abstract = {An automatic detector that finds circular dining plates in chronically recorded images or videos is reported for the study of food intake and obesity. We first detect edges from input images. After a number of processing steps that convert edges into curves, arc filtering and grouping algorithms are applied. Then, convex hulls are identified and the ones that fit the description of ellipses corresponding to dining plates are determined. Our experiments using real-world images indicate that this detector is highly reliable and robust even when the input images contain complex background scenes and the dining plates are severely occluded.},
author = {Nie, Jie and Wei, Zhiqiang and Jia, Wenyan and Li, Lu and Fernstrom, John D. and Sclabassi, Robert J. and Sun, Mingui},
doi = {10.1016/j.micinf.2011.07.011.Innate},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Nie et al. - 2013 - Automatic Detection of Dining Plates for Image-Based Dietary Evaluation.pdf:pdf},
isbn = {6176321972},
journal = {Conf Proc IEEE Eng Med Biol Soc. 2010},
keywords = {computer-aided drug design,cyclophilin,free energy perturbation,hiv,reverse transcriptase},
number = {9},
pages = {1199--1216},
title = {{Automatic Detection of Dining Plates for Image-Based Dietary Evaluation}},
volume = {18},
year = {2013}
}
@inproceedings{Gu2016,
abstract = {Developers often wonder how to implement a certain func- tionality (e.g., how to parse XML files) using APIs. Obtain- ing an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bags-of-words and lack a deep under- standing of the semantics of the query. We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bag-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches.},
annote = {Deep RNN for APIs},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.06655v1},
author = {Gu, Xiaodong and Zhang, Hongyu and Zhang, Dongmei and Kim, Sunghun},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
doi = {10.1145/1235},
eprint = {arXiv:1508.06655v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Gu et al. - 2016 - Deep API Learning.pdf:pdf},
isbn = {9781450321389},
issn = {9781450321389},
keywords = {4d trajectory management,importance sampling,motion planning,separation assurance,tactical planning},
pages = {631--642},
title = {{Deep API Learning}},
year = {2016}
}
@article{Ojala2002,
author = {Ojala, T. and Pietikainen, M. and Maenpaa, T.},
doi = {10.1109/TPAMI.2002.1017623},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Gray-scale,Histograms,Image recognition,Image texture,Multiresolution analysis,Pattern recognition,Prototypes,Quantization,Robustness,Spatial resolution,angular space,computational simplicity,gray-scale variations,image classification,image texture,invariance,local binary patterns,local image texture,multiresolution analysis,multiresolution gray-scale texture classification,nonparametric discrimination,nonparametric statistics,occurrence histogram,prototype distributions,rotation invariant texture classification,sample distributions,spatial resolution,uniform patterns},
language = {English},
month = {jul},
number = {7},
pages = {971--987},
publisher = {IEEE},
title = {{Multiresolution gray-scale and rotation invariant texture classification with local binary patterns}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1017623},
volume = {24},
year = {2002}
}
@article{Hsu2002,
abstract = {Support vector machines (SVMs) were originally designed for binary$\backslash$nclassification. How to effectively extend it for multiclass classification$\backslash$nis still an ongoing research issue. Several methods have been proposed$\backslash$nwhere typically we construct a multiclass classifier by combining$\backslash$nseveral binary classifiers. Some authors also proposed methods that$\backslash$nconsider all classes at once. As it is computationally more expensive$\backslash$nto solve multiclass problems, comparisons of these methods using$\backslash$nlarge-scale problems have not been seriously conducted. Especially$\backslash$nfor methods solving multiclass SVM in one step, a much larger optimization$\backslash$nproblem is required so up to now experiments are limited to small$\backslash$ndata sets. In this paper we give decomposition implementations for$\backslash$ntwo such "all-together" methods. We then compare their performance$\backslash$nwith three methods based on binary classifications: "one-against-all,"$\backslash$n"one-against-one," and directed acyclic graph SVM (DAGSVM). Our experiments$\backslash$nindicate that the "one-against-one" and DAG methods are more suitable$\backslash$nfor practical use than the other methods. Results also show that$\backslash$nfor large problems methods by considering all data at once in general$\backslash$nneed fewer support vectors},
author = {Hsu, Chih-Wei and Lin, Chih-Jen},
doi = {10.1109/72.991427},
isbn = {3-540-32026-1},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
keywords = {DAGSVM,SVMs,binary classifiers,decomposition,direc},
number = {2},
pages = {415--425},
pmid = {18244499},
title = {{A comparison of methods for multiclass support vector machines}},
volume = {13},
year = {2002}
}
@article{Szegedy2014,
abstract = {Abstract We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/ICCV.2011.6126456},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {1550-5499},
journal = {arXiv preprint arXiv:1409.4842},
pages = {1--12},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842v1},
year = {2014}
}
@phdthesis{Sheikh2013,
abstract = {The goal of this project is to design a prototype of a smartphone calorie-counting application that uses image recognition to identify food/food products. The application allows a user to take a picture of food using the smartphone's camera; it then attempts to identify the food using Content Based Image Recognition (CBIR) and subsequently retrieve its calories from an online web-service. The user can share the photos online on social networks such as Facebook. The premise is that by allowing the user to take and share photos while recording calories in the background, calorie counting can be made fun, interactive and painless.},
author = {Sheikh, Waqqas and Sheikh, Talal},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Sheikh, Sheikh - 2013 - Prototype of an Image Recognition Based Calorie Recording Application.pdf:pdf},
school = {Heriot Watt University},
title = {{Prototype of an Image Recognition Based Calorie Recording Application}},
year = {2013}
}
@article{Murphy-Hill2012,
abstract = {Software developers interact with the development environments they use by issuing commands that execute various programming tools, from source code formatters to build tools. However, developers often only use a small subset of the commands offered by modern development environments, reducing their overall development fluency. In this paper, we use several existing command recommender algorithms to suggest new commands to developers based on their existing command usage history, and also introduce several new algorithms. By running these algorithms on data submitted by several thousand Eclipse users, we describe two studies that explore the feasibility of automatically recommending commands to software developers. The results suggest that, while recommendation is more difficult in development environments than in other domains, it is still feasible to automatically recommend commands to developers based on their usage history, and that using patterns of past discovery is a useful way to do so.},
author = {Murphy-Hill, Emerson and Jiresal, Rahul and Murphy, Gail C.},
doi = {10.1145/2393596.2393645},
isbn = {9781450316149},
journal = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering - FSE '12},
keywords = {IDEs,commands,discovery,software developers},
pages = {1},
title = {{Improving software developers' fluency by recommending development environment commands}},
url = {http://dl.acm.org.prox.lib.ncsu.edu/citation.cfm?id=2393596.2393645},
year = {2012}
}
@book{McConnell2011,
abstract = {Code Complete},
address = {Redmond},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McConnell, Steven C.},
booktitle = {The Analyst},
doi = {10.1039/c0an90005b},
edition = {2nd Editio},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/McConnell - 2011 - Code Complete.pdf:pdf},
isbn = {0735619670},
issn = {0003-2654},
number = {1},
pages = {952},
pmid = {21135959},
publisher = {Microsoft Press},
title = {{Code Complete}},
url = {http://xlink.rsc.org/?DOI=c0an90005b},
volume = {136},
year = {2011}
}
@article{FreundYoav1999a,
abstract = {1 INTRODUCTION The AdaBoost algorithm 7, 16 has recently proved to be an important component in practical learning algorithms. Two of the most successful combinations have been boosting decision trees and boosting stumps 6, 1, 13, 8. Stumps are the simplest special case of decision trees which consist of a single decision node and two prediction leaves. Boosting decision trees learning algorithms, such as CART 2 and C4.5 14, yields very good classifiers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Freund, Yoav}, and Llew Mason},
doi = {10.1093/jxb/ern164},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Freund, Yoav - 1999 - The alternating decision tree learning algorithm.pdf:pdf},
isbn = {1558606122},
issn = {14602431},
journal = {International Conference on Machine Learning},
pages = {124--133},
pmid = {18603617},
title = {{The alternating decision tree learning algorithm}},
volume = {99},
year = {1999}
}
@article{Joutou2009,
abstract = {Since health care on foods is drawing people's attention recently, a system that can record everyday meals easily is being awaited. In this paper, we propose an automatic food image recognition system for recording people's eating habits. In the proposed system, we use the Multiple Kernel Learning (MKL) method to integrate several kinds of image features such as color, texture and SIFT adaptively. MKL enables to estimate optimal weights to combine image features for each category. In addition, we implemented a prototype system to recognize food images taken by cellular-phone cameras. In the experiment, we have achieved the 61.34{\%} classification rate for 50 kinds of foods. To the best of our knowledge, this is the first report of a food image classification system which can be applied for practical use.},
annote = {sch{\"{o}}ner Vergleich der Feature Extraction Algorithmen -{\textgreater} performen nicht sehr gut.
Erst mit MKL gut},
author = {Joutou, Taichi and Yanai, Keiji and Joutou, Taichi},
doi = {10.1109/ICIP.2009.5413400},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Joutou, Yanai, Joutou - 2009 - A Food Image Recognition System With Multiple Kernel Learning.pdf:pdf},
isbn = {978-1-4244-5653-6},
issn = {9781424456543},
journal = {IEEE International Conference on Image Processing (ICIP)},
keywords = {BoF,BoW,Color histogram,DoG,Feature Fusion,Gabor Texture Features,Multiple Kernel Learning,SIFT,SVM,food image,generic object recognition,multiple kernel learning},
mendeley-tags = {BoF,BoW,Color histogram,DoG,Feature Fusion,Multiple Kernel Learning,SIFT,SVM},
pages = {285--288},
title = {{A Food Image Recognition System With Multiple Kernel Learning}},
url = {file://filesrv01/mv{\_}mendeley{\_}db{\$}/A Food Image Recognition System with Multiple Kernel Learning{\_}Joutou, Yanai{\_}2009.pdf},
year = {2009}
}
@book{Theodoridis2009k,
abstract = {This chapter deals with problems that are not linearly separable and for which the design of a linear classifier, even in an optimal way, does not lead to satisfactory performance. The XOR problem is examined, and the procedure is then applied to more general cases of nonlinearly separable classes. The chapter discusses the capabilities of a multilayer perceptron, with one hidden layer and units of the McCulloch–Pitts type, to divide the input one-dimenesional space into a number of polyhedral regions. Techniques for training neural networks are explored. Pruning is discussed with an emphasis on generalization issues. Emphasis is also given to Cover's theorem and radial basis function (RBF) networks. The nonlinear support vector machines, decision trees, and combining classifiers are briefly discussed. The concept of combining classifiers is also discussed. One section reviews a large class of nonlinear classifiers known as decision trees, which are multistage decision systems in which classes are sequentially rejected until a finally accepted class is reached.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50006-2},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(4).pdf:pdf},
isbn = {9781597492720},
pages = {151--260},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500062},
year = {2009}
}
@article{Felzenszwalb2010,
abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin- sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
author = {Felzenszwalb, P F and Girshick, R B and McAllester, D and Ramanan, D},
doi = {10.1109/TPAMI.2009.167},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Felzenszwalb et al. - 2010 - Object Detection with Discriminative Trained Part Based Models.pdf:pdf},
isbn = {0162-8828 VO - 32},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object Detection; Object Localization; Part-based},
number = {9},
pages = {1627--1645},
pmid = {20634557},
title = {{Object Detection with Discriminative Trained Part Based Models}},
volume = {32},
year = {2010}
}
@article{Witkin1983,
abstract = {The extrema in a signal and its first few derivatives provide a useful general-purpose qualitative description for many kinds of signals. A fundamental problem in computing such descriptions is scale: a derivative must be taken over some neighborhood, but there is seldom a principled basis for choosing its size. Scale-space filtering is a method that describes signals qualitatively, managing the ambiguity of scale in an organized and natural way. The signal is first expanded by convolution with gaussian masks over a continuum of sizes. This "scale-space" image is then collapsed, using its qualitative structure, into a tree providing a concise but complete qualitative description covering all scales of observation. The description is further refined by applying a stability criterion, to identify events that persist of large changes in scale.},
archivePrefix = {arXiv},
arxivId = {http://dl.acm.org/citation.cfm?id=1623607},
author = {Witkin, Andrew P},
doi = {10.1109/ICASSP.1984.1172729},
eprint = {/dl.acm.org/citation.cfm?id=1623607},
isbn = {0865760640},
journal = {International Joint Conference on Artificial Intelligence},
pages = {1019--1022},
primaryClass = {http:},
title = {{Scale-space filtering}},
url = {http://portal.acm.org/citation.cfm?id=1623607},
volume = {2},
year = {1983}
}
@article{Burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, CJC Christopher J C},
doi = {10.1023/A:1009715923555},
editor = {Fayyad, Usama},
eprint = {1111.6189v1},
institution = {Bell Laboratories, Lucent Technologies},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
publisher = {Springer},
series = {NetGames '06},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf{\%}5Cnhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@article{Agrawal2008,
author = {Agrawal, M and Konolige, K and Blas, M R},
doi = {10.1007/978-3-540-88693-8_8},
journal = {Eccv08},
pages = {IV: 102--115},
title = {{CenSurE: Center Surround Extremas for Realtime Feature Detection and Matching}},
year = {2008}
}
@article{Queiroz2016,
annote = {Defect prediciton with metrics 73{\%} accuracy},
author = {Queiroz, Rodrigo and Berger, Thorsten and Czarnecki, Krzysztof},
doi = {10.1145/3001867.3001874},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Queiroz, Berger, Czarnecki - 2016 - Towards predicting feature defects in software product lines.pdf:pdf},
isbn = {9781450346474},
journal = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development  - FOSD 2016},
keywords = {defect prediction,features,software product lines},
pages = {58--62},
title = {{Towards predicting feature defects in software product lines}},
url = {http://dl.acm.org/citation.cfm?doid=3001867.3001874},
year = {2016}
}
@article{Alec2018,
abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9{\%} on commonsense reasoning (Stories Cloze Test), 5.7{\%} on question answering (RACE), and 1.5{\%} on textual entailment (MultiNLI).},
author = {Alec, Radford and Karthik, Narasimhan and Tim, Salimans and Openai, Ilya Sutskever},
doi = {10.1093/aob/mcp031},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Alec et al. - 2018 - Improving Language Understanding by Generative Pre-Training.pdf:pdf},
issn = {1095-8290},
journal = {OpenAI},
pages = {12},
pmid = {19218577},
title = {{Improving Language Understanding by Generative Pre-Training}},
url = {https://gluebenchmark.com/leaderboard},
year = {2018}
}
@inproceedings{Kitamura2008,
address = {New York, New York, USA},
author = {Kitamura, Keigo and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
booktitle = {Proceeding of the 16th ACM international conference on Multimedia - MM '08},
doi = {10.1145/1459359.1459548},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kitamura, Yamasaki, Aizawa - 2008 - Food log by analyzing food images.pdf:pdf},
isbn = {9781605583037},
keywords = {food,life-log,multimedia interfaces},
month = {oct},
pages = {999},
publisher = {ACM Press},
title = {{Food log by analyzing food images}},
url = {http://dl.acm.org/citation.cfm?id=1459359.1459548},
year = {2008}
}
@article{Schneider2014a,
abstract = {In a sparse-representation-based face recognition scheme, the desired dictionary should have good representational power (i.e., being able to span the subspace of all faces) while supporting optimal discrimination of the classes (i.e., different human subjects). We propose a method to learn an over-complete dictionary that attempts to simultaneously achieve the above two goals. The proposed method, discriminative K-SVD (D-KSVD), is based on extending the K-SVD algorithm by incorporating the classification error into the objective function, thus allowing the performance of a linear classifier and the representational power of the dictionary being considered at the same time by the same optimization procedure. The D-KSVD algorithm finds the dictionary and solves for the classifier using a procedure derived from the K-SVD algorithm, which has proven efficiency and performance. This is in contrast to most existing work that relies on iteratively solving sub-problems with the hope of achieving the global optimal through iterative approximation. We evaluate the proposed method using two commonly-used face databases, the Extended YaleB database and the AR database, with detailed comparison to 3 alternative approaches, including the leading state-of-the-art in the literature. The experiments show that the proposed method outperforms these competing methods in most of the cases. Further, using Fisher criterion and dictionary incoherence, we also show that the learned dictionary and the corresponding classifier are indeed better-posed to support sparse-representation-based recognition.},
author = {Schneider, Ros{\'{a}}lia G. and Tuytelaars, Tinne},
doi = {10.1145/2661229.2661231},
isbn = {9781424469840},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {classification,fisher vectors,sketches,sketches, classification, Fisher vectors},
number = {6},
pages = {2691--2698},
title = {{Sketch classification and classification-driven analysis using Fisher vectors}},
volume = {33},
year = {2014}
}
@article{Jones2003,
author = {Jones, Joel},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Jones - 2003 - Abstract syntax tree implementation idioms.pdf:pdf},
journal = {Proceedings of the 10th conference on pattern languages of programs (plop2003)},
pages = {1--10},
title = {{Abstract syntax tree implementation idioms}},
year = {2003}
}
@book{Theodoridis2009t,
abstract = {This chapter discusses the feature generation stage using data transformations and dimensionality reduction. Feature generation is important in any pattern recognition task. Given a set of measurements, the goal is to discover compact and informative representations of the obtained data. The basic approach followed in this chapter is to transform a given set of measurements to a new set of features. If the transform is suitably chosen, transform domain features can exhibit high information packing properties compared with the original input samples. The chapter reviews Karhunen–Lo{\`{e}}ve transform and the singular value decomposition as dimensionality reduction techniques. The independent component analysis, nonnegative matrix factorization, and nonlinear dimensionality reduction techniques are presented. Then the discrete Fourier transform, discrete cosine transform, discrete sine transform, Hadamard, and Haar transforms are defined. The rest of the chapter focuses on the discrete time wavelet transform. The chapter also shows that the Fourier transform is just one of the tools from a palette of possible transforms.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50008-6},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(6).pdf:pdf},
isbn = {9781597492720},
pages = {323--409},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500086},
year = {2009}
}
@misc{Bloomberg2008,
abstract = {We describe some observations on the practical implementation of the median cut color quantization algorithm, suitably modiﬁed for accurate color rendering. The RGB color space is successively divided in such a way that colors with visual signiﬁcance, even if relatively small in population, are given representatives in the colormap. Appropriately modiﬁed, median cut quantization is nearly as good as our best octree quantization. As with octree quantization, error-diffusion dithering is useful for reducing posterization in large regions with a slow variation in color.},
author = {Bloomberg, Dan S.},
keywords = {color quantisation,mmcq,modified median cut},
mendeley-tags = {color quantisation,mmcq,modified median cut},
title = {{Color quantization using modified median cut}},
url = {http://www.leptonica.com/papers/mediancut.pdf},
year = {2008}
}
@article{Canny1986,
abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
author = {Canny, John},
doi = {10.1109/TPAMI.1986.4767851},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Edge detection,feature extraction,image processing,machine vision,multiscale image analysis},
number = {6},
pages = {679--698},
pmid = {21869365},
title = {{A Computational Approach to Edge Detection}},
volume = {PAMI-8},
year = {1986}
}
@phdthesis{Pouladzadeh2013,
abstract = {As people across the globe are becoming more interested in watching their weight, eating more healthily, and avoiding obesity, a system that can measure calories and nutrition in everyday meals can be very useful. Recently, there has been an increase in the usage of personal mobile technology such as smartphones or tablets, which users carry with them practically all the time. In this paper, we proposed a food calorie and nutrition measurement system that can help patients and dieticians to measure and manage daily food intake. Our system is built on food image processing and uses nutritional fact tables. Via a special calibration technique, our system uses the built-in camera of such mobile devices and records a photo of the food before and after eating it in order to measure the consumption of calorie and nutrient components. The proposed algorithm used color, texture and contour segmentation and extracted important features such as shape, color, size and texture. Using various combinations of these features and applying a support vector machine as a classifier, a good classification was achieved and simulation results show that the algorithm recognizes food categories with an accuracy rate of 92.2{\%}, on average.},
author = {Pouladzadeh, Parisa},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pouladzadeh - 2013 - An Image Processing and Pattern Analysis Approach for Food Recognition.pdf:pdf},
isbn = {http://hdl.handle.net/10393/23677},
keywords = {Classification,Food recognition,Segmantation},
language = {en},
school = {University of Ottawa},
title = {{An Image Processing and Pattern Analysis Approach for Food Recognition}},
url = {http://www.ruor.uottawa.ca/handle/10393/23677},
year = {2013}
}
@article{Ferenc2005,
abstract = {Design patterns present good solutions to frequently occurring problems in object-oriented software design. Thus their correct application in a system's design may significantly improve its internal quality attributes such as reusability and maintainability. In software maintenance the existence of up-to-date documentation is crucial, so the discovery of as yet unknown design pattern instances can help improve the documentation. Hence a reliable design pattern recognition system is very desirable. However simpler methods (based on pattern matching) may give imprecise results due to the vague nature of the patterns' structural description. In previous work we presented a pattern matching-based system using the Columbus framework with which we were able to find pattern instances from the source code by considering the patterns' structural descriptions only, and therefore we could not identify false hits and distinguish similar design patterns such as State and Strategy. In the present work we use machine learning to enhance pattern mining by filtering out as many false hits as possible. To do so we distinguish true and false pattern instances with the help of a learning database created by manually tagging a large C++ system.},
author = {Ferenc, Rudolf and Beszedes, A and Fulop, L and Lele, J},
doi = {10.1109/ICSM.2005.40},
isbn = {1063-6773 0-7695-2368-4},
journal = {Icsm 2005: Proceedings of the 21st Ieee International Conference on Software Maintenance},
keywords = {c,columbus,design patterns,machine learning},
pages = {295--304},
title = {{Design pattern mining enhanced by machine learning}},
year = {2005}
}
@article{Kitamura2009,
abstract = {With the increase of the number of food images on the Internet, we have been developing a food-logging system which has an automated analysis function as a Web application. It can distinguish food images from other images, analyze the food balance, and visualize the log. In this paper, we demonstrate how the performance can be improved by the personalized models. Because our Web application has an interface to review and correct the food analysis results, the generation of the personalized models can be done on-line. Experimental results using two hundred images showed that the extracted image feature vectors differ from user to user but on the other hand the feature vectors and the food balance of each user have a strong correlation. Therefore, the accuracy of the food balance estimation was improved from 37{\%} to 42{\%} on average by the personalized classifier. Copyright 2009 ACM.},
author = {Kitamura, Keigo and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
doi = {10.1145/1630995.1631001},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kitamura, Yamasaki, Aizawa - 2009 - FoodLog Capture, Analysis and Retrieval of Personal Food Images via Web.pdf:pdf},
isbn = {9781605587639},
journal = {Proceedings of the ACM multimedia 2009 workshop on Multimedia for cooking and eating activities - CEA '09},
keywords = {Food,Life-log,Multimedia interfaces},
pages = {23},
title = {{FoodLog: Capture, Analysis and Retrieval of Personal Food Images via Web}},
url = {http://portal.acm.org/citation.cfm?doid=1630995.1631001},
year = {2009}
}
@article{Savakar2012,
abstract = {This paper presents an recognition and classification of similar looking food grain images using artificial neural networks. Schemes for visual classification usually proceed in two stages. First, features are extracted which represents the image and Second, a classifier is applied to the extracted features to reach a decision regarding the represented type of images. We have considered four pairs of eight different types of similar looking commonly available Indian food grain images namely Jira, Badesoup, Mongdaal Woduddal. Ragi, Mustard, Soya, and Alasandi. The algorithms are developed to extract 18 color and 27 texture features. A Back Propagation Neural Network (BPNN) is used to classify and recognize the Food grain image samples using three different types of feature sets, viz, color, texture, combination of both color and texture features. The study reveals that the combination of color and texture features are out performed the individual color and texture features in recognition and classification of different similar looking food grain images samples.},
author = {Savakar, Dayanand},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Savakar - 2012 - Recognition and Classification of Similar Looking Food Grain Images using Artificial Neural Networks.pdf:pdf},
journal = {Journal of Applied Computer Science {\&} Mathematics, no. 13 (6) /2012, Suceava},
keywords = {Feature extraction,Similar looking food grain images,artificial neural networks},
title = {{Recognition and Classification of Similar Looking Food Grain Images using Artificial Neural Networks}},
url = {http://jacs.usv.ro/getpdf.php?paperid=13{\_}9},
volume = {13},
year = {2012}
}
@article{Conneau2018a,
abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. "Downstream" tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
annote = {(probing) Tasks to test sentence embeddings

here is a general consensus in the field that the simple approach of directly averaging a sentence's word vectors (so-called Bag-of-Word approach) gives a strong baseline for many downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1805.01070},
author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"{i}}c and Baroni, Marco},
eprint = {1805.01070},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Conneau et al. - 2018 - What you can cram into a single vector Probing sentence embeddings for linguistic properties.pdf:pdf},
keywords = {sentence embeddings},
mendeley-tags = {sentence embeddings},
month = {may},
title = {{What you can cram into a single vector: Probing sentence embeddings for linguistic properties}},
url = {http://arxiv.org/abs/1805.01070},
year = {2018}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Osindero, Teh - 2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Algorithms,Animals,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16764513{\%}5Cnhttp://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527},
volume = {18},
year = {2006}
}
@article{Hinton2010,
abstract = {(by JL)Les recettes de cuisine de Hinton!Pas de d{\'{e}}tour par la th{\'{e}}orie, mais Hinton livre ses intuitions et son exp{\'{e}}rience.Des conseils pratiques sur comment entrainer les r{\'{e}}seaux, et comment v{\'{e}}rifier que l'entrainement se passe bien (notamment, des conseils pour la visualisation de ce qui se passe...).},
author = {Hinton, Geoffrey},
doi = {10.1007/978-3-642-35289-8_32},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hinton - 2010 - A Practical Guide to Training Restricted Boltzmann Machines A Practical Guide to Training Restricted Boltzmann Machines.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {364235288X},
journal = {Computer},
number = {3},
pages = {1},
pmid = {1000104336},
title = {{A Practical Guide to Training Restricted Boltzmann Machines A Practical Guide to Training Restricted Boltzmann Machines}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.170.9573{\&}rep=rep1{\&}type=pdf},
volume = {9},
year = {2010}
}
@article{Jacobs1995a,
abstract = {We present a method for searching in an image database using a query image that is similar to the intended target. The query image may be a hand-drawn sketch or a (potentially low-quality) scan of the image to be retrieved. Our searching algorithm makes use of multiresolution wavelet decompositions of the query and database images. The coefficients of these decompositions are distilled into small 'signatures' for each image. We introduce an 'image querying metric' that operates on these signatures. This metric essentially compares how many significant wavelet coefficients the query has in common with potential targets. The metric includes parameters that can be tuned, using a statistical analysis, to accommodate the kinds of image distortions found in different types of image queries. The resulting algorithm is simple, requires very little storage overhead for the database of signatures, and is fast enough to be performed on a database of 20,000 images at interactive rates (on standard desktop machines) as a query is sketched. Our experiments with hundreds of queries in databases of 1000 and 20,000 images show dramatic improvement, in both speed and success rate, over using a conventional L1, L2, or color histogram norm.},
author = {Jacobs, Ce and Finkelstein, A and Salesin, Dh},
doi = {10.1145/218380.218454},
isbn = {0897917014},
issn = {0897917014},
journal = {Proceedings of the 22nd annual conference on Computer graphics and interactive techniques},
number = {January},
pages = {277--286},
title = {{Fast multiresolution image querying}},
url = {http://dl.acm.org/citation.cfm?id=218454},
volume = {95},
year = {1995}
}
@article{Lazebnik2006,
abstract = { This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting "spatial pyramid" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba{\&}{\#}146;s "gist" and Lowe{\&}{\#}146;s SIFT descriptors.},
author = {Lazebnik, Svetlana and Schmid, Cordelia and Ponce, Jean},
doi = {10.1109/CVPR.2006.68},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Lazebnik, Schmid, Ponce - 2006 - Beyond bags of features Spatial pyramid matching for recognizing natural scene categories.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2169--2178},
title = {{Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories}},
volume = {2},
year = {2006}
}
@inproceedings{Almaghrabi2012,
abstract = {In this paper, a food nutrition and energy intake recognition system for medical purposes is proposed. This system is built based on food image processing and shape recognition in addition to nutritional fact tables. Recently, countless studies suggested that the usage of technology such as smartphones may enhance the treatments for obesity and overweight patients. Via a special technique, the system records a photo of the food before and after eating in order to estimate the consumption calorie of the selected food and its nutrients components. Our system presents a new instrument in food intake measuring systems which can be useful and effective in obesity management.},
author = {Almaghrabi, Rana and Villalobos, Gregorio and Pouladzadeh, Parisa and Shirmohammadi, Shervin},
booktitle = {2012 IEEE International Instrumentation and Measurement Technology Conference Proceedings},
doi = {10.1109/I2MTC.2012.6229581},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Almaghrabi et al. - 2012 - A novel method for measuring nutrition intake based on food image.pdf:pdf},
isbn = {978-1-4577-1772-7},
issn = {1091-5281},
keywords = {Calories measurement,Databases,Image processing,Image recognition,Obesity,Shape,Shape recognition,Support vector machines,Thumb,Volume Estimation,consumption calorie,energy intake recognition system,food image processing,food technology,image recognition,medical image processing,nutrition intake measuring,obesity,obesity management,overweight patients,shape recognition,smartphones},
language = {English},
mendeley-tags = {Volume Estimation},
month = {may},
pages = {366--370},
publisher = {IEEE},
title = {{A novel method for measuring nutrition intake based on food image}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6229581},
year = {2012}
}
@inproceedings{Meyers2015,
author = {Meyers, Austin and Johnston, Nick and Rathod, Vivek and Korattikara, Anoop and Gorban, Alex and Silberman, Nathan and Guadarrama, Sergio and Papandreou, George and Huang, Jonathan and Murphy, Kevin P.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Meyers et al. - 2015 - Im2Calories Towards an Automated Mobile Vision Food Diary.pdf:pdf},
pages = {1233--1241},
title = {{Im2Calories: Towards an Automated Mobile Vision Food Diary}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Meyers{\_}Im2Calories{\_}Towards{\_}an{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@article{Lee2009,
abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y},
doi = {10.1145/1553374.1553453},
isbn = {9781605585161},
issn = {02643294},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning ICML 09},
pages = {1--8},
pmid = {20957573},
title = {{Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
volume = {2008},
year = {2009}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/LeCun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Shroff2008,
abstract = {We propose DiaWear, a novel assistive mobile phone-based calorie monitoring system to improve the quality of life of diabetes patients and individuals with unique nutrition management needs. Our goal is to achieve improved daily semi-automatic food recognition using a mobile wearable cell phone. DiaWear currently uses a neural network classification scheme to identify food items from a captured image. It is difficult to account for the varying and implicit nature of certain foods using traditional image recognition techniques. To overcome these limitations, we introduce the role of the mobile phone as a platform to gather contextual information from the user and system in obtaining better food recognition.},
author = {Shroff, Geeta and Smailagic, Asim and Siewiorek, Daniel P.},
doi = {10.1109/ISWC.2008.4911602},
isbn = {9781424426379},
issn = {15504816},
journal = {Proceedings - International Symposium on Wearable Computers, ISWC},
pages = {119--120},
title = {{Wearable context-aware food recognition for calorie monitoring}},
year = {2008}
}
@article{Ogden2010,
abstract = {CONTEXT: The prevalence of high body mass index (BMI) among children and adolescents in the United States appeared to plateau between 1999 and 2006.

OBJECTIVES: To provide the most recent estimates of high BMI among children and adolescents and high weight for recumbent length among infants and toddlers and to analyze trends in prevalence between 1999 and 2008.

DESIGN, SETTING, AND PARTICIPANTS: The National Health and Nutrition Examination Survey 2007-2008, a representative sample of the US population with measured heights and weights on 3281 children and adolescents (2 through 19 years of age) and 719 infants and toddlers (birth to 2 years of age).

MAIN OUTCOME MEASURES: Prevalence of high weight for recumbent length ({\textgreater} or = 95th percentile of the Centers for Disease Control and Prevention growth charts) among infants and toddlers. Prevalence of high BMI among children and adolescents defined at 3 levels: BMI for age at or above the 97th percentile, at or above the 95th percentile, and at or above the 85th percentile of the BMI-for-age growth charts. Analyses of trends by age, sex, and race/ethnicity from 1999-2000 to 2007-2008.

RESULTS: In 2007-2008, 9.5{\%} of infants and toddlers (95{\%} confidence interval [CI], 7.3{\%}-11.7{\%}) were at or above the 95th percentile of the weight-for-recumbent-length growth charts. Among children and adolescents aged 2 through 19 years, 11.9{\%} (95{\%} CI, 9.8{\%}-13.9{\%}) were at or above the 97th percentile of the BMI-for-age growth charts; 16.9{\%} (95{\%} CI, 14.1{\%}-19.6{\%}) were at or above the 95th percentile; and 31.7{\%} (95{\%} CI, 29.2{\%}-34.1{\%}) were at or above the 85th percentile of BMI for age. Prevalence estimates differed by age and by race/ethnic group. Trend analyses indicate no significant trend between 1999-2000 and 2007-2008 except at the highest BMI cut point (BMI for age {\textgreater} or = 97th percentile) among all 6- through 19-year-old boys (odds ratio [OR], 1.52; 95{\%} CI, 1.17-2.01) and among non-Hispanic white boys of the same age (OR, 1.87; 95{\%} CI, 1.22-2.94).

CONCLUSION: No statistically significant linear trends in high weight for recumbent length or high BMI were found over the time periods 1999-2000, 2001-2002, 2003-2004, 2005-2006, and 2007-2008 among girls and boys except among the very heaviest 6- through 19-year-old boys.},
author = {Ogden, Cynthia L and Carroll, Margaret D and Curtin, Lester R and Lamb, Molly M and Flegal, Katherine M},
doi = {10.1001/jama.2009.2012},
issn = {1538-3598},
journal = {JAMA},
keywords = {Adolescent,Adult,Body Mass Index,Child,Child, Preschool,Female,Humans,Infant,Infant, Newborn,Male,Nutrition Surveys,Overweight,Overweight: epidemiology,Prevalence,United States,United States: epidemiology,Young Adult},
month = {jan},
number = {3},
pages = {242--9},
pmid = {20071470},
publisher = {American Medical Association},
title = {{Prevalence of high body mass index in US children and adolescents, 2007-2008.}},
url = {http://jama.jamanetwork.com/article.aspx?articleid=185233},
volume = {303},
year = {2010}
}
@article{Wang2009b,
abstract = {Measuring image similarity is a central topic in com- puter vision. In this paper, we learn similarity from Flickr groups and use it to organize photos. Two images are sim- ilar if they are likely to belong to the same Flickr groups. Our approach is enabled by a fast Stochastic Intersection Kernel MAchine (SIKMA) training algorithm, which we propose. This proposed training method will be useful for many vision problems, as it can produce a classifier that is more accurate than a linear classifier, trained on tens of thousands of examples in two minutes. The experimental results show our approach performs better on image match- ing, retrieval, and classification than using conventional vi- sual features.},
author = {Wang, Gang and Hoiem, Derek and Forsyth, David},
isbn = {9781424444199},
journal = {IEEE International Conference on Computer Vision},
keywords = {hand-craft features,image similarity,supervised learning},
mendeley-tags = {hand-craft features,image similarity,supervised learning},
number = {Iccv},
title = {{Learning Image Similarity from {\{}F{\}}lickr Groups Using Stochastic Intersection Kernel Machines}},
year = {2009}
}
@misc{FiorenzaBrady,
abstract = {According to recent Cambridge University research, the global cost of debugging software has risen to {\$}312 billion annually. The research found that, on average, software developers spend 50{\%} of their programming time finding and fixing bugs. When projecting this figure onto the total cost of employing software developers, this inefficiency is estimated to cost the global economy {\$}312 billion per year.},
author = {Brady, Fiorenza},
title = {{Financial Content: Cambridge University study states software bugs cost economy {\$}312 billion per year | CJBS Insight}},
url = {http://insight.jbs.cam.ac.uk/2013/financial-content-cambridge-university-study-states-software-bugs-cost-economy-312-billion-per-year/},
urldate = {2017-01-24},
year = {2013}
}
@inproceedings{Wu2016,
annote = {Software defect prediction},
author = {Wu, Fei and Jing, Xiao-Yuan and Dong, Xiwei and Cao, Jicheng and Xu, Baowen and Ying, Shi},
booktitle = {2016 International Conference on Software Analysis, Testing and Evolution (SATE)},
doi = {10.1109/SATE.2016.24},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Cost-Sensitive Local Collaborative Representation for Software Defect Prediction.pdf:pdf},
isbn = {978-1-5090-4517-4},
keywords = {collaborative,sdp,software defect prediction},
number = {IEEE},
pages = {102--107},
title = {{Cost-Sensitive Local Collaborative Representation for Software Defect Prediction}},
url = {http://ieeexplore.ieee.org/document/7780202/},
year = {2016}
}
@incollection{Theodoridis2009q,
abstract = {This chapter explores the design of linear classifiers, regardless of the underlying distributions describing the training data. The major advantage of linear classifiers is their simplicity and computational attractiveness. The chapter starts with the assumption that all feature vectors from the available classes can be classified correctly using a linear classifier. Techniques are then developed for the computation of the corresponding linear functions. The probability estimation property of the mean square solution, as well as the bias variance dilemma, is briefly mentioned. The basic philosophy underlying the support vector machines is explained. Emphasis is put on the linear separability issue, the perceptron algorithm, and the mean square and least squares solutions. The geometric interpretation offers a better understanding of the SVM theory. The multiclass case for SVM is also presented. Topics of least squares methods, mean square estimation, and logistic discrimination are also explained in the chapter. The perceptron algorithm is explained along with its geometric interpretation, and in the simple two-class case, it is shown that the perceptron algorithm computes the weights of the linear function g(x), provided that the classes are linearly separable.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50005-0},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Chapter 3 Linear Classifiers.pdf:pdf},
isbn = {9781597492720},
pages = {91--150},
publisher = {Elsevier},
title = {{Chapter 3 Linear Classifiers}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500050},
year = {2009}
}
@article{Wang2013,
abstract = {During software development, a developer often needs to discover specific usage patterns of Application Programming Interface (API) methods. However, these usage patterns are often not well documented. To help developers to get such usage patterns, there are approaches proposed to mine client code of the API methods. However, they lack metrics to measure the quality of the mined usage patterns, and the API usage patterns mined by the existing approaches tend to be many and redundant, posing significant barriers for being practical adoption. To address these issues, in this paper, we propose two quality metrics (succinctness and coverage) for mined usage patterns, and further propose a novel approach called Usage Pattern Miner (UP-Miner) that mines succinct and high-coverage usage patterns of API methods from source code. We have evaluated our approach on a large-scale Microsoft codebase. The results show that our approach is effective and outperforms an existing representative approach MAPO. The user studies conducted with Microsoft developers confirm the usefulness of the proposed approach in practice.},
author = {Wang, Jue and Dang, Yingnong and Zhang, Hongyu and Chen, Kai and Xie, Tao and Zhang, Dongmei},
doi = {10.1109/MSR.2013.6624045},
isbn = {9781467329361},
issn = {21601852},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {API usage,Mining software repositories,Sequence mining,Software reuse,Usage pattern},
pages = {319--328},
title = {{Mining succinct and high-coverage API usage patterns from source code}},
year = {2013}
}
@article{Kitamura2010,
abstract = {Food images have been receiving increased attention in recent dietary control methods. We present the current status of our web-based system that can be used as a dietary management support system by ordinary Internet users. The system analyzes image archives of the user to identify images of meals. Further image analysis determines the nutritional composition of these meals and stores the data to form a Foodlog. The user can view the data in different formats, and also edit the data to correct any mistakes that occurred during image analysis. This paper presents detailed analysis of the performance of the current system and proposes an improvement of analysis by pre-classification and personalization. As a result, the accuracy of food balance estimation is significantly improved.},
author = {Kitamura, K. and Silva, C. De and Yamasaki, T. and Aizawa, K.},
doi = {10.1109/ICME.2010.5583021},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kitamura et al. - 2010 - Image processing based approach to food balance analysis for personal food logging.pdf:pdf},
isbn = {978-1-4244-7491-2},
issn = {1945-7871},
journal = {Multimedia and Expo (ICME), 2010 IEEE International Conference on},
keywords = {Food,Image Processing,life log},
pages = {625--630},
title = {{Image processing based approach to food balance analysis for personal food logging}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5583021},
year = {2010}
}
@article{Livingstone2007,
author = {Livingstone, M. B. E. and Robson, P. J. and Wallace, J. M. W.},
doi = {10.1079/BJN20041169},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Livingstone, Robson, Wallace - 2007 - Issues in dietary intake assessment of children and adolescents.pdf:pdf},
issn = {0007-1145},
journal = {British Journal of Nutrition},
keywords = {Adolescents,Children,Dietary assessment},
language = {English},
month = {mar},
number = {S2},
pages = {S213},
publisher = {Cambridge University Press},
title = {{Issues in dietary intake assessment of children and adolescents}},
url = {http://journals.cambridge.org/abstract{\_}S0007114504002326},
volume = {92},
year = {2007}
}
@inproceedings{Nam2013,
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. {\textcopyright} 2013 IEEE.},
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
booktitle = {Proceedings - International Conference on Software Engineering},
keywords = {cross-project defect prediction,empirical software engineering,transfer learning},
pages = {382--391},
title = {{Transfer defect learning}},
year = {2013}
}
@article{He2017,
abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
annote = {Summary

unsuppervised aspect extraction. goal is to learn a set of aspect embeddings where each aspect can be interpreted by looking at the nearest word},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
doi = {10.18653/v1/P17-1036},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2017 - An Unsupervised Neural Attention Model for Aspect Extraction.pdf:pdf},
isbn = {9781945626753},
journal = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
keywords = {LDA,attention,unsuppervised},
mendeley-tags = {LDA,attention,unsuppervised},
pages = {388--397},
title = {{An Unsupervised Neural Attention Model for Aspect Extraction}},
url = {http://aclweb.org/anthology/P17-1036},
year = {2017}
}
@techreport{Kaiser2018,
abstract = {Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. We present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neu-ral machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.},
archivePrefix = {arXiv},
arxivId = {1803.03382v6},
author = {Kaiser, {\L}ukasz and Roy, Aurko and Vaswani, Ashish and Parmar, Niki and Bengio, Samy and Uszkoreit, Jakob and Shazeer, Noam},
eprint = {1803.03382v6},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kaiser et al. - 2018 - Fast Decoding in Sequence Models Using Discrete Latent Variables.pdf:pdf},
title = {{Fast Decoding in Sequence Models Using Discrete Latent Variables}},
url = {https://arxiv.org/pdf/1803.03382.pdf},
year = {2018}
}
@article{McCann2017,
abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
archivePrefix = {arXiv},
arxivId = {1708.00107},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
eprint = {1708.00107},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/McCann et al. - 2017 - Learned in Translation Contextualized Word Vectors.pdf:pdf},
issn = {10495258},
keywords = {CoVe,GloVe,LSTM,Question classification,SNLI,SST,Sequence-to-sequence,TREC,attention,context vectors,encoder,entailment,sentiment analysis,transfer learning},
mendeley-tags = {CoVe,GloVe,LSTM,Question classification,SNLI,SST,Sequence-to-sequence,TREC,attention,context vectors,encoder,entailment,sentiment analysis,transfer learning},
number = {Nips},
pages = {1--12},
title = {{Learned in Translation: Contextualized Word Vectors}},
url = {http://arxiv.org/abs/1708.00107},
year = {2017}
}
@article{Carreira-Perpinan2005,
abstract = {Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called con- trastive divergence (CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution.},
author = {Carreira-Perpi{\~{n}}{\'{a}}n, M a and Hinton, G E},
doi = {10.3389/conf.neuro.10.2009.14.121},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Carreira-Perpi{\~{n}}{\'{a}}n, Hinton - 2005 - On Contrastive Divergence Learning.pdf:pdf},
isbn = {0818681322},
issn = {16625188},
journal = {Artificial Intelligence and Statistics},
pages = {17},
title = {{On Contrastive Divergence Learning}},
url = {http://learning.cs.toronto.edu/{~}hinton/absps/cdmiguel.pdf},
volume = {10},
year = {2005}
}
@book{Theodoridis2009g,
abstract = {This chapter examines feature generation with a focus on image and audio classification. Feature generation is a procedure that computes new variables that in one way or another originate from the stored values of the image array I(m, n). Some of the feature generation techniques can be considered common and can be applicable in both visual and audio modalities. A large number of features are the result of different approaches to exploit the specific nature of the signals and encode the required classification information in a more efficient way. This chapter also focuses on first- and second-order statistics features as well as the run-length method. The chapter discusses typical features used to characterize and classify audio information. The chapter presents a description of statistical properties of signals and images and the ways these can be exploited to extract information-rich features for classification. The chapter examines whether the notion of self-similarity is extendable to stochastic processes and, if it is, how useful it can be. The chain code for shape description is discussed in this chapter. Computer exercises are then offered to generate these features and use them for classification for some case studies.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50009-8},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(7).pdf:pdf},
isbn = {9781597492720},
pages = {411--479},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500098},
year = {2009}
}
@incollection{Christodoulidis2015,
abstract = {Diet management is a key factor for the prevention and treatment of diet-related chronic diseases. Computer vision systems aim to provide automated food intake assessment using meal images. We propose a method for the recognition of already segmented food items in meal images. The method uses a 6-layer deep convolutional neural network to classify food image patches. For each food item, overlapping patches are extracted and classified and the class with the majority of votes is assigned to it. Experiments on a manually annotated dataset with 573 food items justified the choice of the involved components and proved the effectiveness of the proposed system yielding an overall accuracy of 84.9{\%}.},
annote = {Sehr viele Details zur Konfiguration des NN},
author = {Christodoulidis, Stergios and Anthimopoulos, Marios},
booktitle = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
doi = {10.1007/978-3-319-23222-5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Christodoulidis, Anthimopoulos - 2015 - Food Recognition for Dietary Assessment Using Deep Convolutional Neural Networks.pdf:pdf;:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Christodoulidis, Anthimopoulos - 2015 - Food Recognition for Dietary Assessment Using Deep Convolutional Neural Networks(2).pdf:pdf},
isbn = {978-3-319-23221-8},
keywords = {agement,convolutional neural networks,dietary man-,food recognition,machine learning},
pages = {458--465},
title = {{Food Recognition for Dietary Assessment Using Deep Convolutional Neural Networks}},
url = {http://link.springer.com/10.1007/978-3-319-23222-5},
volume = {9281},
year = {2015}
}
@techreport{Liu,
abstract = {We show that generating English Wikipedia articles can be approached as a multi-document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
archivePrefix = {arXiv},
arxivId = {1801.10198v1},
author = {Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, {\L}ukasz and Shazeer, Noam},
eprint = {1801.10198v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - Unknown - GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES.pdf:pdf},
isbn = {1801.10198v1},
title = {{GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES}},
url = {https://en.wikipedia.org/wiki/Wikipedia:Manual{\_}of{\_}Style}
}
@article{Srivastava2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@article{Shotton2013,
abstract = {We propose a new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state of the art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.},
author = {Shotton, Jamie and Fitzgibbon, Andrew and Cook, Mat and Sharp, Toby and Finocchio, Mark and Moore, Richard and Kipman, Alex and Blake, Andrew},
journal = {Studies in Computational Intelligence},
pages = {119--135},
title = {{Real-time human pose recognition in parts from single depth images}},
volume = {411},
year = {2013}
}
@article{Bay2006,
abstract = {Abstract. In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Ro- bust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, H. and Tuytelaars, T. and {Van Gool}, L.},
doi = {10.1007/11744023_32},
isbn = {3540338322},
issn = {03029743},
journal = {Lecture notes in computer science},
pages = {14},
pmid = {16081019},
title = {{Speeded up robust features}},
url = {http://www.springerlink.com/index/e580h2k58434p02k.pdf},
volume = {3951},
year = {2006}
}
@article{tca,
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. {\textcopyright} 2013 IEEE.},
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
doi = {10.1109/ICSE.2013.6606584},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Nam, Pan, Kim - 2013 - Transfer defect learning.pdf:pdf},
isbn = {9781467330763},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {cross-project defect prediction,empirical software engineering,transfer learning},
pages = {382--391},
title = {{Transfer defect learning}},
year = {2013}
}
@article{Dieleman2015,
author = {Dieleman, Sander and Heilman, Michael and Kelly, Jack and Thoma, Martin and Rasul, Dr. Kashif and Battenberg, Eric and Weideman, Hendrik and S{\o}nderby, S{\o}ren Kaae and Instagibbs and Britefury and Raffel, Colin and Degrave, Jonas and Peterderivaz and Jon and Fauw, Jeffrey De and Diogo149 and Nouri, Daniel and Schl{\"{u}}ter, Jan and Maturana, Daniel and CongLiu and Olson, Eben and McFee, Brian and Takacsg84},
doi = {10.5281/zenodo.27878},
month = {aug},
title = {{Lasagne: First release.}},
url = {http://zenodo.org/record/27878},
year = {2015}
}
@article{Pouladzadeh2014,
abstract = {As people across the globe are becoming more interested in watching their weight, eating more healthy, and avoiding obesity, a system that can measure calories and nutrition in every day meals can be very useful. In this paper, we propose a food calorie and nutrition measurement system that can help patients and dietitians to measure and manage daily food intake. Our system is built on food image processing and uses nutritional fact tables. Recently, there has been an increase in the usage of personal mobile technology such as smartphones or tablets, which users carry with them practically all the time. Via a special calibration technique, our system uses the built-in camera of such mobile devices and records a photo of the food before and after eating it to measure the consumption of calorie and nutrient components. Our results show that the accuracy of our system is acceptable and it will greatly improve and facilitate current manual calorie measurement techniques.},
author = {Pouladzadeh, Parisa and Shirmohammadi, Shervin and Al-Maghrabi, Rana},
doi = {10.1109/TIM.2014.2303533},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pouladzadeh, Shirmohammadi, Al-Maghrabi - 2014 - Measuring Calorie and Nutrition From Food Image.pdf:pdf},
issn = {0018-9456},
journal = {IEEE Transactions on Instrumentation and Measurement},
keywords = {Accuracy,Calorie measurement,Feature extraction,Image color analysis,Image segmentation,Support vector machines,Thumb,Volume measurement,biomedical measurement,built-in camera,calibration,calibration technique,cameras,computerised instrumentation,food calorie measurement system,food image processing,food nutrition measurement system,food photo recording,food products,medical image processing,mobile computing,mobile device,nutritional fact tables,obesity management,obesity management.,patient treatment,personal mobile technology,volume measurement},
month = {aug},
number = {8},
pages = {1947--1956},
shorttitle = {Instrumentation and Measurement, IEEE Transactions},
title = {{Measuring Calorie and Nutrition From Food Image}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6748066},
volume = {63},
year = {2014}
}
@article{Mbeddings2017,
abstract = {The success of neural network methods for computing word embeddings has mo- tivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embed- dings and basic linear regression. The method ofWieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsuper- vised sentence embedding is a formidable baseline: Use word embeddings com- puted using one of the popular methods on unlabeled corpus like Wikipedia, rep- resent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10{\%} to 30{\%} in textual similarity tasks, and beats sophisticated supervised methods in- cluding RNN's and LSTM's. It even improves Wieting et al.'s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsu- pervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new “smoothing” terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts. 1},
author = {Arora, Sanjeev and Yingyu, Liang and Ma, Tengyu},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Arora, Yingyu, Ma - 2017 - A Simple but Tough-to-Beat Baseline for Sentence Embeddings.pdf:pdf},
journal = {Iclr 2017},
pages = {1--16},
title = {{A Simple but Tough-to-Beat Baseline for Sentence Embeddings}},
year = {2017}
}
@article{Keller1985,
abstract = {Classification of objects is an important area of research and application in a variety of fields. In the presence of full knowledge of the underlying probabilities, Bayes decision theory gives optimal error rates. In those cases where this information is not present, many algorithms make use of distance or similarity among samples as a means of classification. The K-nearest neighbor decision rule has often been used in these pattern recognition problems. One of the difficulties that arises when utilizing this technique is that each of the labeled samples is given equal importance in deciding the class memberships of the pattern to be classified, regardless of their `typicalness'. The theory of fuzzy sets is introduced into the K-nearest neighbor technique to develop a fuzzy version of the algorithm. Three methods of assigning fuzzy memberships to the labeled samples are proposed, and experimental results and comparisons to the crisp version are presented.},
author = {Keller, James M. and Gray, Michael R.},
doi = {10.1109/TSMC.1985.6313426},
isbn = {0018-9472},
issn = {21682909},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {4},
pages = {580--585},
title = {{A Fuzzy K-Nearest Neighbor Algorithm}},
volume = {SMC-15},
year = {1985}
}
@misc{Krem,
abstract = {Ein Start-up aus Israel hat ein Ger{\"{a}}t gebaut, das an den Tricorder aus "Star Trek" erinnert. Es soll feste oder fl{\"{u}}ssige Stoffe in Sekundenschnelle analysieren - und das klappt verbl{\"{u}}ffend gut.},
author = {Kremp, Matthias},
keywords = {Mobile World Congress,Sprectral analyse,Tricorder},
mendeley-tags = {Mobile World Congress,Sprectral analyse,Tricorder},
title = {{Mobile World Congress 2016: Ist das wirklich ein Tricorder? - SPIEGEL ONLINE}},
url = {http://www.spiegel.de/netzwelt/gadgets/mobile-world-congress-2016-ist-das-wirklich-ein-tricorder-a-1078790.html},
urldate = {2016-02-23},
year = {2016}
}
@article{Fletcher2009,
abstract = {This document has been written in an attempt to make the Support Vector Machines (SVM), initially conceived of by Cortes and Vapnik [1], as sim- ple to understand as possible for those with minimal experience of Machine Learning. It assumes basic mathematical knowledge in areas such as cal- culus, vector geometry and Lagrange multipliers. The document has been split into Theory and Application sections so that it is obvious, after the maths has been dealt with, how to actually apply the SVM for the different forms of problem that each section is centred on. The document's first section details the problem of classification for linearly separable data and introduces the concept of margin and the essence of SVM - margin maximization. The methodology of the SVM is then extended to data which is not fully linearly separable. This soft margin SVM introduces the idea of slack variables and the trade-off between maximizing the margin and minimizing the number of misclassified variables in the second section. The third section develops the concept of SVM further so that the technique can be used for regression. The fourth section explains the other salient feature of SVM - the Kernel Trick. It explains how incorporation of this mathematical sleight of hand allows SVM to classify and regress nonlinear data.},
author = {Fletcher, Tristan},
doi = {10.1002/9780470503065.app2},
isbn = {9780470371923},
journal = {Online]. http://sutikno. blog. undip. ac. id/files/2011/11/SVM-Explained. pdf.[Accessed 06 06 2013]},
pages = {1--19},
title = {{Support Vector Machines Explained}},
url = {http://sutikno.blog.undip.ac.id/files/2011/11/SVM-Explained.pdf},
year = {2009}
}
@article{Kumar2015,
abstract = {This paper deals with automatic systems for image recipe recognition. For this purpose, we compare and evaluate leading vision-based and text-based technologies on a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories. Each item in this dataset is represented by one image plus textual information. We present deep experiments of recipe recognition on our dataset using visual, textual information and fusion. Additionally, we present experiments with text-based embedding technology to represent any food word in a semantical continuous space. We also compare our dataset features with a twin dataset provided by ETHZ university: we revisit their data collection protocols and carry out transfer learning schemes to highlight similarities and differences between both datasets. Finally, we propose a real application for daily users to identify recipes. This application is a web search engine that allows any mobile device to send a query image and retrieve the most relevant recipes in our dataset.},
annote = {UPMC Food 101},
author = {Kumar, Devinder and Thome, Nicolas and Cord, Matthieu and Precioso, Frederic},
doi = {10.1109/ICMEW.2015.7169757},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2015 - Recipe recognition with large multimodal food dataset.pdf:pdf},
isbn = {978-1-4799-7079-7},
journal = {2015 IEEE International Conference on Multimedia {\&} Expo Workshops (ICMEW)},
keywords = {Accuracy,Feature extraction,Google,HTML,Internet,Protocols,Training,UPMC Food-101,Visualization,Web search engine,computer vision,data collection protocols,food categories,food technology,image fusion,image recipe recognition,image recognition,image retrieval,mobile device,multimodal dataset,multimodal food dataset,query image,relevant recipe retrieval,search engines,semantical continuous space,text analysis,text-based embedding technology,text-based technology,transfer learning schemes,vision-based technology},
number = {1},
pages = {1--6},
title = {{Recipe recognition with large multimodal food dataset}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7169757},
year = {2015}
}
@article{Bettenburg2008,
abstract = {In a survey we found that most developers have experienced duplicated bug reports, however, only few considered them as a serious problem. This contradicts popular wisdom that considers bug duplicates as a serious problem for open source projects. In the survey, developers also pointed out that the additional information provided by duplicates helps to resolve bugs quicker. In this paper, we therefore propose to merge bug duplicates, rather than treating them separately. We quantify the amount of information that is added for developers and show that automatic triaging can be improved as well. In addition, we discuss the different reasons why users submit duplicate bug reports in the first place.},
author = {Bettenburg, Nicolas and Premraj, Rahul and Zimmermann, Thomas and Kim, Sunghun},
doi = {10.1109/ICSM.2008.4658082},
isbn = {9781424426140},
issn = {1063-6773},
journal = {IEEE International Conference on Software Maintenance, ICSM},
keywords = {[Electronic Manuscript]},
pages = {337--345},
title = {{Duplicate bug reports considered harmful... Really?}},
year = {2008}
}
@article{Probst2015,
abstract = {Dietary assessment, while traditionally based on pen-and-paper, is rapidly moving towards automatic approaches. This study describes an Australian automatic food record method and its prototype for dietary assessment via the use of a mobile phone and techniques of image processing and pattern recognition. Common visual features including scale invariant feature transformation (SIFT), local binary patterns (LBP), and colour are used for describing food images. The popular bag-of-words (BoW) model is employed for recognizing the images taken by a mobile phone for dietary assessment. Technical details are provided together with discussions on the issues and future work.},
author = {Probst, Yasmine and Nguyen, Duc and Tran, Minh and Li, Wanqing},
doi = {10.3390/nu7085274},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Probst et al. - 2015 - Dietary Assessment on a Mobile Phone Using Image Processing and Pattern Recognition Techniques Algorithm Design a.pdf:pdf},
issn = {2072-6643},
journal = {Nutrients},
keywords = {"mHealth,food image,food image",food record,image processing,mhealth,pattern recognition},
number = {8},
pages = {6128--6138},
title = {{Dietary Assessment on a Mobile Phone Using Image Processing and Pattern Recognition Techniques: Algorithm Design and System Prototyping}},
url = {http://www.mdpi.com/2072-6643/7/8/5274/},
volume = {7},
year = {2015}
}
@article{Morvant2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7796v1},
author = {Morvant, Emilie and Habrard, Amaury and Ayache, St??phane},
doi = {10.1007/978-3-662-44415-3_16},
eprint = {arXiv:1404.7796v1},
isbn = {9783662444146},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Classifier fusion,Majority vote,Multimedia analysis,Ranking},
pages = {153--162},
title = {{Majority vote of diverse classifiers for late fusion}},
volume = {8621 LNCS},
year = {2014}
}
@article{Chih-WeiHsuChih-ChungChang2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@inproceedings{Chen2009,
abstract = {We introduce the first visual dataset of fast foods with a total of 4,545 still images, 606 stereo pairs, 303 360° videos for structure from motion, and 27 privacy-preserving videos of eating events of volunteers. This work was motivated by research on fast food recognition for dietary assessment. The data was collected by obtaining three instances of 101 foods from 11 popular fast food chains, and capturing images and videos in both restaurant conditions and a controlled lab setting. We benchmark the dataset using two standard approaches, color histogram and bag of SIFT features in conjunction with a discriminative classifier. Our dataset and the benchmarks are designed to stimulate research in this area and will be released freely to the research community.},
author = {Chen, Mei and Dhingra, Kapil and Wu, Wen and Yang, Lei and Sukthankar, Rahul and Yang, Jie},
booktitle = {ICIP'09 Proceedings of the 16th IEEE international conference on Image processing},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2009 - PFID pittsburgh fast-food image dataset.pdf:pdf},
isbn = {978-1-4244-5653-6},
keywords = {food image dataset,object recognition},
month = {nov},
pages = {289--292},
publisher = {IEEE Press},
title = {{PFID: pittsburgh fast-food image dataset}},
url = {http://dl.acm.org/citation.cfm?id=1818719.1818817},
year = {2009}
}
@misc{promiserepo,
author = {Menzies, T. and Krishna, R. and Pryor, D},
title = {{The Promise Repository of Empirical Software Engineering Data}},
year = {2015}
}
@misc{Kim2013,
abstract = {Data analysis of a food item based on one or more digital images of the food item is disclosed. In one embodiment, the method comprises displaying, on a display unit of the smart device, first and second digital images of a meal, where the first digital image is captured before the second digital image. The method also comprises determining a volume of each food item in the first digital image and a volume of each food item in the second digital image by analyzing the first digital image and the second digital image using a digital image processing technique. The method further comprises generating, on the display unit, an amount of intake for the meal based on a difference between the volume of each food item in the first digital image and the volume of each food item in the second digital image.},
annote = {Erkl{\"{a}}rt nicht wie die Klassifizierung funktionieren soll. "Image Processing" wird erw{\"{a}}hnt, aber nicht weiter ausgef{\"{u}}hrt.},
author = {Kim, Kyungjin and Lee, Kiwon and Cho, Sungil and Hong, Jiyoung and Kim, Sungeun},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2013 - Analysis of food items captured in digital images.pdf:pdf},
month = {dec},
title = {{Analysis of food items captured in digital images}},
url = {https://www.google.com.ar/patents/US20130335418},
year = {2013}
}
@article{Deng2001,
author = {Deng, Yining and Manjunath, B S},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Deng, Manjunath - 2001 - Unsupervised Segmentation of Color-TextureRegions in Images and Video.pdf:pdf},
journal = {Ieee Transactions on Pattern Analysis and Machine Intelligence},
pages = {800--810},
title = {{Unsupervised Segmentation of Color-TextureRegions in Images and Video}},
volume = {23},
year = {2001}
}
@article{Ojala1999,
abstract = {This paper presents an unsupervised texture segmentation method, which uses distributions of local binary patterns and pattern contrasts for measuring the similarity of adjacent image regions during the segmentation process. Nonparametric log-likelihood test, the G statistic, is engaged as a pseudo-metric for comparing feature distributions. A region-based algorithm is developed for coarse image segmentation and a pixelwise classification scheme for improving localization of region boundaries. The performance of the method is evaluated with various types of test images.},
author = {Ojala, Timo and Pietik{\"{a}}inen, Matti},
doi = {10.1016/S0031-3203(98)00038-7},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {contrast,feature distribution,g statistic,local binary pattern,spatial operator,texture segmentation},
number = {3},
pages = {477--486},
title = {{Unsupervised texture segmentation using feature distributions}},
volume = {32},
year = {1999}
}
@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza- tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar- ison with a large previous study that used grid search and manual search to configure neural net- works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con- figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput”methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that randomsearch is a natural base- line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Bengio, Yoshua},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
volume = {13},
year = {2012}
}
@article{Kajiwara2015,
abstract = {This paper presents a system for assisting nutrition management for solitary elderly persons. Since dealing with diseases is one of the important issues for solitary elderly, their health control in daily life has been in focus in recent years. As preprocessing to develop a nutrition management system for solitary elderly, systems for discriminating the category of a food image have been proposed. However, classification of food images is still a challenging task due to the variety of their shape and color. In order to improve the performance on the classification, we propose three regions of interests extracted by HSV-AKAZE. The three regions are used to extract various local features such as AKAZE, HSV-AKAZE, and color information, enhances the classification performance. Evaluation experiments for 2000 food images in 50 categories have shown that the classification accuracy has increased by 8{\%} compared with the existing system.},
author = {Kajiwara, Yusuke and Nakamura, Munehiro and Kimura, Haruhiko},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kajiwara, Nakamura, Kimura - 2015 - Classification of Single-Food Images by Combining Local HSV-AKAZE Features and Global Features.pdf:pdf},
journal = {International Research Journal of Computer Science (IRJCS)},
keywords = {hsv-akaze,m achine learning,roi,single food image,solitary elderly p ersons},
number = {1},
pages = {12--17},
title = {{Classification of Single-Food Images by Combining Local HSV-AKAZE Features and Global Features}},
volume = {2},
year = {2015}
}
@book{Theodoridis2009h,
abstract = {This chapter discusses clustering validity stage of a clustering procedure. The chapter presents methods suitable for quantitative evaluation of the results of a clustering algorithm, known under the general term cluster validity. Cluster validity can be approached in three possible directions. First is to evaluate C (where C is the clustering structure resulting from the application of a clustering algorithm on data set X) in terms of an independently drawn structure, which is imposed on X a priori and reflects intuition about the clustering structure of X. The criteria used for the evaluation of this kind are called external criteria. External criteria may be used to measure the degree to which the available data confirm a prespecified structure, without applying any clustering algorithm to X. The criteria used for this kind of evaluation are called internal criteria. Last approach is to evaluate C by comparing it with other clustering structures, resulting from the application of the same clustering algorithm, but with different parameter values, or of other clustering algorithms to X. Criteria of this kind are called relative criteria. This chapter also focuses on the definitions of internal, external, and relative criteria and the random hypotheses used in each case. Indices, adopted in the framework of external and internal criteria, are presented, and examples are provided showing the use of these indices.},
author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
booktitle = {Pattern Recognition},
doi = {10.1016/B978-1-59749-272-0.50018-9},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Theodoridis, Koutroumbas - 2009 - Pattern Recognition(16).pdf:pdf},
isbn = {9781597492720},
pages = {863--913},
publisher = {Elsevier},
title = {{Pattern Recognition}},
url = {http://www.sciencedirect.com/science/article/pii/B9781597492720500189},
year = {2009}
}
@inproceedings{Khan2013,
author = {Khan, Rahat and van de Weijer, Joost and {Shahbaz Khan}, Fahad and Muselet, Damien and Ducottet, Christophe and Barat, Cecile},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Khan et al. - 2013 - Discriminative Color Descriptors.pdf:pdf},
pages = {2866--2873},
title = {{Discriminative Color Descriptors}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2013/html/Khan{\_}Discriminative{\_}Color{\_}Descriptors{\_}2013{\_}CVPR{\_}paper.html},
year = {2013}
}
@book{Bishop2006a,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Ma2016,
abstract = {This paper studies contrastive divergence (CD) learning algorithm and proposes a new algorithm for training restricted Boltzmann machines (RBMs). We derive that CD is a biased estimator of the log-likelihood gradient method and make an analysis of the bias. Meanwhile, we propose a new learning algorithm called average contrastive divergence (ACD) for training RBMs. It is an improved CD algorithm, and it is different from the traditional CD algorithm. Finally, we obtain some experimental results. The results show that the new algorithm is a better approximation of the log-likelihood gradient method and outperforms the traditional CD algorithm.},
author = {Ma, Xuesi and Wang, Xiaojie},
doi = {10.3390/e18010035},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Ma, Wang - 2016 - Average Contrastive Divergence for Training Restricted Boltzmann Machines.pdf:pdf},
issn = {1099-4300},
journal = {Entropy},
keywords = {average contrastive divergence,contrastive divergence,gradient method,log-likelihood,restricted Boltzmann machines},
language = {en},
month = {jan},
number = {2},
pages = {35},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Average Contrastive Divergence for Training Restricted Boltzmann Machines}},
url = {http://www.mdpi.com/1099-4300/18/1/35/htm},
volume = {18},
year = {2016}
}
@article{Weiss2010,
author = {Weiss, Rick and Stumbo, Phyllis J and Divakaran, Ajay},
doi = {10.1016/j.jada.2009.10.011},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Weiss, Stumbo, Divakaran - 2010 - Automatic food documentation and volume computation using digital imaging and electronic transmission.pdf:pdf},
issn = {1878-3570},
journal = {Journal of the American Dietetic Association},
keywords = {Artificial Intelligence,Cell Phones,Data Collection,Data Collection: instrumentation,Data Collection: methods,Diet Surveys,Energy Intake,Humans,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Image Processing, Computer-Assisted: standards,Pattern Recognition, Automated,Reproducibility of Results,Sensitivity and Specificity,United States},
month = {jan},
number = {1},
pages = {42--4},
pmid = {20102824},
title = {{Automatic food documentation and volume computation using digital imaging and electronic transmission.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2813222{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {110},
year = {2010}
}
@article{Hinton1983,
abstract = {When a vision system creates an interpretation of some input data, it assigns truth values or probabilities to internal hypotheses about teh world. We present a non-deterministic method for assigning truth values that avoids many of the problems encountered by existing relaxation methods. Instead of representing probabilities with real-numbers, we use a more direct encoding in which the probability associated with a hypothesis is represented by teh probability that it is in one of two states, true or false. We give a particular non-deterministic operator, based on statistical mechanics, for updating the truth values of hypotheses. The operator ensures that the probability of discovering a particular combination of hypotheses is a simple function of how good that combination is. We shor that there is a simple relationshiop between this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set of weights that optimizes its perceptual inferences.},
author = {Hinton, Geoffrey E and Sejnowski, Terrence J},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Sejnowski - 1983 - Optimal perceptual inference.pdf:pdf},
isbn = {0818600535},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {448--453},
title = {{Optimal perceptual inference}},
year = {1983}
}
@article{Muller2015,
abstract = {Software developers working on change tasks commonly experience a broad range of emotions, ranging from happiness all the way to frustration and anger. Research, primarily in psychology, has shown that for certain kinds of tasks, emotions correlate with progress and that biometric measures, such as electro-dermal activity and electroencephalography data, might be used to distinguish between emotions. In our research, we are building on this work and investigate developers' emotions, progress and the use of biometric measures to classify them in the context of software change tasks. We conducted a lab study with 17 participants working on two change tasks each. Participants were wearing three biometric sensors and had to periodically assess their emotions and progress. The results show that the wide range of emotions experienced by developers is correlated with their perceived progress on the change tasks. Our analysis also shows that we can build a classifier to distinguish between positive and negative emotions in 71.36{\%} and between low and high progress in 67.70{\%} of all cases. These results open up opportunities for improving a developer's productivity. For instance, one could use such a classifier for providing recommendations at opportune moments when a developer is stuck and making no progress.},
author = {M{\"{u}}ller, Sebastian C. and Fritz, Thomas},
doi = {10.1109/ICSE.2015.334},
isbn = {9781479919345},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {688--699},
title = {{Stuck and frustrated or in flow and happy: Sensing developers' emotions and progress}},
volume = {1},
year = {2015}
}
@article{FreundYoav1999,
abstract = {1 INTRODUCTION The AdaBoost algorithm 7, 16 has recently proved to be an important component in practical learning algorithms. Two of the most successful combinations have been boosting decision trees and boosting stumps 6, 1, 13, 8. Stumps are the simplest special case of decision trees which consist of a single decision node and two prediction leaves. Boosting decision trees learning algorithms, such as CART 2 and C4.5 14, yields very good classifiers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Freund, Yoav}, and Llew Mason},
doi = {10.1093/jxb/ern164},
eprint = {arXiv:1011.1669v3},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Freund, Yoav - 1999 - The alternating decision tree learning algorithm.pdf:pdf},
isbn = {1558606122},
issn = {14602431},
journal = {International Conference on Machine Learning},
pages = {124--133},
pmid = {18603617},
title = {{The alternating decision tree learning algorithm}},
volume = {99},
year = {1999}
}
@article{Holden2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural$\backslash$rnetwork with a small central layer to reconstruct high-dimensional input vectors. Gradient descent$\backslash$r$\backslash$ncan be used for fine-tuning the weights in such ‘‘autoencoder'' networks, but this works well only if$\backslash$r$\backslash$nthe initial weights are close to a good solution. We describe an effective way of initializing the$\backslash$r$\backslash$nweights that allows deep autoencoder networks to learn low-dimensional codes that work much$\backslash$r$\backslash$nbetter than principal components analysis as a tool to reduce the dimensionality of data.$\backslash$r$\backslash$n},
archivePrefix = {arXiv},
arxivId = {20},
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
doi = {10.1126/science.1127647},
eprint = {20},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Networksr.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {1095-9203},
journal = {Science},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks$\backslash$r}},
volume = {313},
year = {2006}
}
@article{Wen2009,
abstract = {Accurate and passive acquisition of dietary data from patients is essential for a better understanding of the etiology of obesity and development of effective weight management programs. Self-reporting is currently the main method for such data acquisition. However, studies have shown that data obtained by self-reporting seriously underestimate food intake and thus do not accurately reflect the real habitual behavior of individuals. Computer food recognition programs have not yet been developed. In this paper, we present a study for recognizing foods from videos of eating, which are directly recorded in restaurants by a web camera. From recognition results, our method then estimates food calories of intake. We have evaluated our method on a database of 101 foods from 9 food restaurants in USA and obtained promising results.},
author = {Wen, Wu and Jie, Yang},
doi = {10.1109/ICME.2009.5202718},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Wen, Jie - 2009 - Fast food recognition from videos of eating for calorie estimation.pdf:pdf},
isbn = {9781424442911},
issn = {1945-7871},
journal = {Proceedings - 2009 IEEE International Conference on Multimedia and Expo, ICME 2009},
keywords = {Calorie estimation,Fast food recognition},
pages = {1210--1213},
title = {{Fast food recognition from videos of eating for calorie estimation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.186.9475{\&}rep=rep1{\&}type=pdf},
year = {2009}
}
@techreport{Li2017,
abstract = {In this paper, drawing intuition from the Turing test, we propose using ad-versarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a genera-tive model to produce response sequences, and a discriminator-analagous to the human evaluator in the Turing test-to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.},
archivePrefix = {arXiv},
arxivId = {1701.06547v5},
author = {Li, Jiwei and Monroe, Will and Shi, Tianlin and Jean, S{\'{e}}bastien and Ritter, Alan and Jurafsky, Dan},
eprint = {1701.06547v5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2017 - Adversarial Learning for Neural Dialogue Generation.pdf:pdf},
title = {{Adversarial Learning for Neural Dialogue Generation}},
url = {https://arxiv.org/pdf/1701.06547.pdf},
year = {2017}
}
@inproceedings{Herranz2015,
abstract = {A large amount of food photos are taken in restaurants for diverse reasons. This dish recognition problem is very challenging, due to different cuisines, cooking styles and the intrinsic difficulty of modeling food from its visual appearance. Contextual knowledge is crucial to improve recognition in such scenario. In particular, geocontext has been widely exploited for outdoor landmark recognition. Similarly, we exploit knowledge about menus and geolocation of restaurants and test images. We first adapt a framework based on discarding unlikely categories located far from the test image. Then we reformulate the problem using a probabilistic model connecting dishes, restaurants and geolocations. We apply that model in three different tasks: dish recognition, restaurant recognition and geolocation refinement. Experiments on a dataset including 187 restaurants and 701 dishes show that combining multiple evidences (visual, geolocation, and external knowledge) can boost the performance in all tasks.},
author = {Herranz, Luis and {Key Lab. of Intell. Inf. Process Beijing China} and Ruihan, Xu and Jiang, Shuqiang},
booktitle = {2015 IEEE International Conference on Multimedia and Expo (ICME)},
doi = {10.1109/ICME.2015.7177464},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Herranz et al. - 2015 - A probabilistic model for food image recognition in restaurants.pdf:pdf},
isbn = {978-1-4799-7082-7},
keywords = {Visualization,catering industry,contextual knowledge,dish recognition,food image recognition,food photos,food recognition,geocontext,geolocation,geolocation refinement,mobile,mobile computing,object recognition,outdoor landmark recognition,probabilistic model,restaurant geolocation,restaurant menu,restaurant recognition,restaurants,statistical analysis,visual appearance},
month = {jun},
pages = {1--6},
publisher = {IEEE},
shorttitle = {Multimedia and Expo (ICME), 2015 IEEE Internationa},
title = {{A probabilistic model for food image recognition in restaurants}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7177464},
year = {2015}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {1207.0580},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptation of feature detectors.pdf:pdf},
keywords = {dropout},
mendeley-tags = {dropout},
month = {jul},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Hollis2008,
abstract = {BACKGROUND: To improve methods for long-term weight management, the Weight Loss Maintenance (WLM) trial, a four-center randomized trial, was conducted to compare alternative strategies for maintaining weight loss over a 30-month period. This paper describes methods and results for the initial 6-month weight-loss program (Phase I).

METHODS: Eligible adults were aged {\textgreater} or =25, overweight or obese (BMI=25-45 kg/m2), and on medications for hypertension and/or dyslipidemia. Anthropomorphic, demographic, and psychosocial measures were collected at baseline and 6 months. Participants (n=1685) attended 20 weekly group sessions to encourage calorie restriction, moderate-intensity physical activity, and the DASH (dietary approaches to stop hypertension) dietary pattern. Weight-loss predictors with missing data were replaced by multiple imputation.

RESULTS: Participants were 44{\%} African American and 67{\%} women; 79{\%} were obese (BMI{\textgreater} or =30), 87{\%} were taking anti-hypertensive medications, and 38{\%} were taking antidyslipidemia medications. Participants attended an average of 72{\%} of 20 group sessions. They self-reported 117 minutes of moderate-intensity physical activity per week, kept 3.7 daily food records per week, and consumed 2.9 servings of fruits and vegetables per day. The Phase-I follow-up rate was 92{\%}. Mean (SD) weight change was -5.8 kg (4.4), and 69{\%} lost at least 4 kg. All race-gender subgroups lost substantial weight: African-American men (-5.4 kg +/- 7.7); African-American women (-4.1 kg +/- 2.9); non-African-American men (-8.5 kg +/- 12.9); and non-African-American women (-5.8 kg +/- 6.1). Behavioral measures (e.g., diet records and physical activity) accounted for most of the weight-loss variation, although the association between behavioral measures and weight loss differed by race and gender groups.

CONCLUSIONS: The WLM behavioral intervention successfully achieved clinically significant short-term weight loss in a diverse population of high-risk patients.},
author = {Hollis, Jack F and Gullion, Christina M and Stevens, Victor J and Brantley, Phillip J and Appel, Lawrence J and Ard, Jamy D and Champagne, Catherine M and Dalcin, Arlene and Erlinger, Thomas P and Funk, Kristine and Laferriere, Daniel and Lin, Pao-Hwa and Loria, Catherine M and Samuel-Hodge, Carmen and Vollmer, William M and Svetkey, Laura P},
doi = {10.1016/j.amepre.2008.04.013},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Hollis et al. - 2008 - Weight loss during the intensive intervention phase of the weight-loss maintenance trial.pdf:pdf},
issn = {0749-3797},
journal = {American journal of preventive medicine},
keywords = {Adult,Combined Modality Therapy,Diet,Diet Records,Exercise,Female,Humans,Male,Middle Aged,Obesity,Obesity: diet therapy,Obesity: therapy,Overweight,Overweight: diet therapy,Overweight: therapy,Patient Compliance,Patient Compliance: statistics {\&} numerical data,Weight Loss},
month = {aug},
number = {2},
pages = {118--26},
pmid = {18617080},
title = {{Weight loss during the intensive intervention phase of the weight-loss maintenance trial.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2515566{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {35},
year = {2008}
}
@inproceedings{Kagaya2015,
author = {Kagaya, Hokuto and Aizawa, Kiyoharu and Ogawa, Makoto},
booktitle = {ACM Multimedia},
doi = {10.13140/2.1.3082.1120},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kagaya, Aizawa, Ogawa - 2015 - Food Detection and Recognition Using Convolutional Neural Network Food Detection and Recognition Using Co.pdf:pdf},
isbn = {9781450330633},
keywords = {convolu-,deep learning,food detection,food recognition,tional neural network},
number = {August},
title = {{Food Detection and Recognition Using Convolutional Neural Network Food Detection and Recognition Using Convolutional Neural Network}},
year = {2015}
}
@misc{openCVWebsite,
booktitle = {2016},
title = {{ABOUT | OpenCV http://opencv.org/about.html}},
url = {http://opencv.org/about.html},
urldate = {2016-03-22}
}
@article{Lazebnik2003,
abstract = { This paper introduces a texture representation suitable for recognizing images of textured surfaces under a wide range of transformations, including viewpoint changes and nonrigid deformations. At the feature extraction stage, a sparse set of affine-invariant local patches is extracted from the image. This spatial selection process permits the computation of characteristic scale and neighborhood shape for every texture element. The proposed texture representation is evaluated in retrieval and classification tasks using the entire Brodatz database and a collection of photographs of textured surfaces taken from different viewpoints.},
author = {Lazebnik, S. and Schmid, C. and Ponce, J.},
doi = {10.1109/CVPR.2003.1211486},
isbn = {0-7695-1900-8},
issn = {1063-6919},
journal = {2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.},
pmid = {16119265},
title = {{A sparse texture representation using affine-invariant regions}},
volume = {2},
year = {2003}
}
@article{Jacobs1995,
abstract = {We present a method for searching in an image database using a query image that is similar to the intended target. The query image may be a hand-drawn sketch or a (potentially low-quality) scan of the image to be retrieved. Our searching algorithm makes use of multiresolution wavelet decompositions of the query and database images. The coefficients of these decompositions are distilled into small 'signatures' for each image. We introduce an 'image querying metric' that operates on these signatures. This metric essentially compares how many significant wavelet coefficients the query has in common with potential targets. The metric includes parameters that can be tuned, using a statistical analysis, to accommodate the kinds of image distortions found in different types of image queries. The resulting algorithm is simple, requires very little storage overhead for the database of signatures, and is fast enough to be performed on a database of 20,000 images at interactive rates (on standard desktop machines) as a query is sketched. Our experiments with hundreds of queries in databases of 1000 and 20,000 images show dramatic improvement, in both speed and success rate, over using a conventional L1, L2, or color histogram norm.},
annote = {Sehr alt (95) - kleiner Testdatensatz},
author = {Jacobs, Ce and Finkelstein, A and Salesin, Dh},
doi = {10.1145/218380.218454},
isbn = {0897917014},
issn = {0897917014},
journal = {Proceedings of the 22nd annual conference on Computer graphics and interactive techniques},
number = {January},
pages = {277--286},
title = {{Fast multiresolution image querying}},
url = {http://dl.acm.org/citation.cfm?id=218454},
volume = {95},
year = {1995}
}
@article{Wang2009a,
abstract = {Measuring image similarity is a central topic in com- puter vision. In this paper, we learn similarity from Flickr groups and use it to organize photos. Two images are sim- ilar if they are likely to belong to the same Flickr groups. Our approach is enabled by a fast Stochastic Intersection Kernel MAchine (SIKMA) training algorithm, which we propose. This proposed training method will be useful for many vision problems, as it can produce a classifier that is more accurate than a linear classifier, trained on tens of thousands of examples in two minutes. The experimental results show our approach performs better on image match- ing, retrieval, and classification than using conventional vi- sual features.},
author = {Wang, Gang and Hoiem, Derek and Forsyth, David},
isbn = {9781424444199},
journal = {IEEE International Conference on Computer Vision},
keywords = {hand-craft features,image similarity,supervised learning},
mendeley-tags = {hand-craft features,image similarity,supervised learning},
number = {Iccv},
title = {{Learning Image Similarity from {\{}F{\}}lickr Groups Using Stochastic Intersection Kernel Machines}},
year = {2009}
}
@inproceedings{Bhagyalaksluni2014,
author = {Bhagyalaksluni, Ivis A},
booktitle = {2014 IEEE International Conference on Computer Communication and Systems(ICCCS '14)},
isbn = {9781479936717},
keywords = {content based image retrieval,local binary patterns,local derivative,local ternary patterns,local tetra patterns,patterns},
pages = {18--23},
title = {{A Survey on Content Based Image Retrieval Using Various Operators}},
year = {2014}
}
@article{Boureau,
abstract = {Many successful models for scene or object recognition transform low-level descriptors (such as Gabor filter re-sponses, or SIFT descriptors) into richer representations of intermediate complexity. This process can often be bro-ken down into two steps: (1) a coding step, which per-forms a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pool-ing step, which summarizes the coded features over larger neighborhoods. Several combinations of coding and pool-ing schemes have been proposed in the literature. The goal of this paper is threefold. We seek to establish the rela-tive importance of each step of mid-level feature extrac-tion through a comprehensive cross evaluation of several types of coding modules (hard and soft vector quantization, sparse coding) and pooling schemes (by taking the aver-age, or the maximum), which obtains state-of-the-art per-formance or better on several recognition benchmarks. We show how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding. We provide theoretical and empirical insight into the remarkable performance of max pooling. By teasing apart components shared by modern mid-level feature ex-tractors, our approach aims to facilitate the design of better recognition architectures.},
author = {Boureau, Y-Lan and Bach, Francis and Lecun, Yann and Ponce, Jean},
file = {::},
keywords = {()},
title = {{Learning Mid-Level Features For Recognition}}
}
@article{Wang2014,
abstract = {Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images.It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.4661v1},
author = {Wang, Jiang and Song, Yang and Leung, Thomas and Rosenberg, Chuck and Wang, Jingbin and Philbin, James and Chen, Bo and Wu, Ying},
doi = {10.1109/CVPR.2014.180},
eprint = {arXiv:1404.4661v1},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1386--1393},
title = {{Learning fine-grained image similarity with deep ranking}},
year = {2014}
}
@article{Schneider2014,
abstract = {In a sparse-representation-based face recognition scheme, the desired dictionary should have good representational power (i.e., being able to span the subspace of all faces) while supporting optimal discrimination of the classes (i.e., different human subjects). We propose a method to learn an over-complete dictionary that attempts to simultaneously achieve the above two goals. The proposed method, discriminative K-SVD (D-KSVD), is based on extending the K-SVD algorithm by incorporating the classification error into the objective function, thus allowing the performance of a linear classifier and the representational power of the dictionary being considered at the same time by the same optimization procedure. The D-KSVD algorithm finds the dictionary and solves for the classifier using a procedure derived from the K-SVD algorithm, which has proven efficiency and performance. This is in contrast to most existing work that relies on iteratively solving sub-problems with the hope of achieving the global optimal through iterative approximation. We evaluate the proposed method using two commonly-used face databases, the Extended YaleB database and the AR database, with detailed comparison to 3 alternative approaches, including the leading state-of-the-art in the literature. The experiments show that the proposed method outperforms these competing methods in most of the cases. Further, using Fisher criterion and dictionary incoherence, we also show that the learned dictionary and the corresponding classifier are indeed better-posed to support sparse-representation-based recognition.},
author = {Schneider, Ros{\'{a}}lia G. and Tuytelaars, Tinne},
doi = {10.1145/2661229.2661231},
isbn = {9781424469840},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {classification,fisher vectors,sketches,sketches, classification, Fisher vectors},
number = {6},
pages = {2691--2698},
title = {{Sketch classification and classification-driven analysis using Fisher vectors}},
volume = {33},
year = {2014}
}
@article{Conneau2017c,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
archivePrefix = {arXiv},
arxivId = {1705.02364},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
doi = {10.1.1.156.2685},
eprint = {1705.02364},
file = {::},
isbn = {978-1-109-24088-7},
issn = {0176-1617},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
year = {2017}
}
@article{Zhao2011,
abstract = {Twitter as a new form of social media can potentially contain much useful information, but content analysis on Twitter has not been well studied. In particular, it is not clear whether as an information source Twitter can be simply regarded as a faster news feed that covers mostly the same information as traditional news media. In This paper we empirically compare the content of Twitter with a traditional news medium, New York Times, using unsupervised topic modeling. We use a Twitter-LDA model to discover topics from a representative sample of the entire Twitter. We then use text mining techniques to compare these Twitter topics with topics from New York Times, taking into consideration topic categories and types. We also study the relation between the proportions of opinionated tweets and retweets and topic categories and types. Our comparisons show interesting and useful findings for downstream IR or DM applications.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Zhao, Wayne Xin and Jiang, Jing and Weng, Jianshu and He, Jing and Lim, Ee-Peng and Yan, Hongfei and Li, Xiaoming},
doi = {10.1007/978-3-642-20161-5_34},
eprint = {9780201398298},
isbn = {978-3-642-20160-8},
issn = {0302-9743},
journal = {33rd European Conference on IR Research, ECIR 2011},
keywords = {microblogging,topic modeling,twitter},
pages = {338--349},
pmid = {4520227},
title = {{Comparing twitter and traditional media using topic models}},
url = {http://dl.acm.org/citation.cfm?id=1996889.1996934},
year = {2011}
}
@incollection{Kuang2015,
abstract = {Nonnegative matrix factorization (NMF) approximates a nonnegative ma-trix by the product of two low-rank nonnegative matrices. Since it gives semanti-cally meaningful result that is easily interpretable in clustering applications, NMF has been widely used as a clustering method especially for document data, and as a topic modeling method. We describe several fundamental facts of NMF and introduce its optimization framework called block coordinate descent. In the context of clustering, our frame-work provides a flexible way to extend NMF such as the sparse NMF and the weakly-supervised NMF. The former provides succinct representations for better interpretations while the latter flexibly incorporate extra information and user feed-back in NMF, which effectively works as the basis for the visual analytic topic mod-eling system that we present. Using real-world text data sets, we present quantitative experimental results showing the superiority of our framework from the following aspects: fast con-vergence, high clustering accuracy, sparse representation, consistent output, and user interactivity. In addition, we present a visual analytic system called UTOPIAN (User-driven Topic modeling based on Interactive NMF) and show several usage scenarios. Overall, our book chapter cover the broad spectrum of NMF in the context of clustering and topic modeling, from fundamental algorithmic behaviors to practical visual analytics systems.},
author = {Kuang, Da and Choo, Jaegul and Park, Haesun},
booktitle = {Partitional Clustering Algorithms},
doi = {10.1007/978-3-319-09259-1_7},
file = {::},
isbn = {9783319092591},
pages = {215--243},
title = {{Nonnegative Matrix Factorization for Interactive Topic Modeling and Document Clustering}},
url = {http://link.springer.com/10.1007/978-3-319-09259-1{\_}7},
year = {2015}
}
@techreport{Liu,
abstract = {We show that generating English Wikipedia articles can be approached as a multi-document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
archivePrefix = {arXiv},
arxivId = {1801.10198v1},
author = {Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, {\L}ukasz and Shazeer, Noam},
eprint = {1801.10198v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - Unknown - GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES.pdf:pdf},
isbn = {1801.10198v1},
title = {{GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES}},
url = {https://en.wikipedia.org/wiki/Wikipedia:Manual{\_}of{\_}Style}
}
@misc{Chen2014a,
abstract = {Some introduction to tree, tree ensemble and boosted tree},
author = {Chen, Tianqi},
booktitle = {Data Mining with Decision Trees},
doi = {10.1142/9789812771728_0012},
isbn = {1879-2057 (Electronic)$\backslash$n0001-4575 (Linking)},
issn = {0957-4174},
pages = {187--213},
pmid = {23021419},
title = {{Introduction to Boosted Trees}},
url = {http://www.worldscientific.com/doi/abs/10.1142/9789812771728{\_}0012},
year = {2014}
}
@misc{B2018,
author = {B{\"{O}}LW},
booktitle = {Zahlen, Daten, Fakten: Die Bio-Branche 2018},
file = {::},
title = {{Umsatz mit Bio-Lebensmitteln in ausgew{\"{a}}hlten L{\"{a}}ndern weltweit im Jahr 2016 (in Millionen Euro)}},
url = {http://de.statista.com/statistik/daten/studie/325242/umfrage/umsatz-mit-fairtrade-produkten-weltweit-nach-laendern/},
urldate = {2018-03-16},
year = {2018}
}
@article{Nikolenko2017,
abstract = {Qualitative studies, such as sociological research, opinion analysis and media studies, can benefit greatly from automated topic mining provided by topic models such as latent Dirichlet allocation (LDA). However, examples of qualitative studies that employ topic modelling as a tool are currently few and far between. In this work, we identify two important problems along the way to using topic models in qualitative studies: lack of a good quality metric that closely matches human judgement in understanding topics and the need to indicate specific subtopics that a specific qualitative study may be most interested in mining. For the first problem, we propose a new quality metric, tf-idf coherence, that reflects human judgement more accurately than regular coherence, and conduct an experiment to verify this claim. For the second problem, we propose an interval semi-supervised approach (ISLDA) where certain predefined sets of keywords (that define the topics researchers are interested in) are restricted to specific intervals of topic assignments. Our experiments show that ISLDA is better for topic extraction than LDA in terms of tf-idf coherence, number of topics identified to predefined keywords and topic stability. We also present a case study on a Russian LiveJournal dataset aimed at ethnicity discourse analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Nikolenko, Sergey I. and Koltcov, Sergei and Koltsova, Olessia},
doi = {10.1177/0165551515617393},
eprint = {arXiv:1011.1669v3},
isbn = {0165-5515},
issn = {17416485},
journal = {Journal of Information Science},
keywords = {LDA extensions,Latent Dirichlet allocation,topic modelling,topic quality},
pmid = {24131618},
title = {{Topic modelling for qualitative studies}},
year = {2017}
}
@article{Mohr2013,
abstract = {Poetics, 41 (2013) 545-569. doi:10.1016/j.poetic.2013.10.001},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Mohr, John W. and Bogdanov, Petko},
doi = {10.1016/j.poetic.2013.10.001},
eprint = {9605103},
isbn = {0001250000},
issn = {0304422X},
journal = {Poetics},
number = {6},
pages = {545--569},
pmid = {1000111929},
primaryClass = {cs},
title = {{Introduction-Topic models: What they are and why they matter}},
volume = {41},
year = {2013}
}
@article{MohitIyyerVarunManjunathaJordanBoyd-Graber2015,
author = {{Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber}, Hal Daum{\'{e}} III1},
file = {::},
title = {{Deep Unordered Composition Rivals Syntactic Methods for Text Classification Mohit}},
year = {2015}
}
@book{thanaki2017python,
author = {Thanaki, J},
isbn = {9781787285521},
publisher = {Packt Publishing},
title = {{Python Natural Language Processing}},
url = {https://books.google.de/books?id=IedDDwAAQBAJ},
year = {2017}
}
@article{Vavasis2009a,
abstract = {Nonnegative matrix factorization (NMF) has become a prominent technique for the analysis of image databases, text databases, and other information retrieval and clustering applica-tions. The problem is most naturally posed as continuous optimization. In this report, we define an exact version of NMF. Then we establish several results about exact NMF: (i) that it is equivalent to a problem in polyhedral combinatorics; (ii) that it is NP-hard; and (iii) that a polynomial-time local search heuristic exists.},
archivePrefix = {arXiv},
arxivId = {arXiv:0708.4149v2},
author = {Vavasis, Stephen A},
doi = {10.1137/070709967},
eprint = {arXiv:0708.4149v2},
issn = {10526234},
journal = {SIAM Journal On Optimization,},
keywords = {15A48,68T05,90C26,90C60,NP-hard,complexity,data mining,nonnegative matrix factorization,nonnegative rank},
number = {3},
pages = {1364--1377},
title = {{On the complexity of nonnegative matrix factorization}},
url = {http://epubs.siam.org/doi/pdf/10.1137/070709967},
volume = {20},
year = {2009}
}
@article{Zhao2008,
abstract = {We present a novel paradigm for statistical machine translation (SMT), based on a joint modeling of word alignment and the topical aspects underlying bilingual document-pairs, via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM). In this paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-flow, to ensure coherence of topical context in the alignment of mapping words between languages, likelihood-based training of topic-dependent translational lexicons, as well as in the inference of topic representations in each language. The learned HM-BiTAM can not only display topic patterns like methods such as LDA , but now for bilingual corpora; it also offers a principled way of inferring optimal translation using document context. Our method integrates the conventional model of HMM - a key component for most of the state-of-the-art SMT systems, with the recently proposed BiTAM model; we report an extensive empirical analysis (in many ways complemen- tary to the description-oriented) of our method in three aspects: bilingual topic representation, word alignment, and translation.},
author = {Zhao, Bing and Xing, Eric P.},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {1689--1696},
title = {{HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation}},
year = {2008}
}
@article{Kou2015,
abstract = {Before deciding to buy a product, many people tend to consult others' opinions on it. Web provides a perfect platform which$\backslash$n one can get information to find out the advantages and disadvantages of the product of his interest. How to automatically$\backslash$n manage the numerous opinionated documents and then to give suggestions to the potential customers is becoming a research hotspot$\backslash$n recently. Constructing a sentiment resource is one of the vital elements of opinion finding and polarity analysis tasks. For$\backslash$n a specific domain, the sentiment resource can be regarded as a dictionary, which contains a list of product feature words$\backslash$n and several opinion words with sentiment polarity for each feature word. This paper proposes an automatic algorithm to extraction$\backslash$n feature words and opinion words for the sentiment resource. We mine the feature words and opinion words from the comments$\backslash$n on the Web with both NLP technique and statistical method. Left context entropy is proposed to extract unknown feature words;$\backslash$n Adjective rules and background corpus are taken into consideration in the algorithm. Experimental results show the effectiveness$\backslash$n of the proposed automatic sentiment resource construction approach. The proposed method that combines NLP and statistical$\backslash$n techniques is better than using only NLP-based technique. Although the experiment is built on mobile telephone comments in$\backslash$n Chinese, the algorithm is domain independent.},
author = {Kou, Wanqiu and Li, Fang and Baldwin, Timothy},
doi = {10.1007/978-3-319-28940-3_20},
file = {::},
isbn = {9783319289397},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Letter trigram vectors,Topic labelling,Word vectors},
number = {1},
pages = {253--264},
title = {{Automatic labelling of topic models using word vectors and letter trigram vectors}},
volume = {9460},
year = {2015}
}
@inproceedings{Bhatia2017a,
abstract = {Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.},
archivePrefix = {arXiv},
arxivId = {1706.05140},
author = {Bhatia, Shraey and Lau, Jey Han and Baldwin, Timothy},
booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning, CoNLL},
eprint = {1706.05140},
pages = {206--215},
title = {{An Automatic Approach for Document-level Topic Model Evaluation}},
url = {http://arxiv.org/abs/1706.05140},
year = {2017}
}
@article{Magatti2009,
abstract = {An algorithm for the automatic labeling of topics accordingly to a hierarchy is presented. Its main ingredients are a set of similarity measures and a set of topic labeling rules. The labeling rules are specifically designed to find the most agreed labels between the given topic and the hierarchy. The hierarchy is obtained from the Google Directory service, extracted via an ad-hoc developed software procedure and expanded through the use of the OpenOffice English Thesaurus. The performance of the proposed algorithm is investigated by using a document corpus consisting of 33,801 documents and a dictionary consisting of 111,795 words. The results are encouraging, while particularly interesting and significant labeling cases emerged.},
author = {Magatti, Davide and Calegari, Silvia and Ciucci, Davide and Stella, Fabio},
doi = {10.1109/ISDA.2009.165},
isbn = {9780769538723},
journal = {ISDA 2009 - 9th International Conference on Intelligent Systems Design and Applications},
pages = {1227--1232},
title = {{Automatic labeling of topics}},
year = {2009}
}
@article{Minka2002,
abstract = {The generative aspect model is an extension of the multinomial model$\backslash$nfor text that allows word probabilities to vary stochastically across$\backslash$ndocuments. Previous results with aspect models have been promising,$\backslash$nbut hindered by the computational difficulty of carrying out inference$\backslash$nand learning. This paper demonstrates that the simple variational$\backslash$nmethods of Blei et al (2001) can lead to inaccurate inferences and$\backslash$nbiased learning for the generative aspect model. We develop an alternative$\backslash$napproach that leads to higher accuracy at comparable cost. An extension$\backslash$nof Expectation-Propagation is used for inference and then embedded$\backslash$nin an EM algorithm for learning. Experimental results are presented$\backslash$nfor both synthetic and real data sets.},
author = {Minka, Thomas and Lafferty, John},
doi = {ISBN 1-55860-897-4},
isbn = {1-55860-897-4},
journal = {Uncertainty in Artificial Intelligence},
pages = {352--359},
title = {{Expectation-Propagation for the Generative Aspect Model}},
year = {2002}
}
@article{Hill2016,
abstract = {Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.},
archivePrefix = {arXiv},
arxivId = {1602.03483},
author = {Hill, Felix and Cho, Kyunghyun and Korhonen, Anna},
doi = {10.18653/v1/N16-1162},
eprint = {1602.03483},
file = {::},
isbn = {9781941643914},
title = {{Learning Distributed Representations of Sentences from Unlabelled Data}},
url = {http://arxiv.org/abs/1602.03483},
year = {2016}
}
@misc{Walker2018,
abstract = {This piece on descriptive analytics is the second in a series of guest posts written by Dan Vesset, Group Vice President of the Analytics and Information Management market research and advisory practice at IDC},
author = {Walker, Tim},
title = {{Descriptive analytics 101: What happened?}},
url = {https://www.ibm.com/blogs/business-analytics/descriptive-analytics-101-what-happened/},
year = {2018}
}
@article{Guu2017,
abstract = {We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.},
archivePrefix = {arXiv},
arxivId = {1709.08878},
author = {Guu, Kelvin and Hashimoto, Tatsunori B. and Oren, Yonatan and Liang, Percy},
eprint = {1709.08878},
file = {::},
issn = {1938-7228},
title = {{Generating Sentences by Editing Prototypes}},
url = {http://arxiv.org/abs/1709.08878},
volume = {2},
year = {2017}
}
@article{Bergstra2013,
abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters of-ten must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on per-sonal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the al-gorithm and the details of its tuning, it is sometimes di to know whether a given technique is genuinely better, or simply bet-ter tuned. In this work, we propose a meta-modeling ap-proach to support automated hyperparam-eter optimization, with the goal of provid-ing practical tools that replace hand-tuning with a reproducible and unbiased optimiza-Proceedings of the 30 th International Conference on Ma-chine Learning, Atlanta, Georgia, USA, 2013. JMLR: W{\&}CP volume 28. Copyright 2013 by the author(s). tion process. Our approach is to expose the underlying expression graph of how a perfor-mance metric (e.g. classification accuracy on validation examples) is computed from hy-perparameters that govern not only how indi-vidual processing steps are applied, but even which processing steps are included. A hy-perparameter optimization algorithm trans-forms this graph into a program for opti-mizing that performance metric. Our ap-proach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architec-tures.},
author = {Bergstra, James and Yamins, Daniel L K and Cox, D},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bergstra, Yamins, Cox - 2013 - Making a science of model search Hyperparameter optimization in hundreds of dimensions for vision archite.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {115--123},
title = {{Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures}},
url = {http://proceedings.mlr.press/v28/bergstra13.pdf http://jmlr.org/proceedings/papers/v28/bergstra13.html},
volume = {28},
year = {2013}
}
@inproceedings{Donahue2013,
abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature , and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
archivePrefix = {arXiv},
arxivId = {1310.1531v1},
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
booktitle = {Proceedings of the 31st International Conference on Machine Learning},
eprint = {1310.1531v1},
file = {::},
pages = {647--655},
publisher = {PMLR},
title = {{DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition}},
url = {http://decaf.berkeleyvision.org/},
year = {2013}
}
@article{Decker2010,
abstract = {Today, consumer reviews are available on the Internet for a large number of product categories. The pros and cons expressed in this way uncover individually perceived strengths and weaknesses of the respective products, whereas the usually assigned product ratings represent their overall valuation. The key question at this point is how to turn the available plentitude of individual consumer opinions into aggregate consumer preferences, which can be used, for example, in product development or improvement processes. To solve this problem, an econometric framework is presented that can be applied to the mentioned type of data after having prepared it using natural language processing techniques. The suggested methodology enables the estimation of parameters, which allow inferences on the relative effect of product attributes and brand names on the overall evaluation of the products. Specifically, we discuss options for taking opinion heterogeneity into account. Both the practicability and the benefits of the suggested approach are demonstrated using product review data from the mobile phone market. This paper demonstrates that the review-based results compare very favorably with consumer preferences obtained through conjoint analysis techniques. {\textcopyright} 2010 Elsevier B.V.},
author = {Decker, Reinhold and Trusov, Michael},
doi = {10.1016/j.ijresmar.2010.09.001},
isbn = {0167-8116},
issn = {01678116},
journal = {International Journal of Research in Marketing},
keywords = {Consumer behavior,Electronic commerce and internet marketing,Market analysis and response,Preference analysis,Product management,User generated content},
number = {4},
pages = {293--307},
pmid = {55497791},
publisher = {Elsevier B.V.},
title = {{Estimating aggregate consumer preferences from online product reviews}},
url = {http://dx.doi.org/10.1016/j.ijresmar.2010.09.001},
volume = {27},
year = {2010}
}
@article{Stevens2012,
abstract = {We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation of documents and words in a corpus.},
author = {Stevens, Keith and Kegelmeyer, Philip and Andrzejewski, David and Buttler, David},
isbn = {9781937284435},
journal = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
number = {July},
pages = {952--961},
title = {{Exploring Topic Coherence over Many Models and Many Topics}},
url = {http://aclanthology.info/papers/exploring-topic-coherence-over-many-models-and-many-topics},
year = {2012}
}
@article{Mitchell2010,
abstract = {Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task.},
author = {Mitchell, Jeff and Lapata, Mirella},
doi = {10.1111/j.1551-6709.2010.01106.x},
file = {::},
isbn = {1551-6709 (Electronic)$\backslash$r0364-0213 (Linking)},
issn = {03640213},
journal = {Cognitive Science},
keywords = {compositionality,connectionism,distributional models,meaning representations,phrase similarity,semantic spaces},
number = {8},
pages = {1388--1429},
pmid = {21564253},
title = {{Composition in Distributional Models of Semantics}},
url = {http://doi.wiley.com/10.1111/j.1551-6709.2010.01106.x},
volume = {34},
year = {2010}
}
@article{Das2014c,
abstract = {We address the problem of discovering topical phrases or "aspects" from microblogging sites like Twitter, that correspond to key talking points or buzz around a particular topic or entity of interest. Inferring such topical aspects enables various applications such as trend detection and opinion mining for business analytics. However, mining high-volume microblog streams for aspects poses unique challenges due to the inherent noise, redundancy and ambiguity in users' social posts. We address these challenges by using a probabilistic model that incorporates various global and local indicators such as "uniqueness", "diversity" and "burstiness" of phrases, to infer relevant aspects. Our model is learned using an EM algorithm that uses automatically generated noisy labels, without requiring manual effort or domain knowledge. We present results on three months of Twitter data across different types of entities to validate our approach.},
author = {Das, A and Kannan, A},
isbn = {9781941643266},
journal = {COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers},
keywords = {Algorithms; Computational linguistics; Linguistics,Automatically generated; Business analytics; Doma,Data mining},
pages = {860--871},
title = {{Discovering topical aspects in microblogs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959905236{\&}partnerID=40{\&}md5=b240ed74c9163bdf9f6cec0a8ecb47a8},
year = {2014}
}
@inproceedings{Technologies2017,
author = {Wojatzki, Michael and Eugen, Ruppert and Zesch, Torsten and Biemann, Chris},
booktitle = {Proceedings of the GSCL GermEval Shared Task on Aspect-based Sentiment in Social Media Customer Feedback},
file = {::},
month = {sep},
title = {{Language Technologies for the Challenges of the Digital Age}},
year = {2017}
}
@inproceedings{Lee2017,
abstract = {This paper describes our submissions to the GermEval 2017 Shared Task, which focused on the analysis of customer feedback about the Deutsche Bahn AG. We used sentence embeddings and an ensemble of classifiers for two sub-tasks as well as state-of-the-art sequence taggers for two other sub-tasks. Relevant aspects to reproduce our experiments are available from https://github.com/UKPLab/ germeval2017-sentiment-detection.},
address = {Berlin},
author = {Lee, Ji-Ung and Eger, Steffen and Daxenberger, Johannes and Gurevych, Iryna},
booktitle = {Proceedings of the GSCL GermEval Shared Task on Aspect-based Sentiment in Social Media Customer Feedback},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2017 - UKP TU-DA at GermEval 2017 Deep Learning for Aspect Based Sentiment Detection.pdf:pdf},
pages = {22----29},
title = {{UKP TU-DA at GermEval 2017: Deep Learning for Aspect Based Sentiment Detection}},
url = {http://www.ukp.tu-darmstadt.de},
year = {2017}
}
@inproceedings{Bergstra2013a,
abstract = {Sequential model-based optimization (also known as Bayesian optimization) is one of the most efficient methods (per function evaluation) of function minimization. This efficiency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. The paper closes with some discussion of ongoing and future work.},
address = {Austin, Texas},
author = {Bergstra, James and Yamins, Dan and Cox, David D},
booktitle = {PROC. OF THE 12th PYTHON IN SCIENCE CONF. (SCIPY},
file = {::},
number = {1},
title = {{Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms}},
url = {https://pdfs.semanticscholar.org/d4f4/9717c9adb46137f49606ebbdf17e3598b5a5.pdf},
volume = {12},
year = {2013}
}
@article{Lin2007,
abstract = {Nonnegative matrix factorization (NMF) can be formulated as a minimization problem with bound constraints. Although bound-constrained optimization has been studied extensively in both theory and practice, so far no study has formally applied its techniques to NMF. In this letter, we propose two projected gradient methods for NMF, both of which exhibit strong optimization properties. We discuss efficient implementations and demonstrate that one of the proposed methods converges faster than the popular multiplicative update approach. A simple Matlab code is also provided.},
author = {Lin, Chih-Jen},
doi = {10.1162/neco.2007.19.10.2756},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {10},
pages = {2756--2779},
pmid = {17716011},
title = {{Projected Gradient Methods for Nonnegative Matrix Factorization}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.2007.19.10.2756},
volume = {19},
year = {2007}
}
@book{zheng2018feature,
author = {Zheng, A and Casari, A},
isbn = {9781491953198},
publisher = {O'Reilly Media},
title = {{Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists}},
url = {https://books.google.de/books?id=sthSDwAAQBAJ},
year = {2018}
}
@book{Rajaraman:2011:MMD:2124405,
address = {New York, NY, USA},
author = {Rajaraman, Anand and Ullman, Jeffrey David},
isbn = {1107015359, 9781107015357},
publisher = {Cambridge University Press},
title = {{Mining of Massive Datasets}},
year = {2011}
}
@article{Gwet2014,
abstract = {The notion of intrarater reliability will be of interest to researchers concerned about the reproducibility of clinical measurements. A rater in this context refers to any data- generating system, which includes individ- uals and laboratories; intrarater reliability is a metric for rater's self-consistency in the scoring of subjects. The importance of data reproducibility stems from the need for scientific inquiries to be based on solid evi- dence. Reproducible clinical measurements are recognized as representing a well-defined characteristic of interest. Reproducibility is a source of concern caused by the extensive manipulation of medical equipment in test laboratories and the complexity of the judg- mental processes involved in clinical data gathering. Grundy},
author = {Gwet, Kilem L.},
doi = {10.1002/9781118596333.ch19},
file = {::},
isbn = {9781118596333},
issn = {047146242X},
journal = {Methods and Applications of Statistics in Clinical Trials},
keywords = {Confidence interval,Intrarater reliability,Randomized block design,Rater factor,Statistical inference},
number = {1},
pages = {340--356},
title = {{Intrarater Reliability}},
volume = {2},
year = {2014}
}
@inproceedings{Turian2010,
abstract = {If we take an existing supervised {\{}NLP{\}} system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and {\{}HLBL{\}} (Mnih {\&} Hinton, 2009) embeddings of words on both {\{}NER{\}} and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing {\{}NLP{\}} systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/},
author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Turian, Ratinov, Bengio - Unknown - Word representations A simple and general method for semi-supervised learning(2).pdf:pdf},
number = {July},
pages = {384--394},
title = {{Word representations : A simple and general method for semi-supervised learning}},
url = {http://metaoptimize.},
year = {2010}
}
@article{Webb2010,
abstract = {We investigate the use of unlabeled data to help labeled data in classification. We propose a simple iterative algorithm, label propagation, to propagate labels through the dataset along high density areas defined by unlabeled data. We analyze the algorithm, show its solution, and its connection to several other algorithms. We also show how to learn parameters by minimum spanning tree heuristic and entropy minimization, and the algorithms ability to perform feature selection. Experiment results are promising.},
author = {Webb, Geoffrey I. and Raedt, Luc De and Perlich, Claudia and Getoor, Lise and Wrobel, Stefan and Ting, Jo-Anne and Buntine, Wray L. and Horv{\'{a}}th, Tam{\'{a}}s and Namata, Galileo and Vijayakumar, Sethu and Lagoudakis, Michail G. and Han, Xin Jin, Jiawei and Korb, Kevin B. and Leslie, Christina and Sammut, Claude and Quadrianto, Novi and Schaal, Stefan and Noble, William Stafford},
doi = {10.1007/978-0-387-30164-8_459},
file = {::},
isbn = {0780372786},
issn = {19524013},
journal = {Encyclopedia of Machine Learning},
pages = {584--584},
pmid = {28508753},
title = {{Learning from Labeled and Unlabeled Data}},
year = {2010}
}
@article{Xu2016,
abstract = {Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task{\~{}}8, and achieve an F1-score of 86.1{\%}, outperforming previous state-of-the-art recorded results.},
archivePrefix = {arXiv},
arxivId = {1601.03651},
author = {Xu, Yan and Jia, Ran and Mou, Lili and Li, Ge and Chen, Yunchuan and Lu, Yangyang and Jin, Zhi},
doi = {10.1109/EPE.2005.219688},
eprint = {1601.03651},
file = {::},
isbn = {9781510827585},
title = {{Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation}},
url = {http://arxiv.org/abs/1601.03651},
year = {2016}
}
@book{goldberg2017neural,
author = {Goldberg, Y and Hirst, G},
isbn = {9781627052955},
publisher = {Morgan {\&} Claypool Publishers},
series = {Synthesis Lectures on Human Language Technologies},
title = {{Neural Network Methods in Natural Language Processing}},
url = {https://books.google.de/books?id=Za2zDgAAQBAJ},
year = {2017}
}
@article{Hallgren2012,
author = {Hallgren, Kevin A},
file = {::},
keywords = {behavioral observation,coding,inter-rater agreement,intra-class correlation,kappa,reliability},
number = {1},
pages = {23--34},
title = {{NIH Public Access}},
volume = {8},
year = {2012}
}
@article{Langville2006,
abstract = {The need to process and conceptualize large sparse matrices effectively and efficiently (typically via low-rank approxima-tions) is essential for many data mining applications, includ-ing document and image analysis, recommendation systems, and gene expression analysis. The nonnegative matrix fac-torization (NMF) has many advantages to alternative tech-niques for processing such matrices, but its use comes with a caveat: the NMF must be initialized and the initialization selected is crucial to getting good solutions. It is well-known that good initializations can improve the speed and accu-racy of the solutions of many NMF algorithms [43]. Add to this the fact that many NMF algorithms are sensitive with respect to the initialization of one or both NMF factors, and the impact of initializations becomes very important. In this paper, we compare the results of six initialization procedures (two standard and four new) on two alternating least squares algorithms, which we presented in [27].},
author = {an Langville and Meyer, Cd and Albright, Russell and Cox, J},
journal = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
keywords = {by nsf career-ccf-,clustering,convergence,initializations,nonnegative matrix factorization,research supported in part,text mining},
pages = {1--8},
title = {{Initializations for the nonnegative matrix factorization}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.4302{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@article{Hsu2010,
abstract = {The support vector machine (SVM) is a popular classification technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but significant steps. In this guide, we propose a simple procedure which usually gives reasonable results.},
author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen},
file = {::},
isbn = {0263-2764},
issn = {05440440},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin{\%}0AInitial},
year = {2010}
}
@article{10.1093/ptj/85.3.257,
abstract = {Purpose. This article examines and illustrates the use and interpretation of the kappa statistic in musculoskeletal research. Summary of Key Points. The reliability of clinicians' ratings is an important consideration in areas such as diagnosis and the interpretation of examination findings. Often, these ratings lie on a nominal or an ordinal scale. For such data, the kappa coefficient is an appropriate measure of reliability. Kappa is defined, in both weighted and unweighted forms, and its use is illustrated with examples from musculoskeletal research. Factors that can influence the magnitude of kappa (prevalence, bias, and nonindependent ratings) are discussed, and ways of evaluating the magnitude of an obtained kappa are considered. The issue of statistical testing of kappa is considered, including the use of confidence intervals, and appropriate sample sizes for reliability studies using kappa are tabulated. Conclusions. The article concludes with recommendations for the use and interpretation of kappa.},
author = {Sim, Julius and Wright, Chris C},
doi = {10.1093/ptj/85.3.257},
issn = {0031-9023},
journal = {Physical Therapy},
number = {3},
pages = {257--268},
title = {{The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements}},
url = {https://dx.doi.org/10.1093/ptj/85.3.257},
volume = {85},
year = {2005}
}
@inproceedings{Tang2016,
abstract = {Target-dependent sentiment classification remains a challenge: modeling the semantic related- ness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence represen- tation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1},
address = {Osaka, Japan},
author = {Tang, Duyu and Qin, Bing and Feng, Xiaocheng and Liu, Ting},
booktitle = {Proceedings of COLING 2016, the 26th international conference on computational linguis- tics: technical papers},
file = {:Users/felix/Downloads/C16-1311.pdf:pdf},
pages = {3298--3307},
title = {{Effective LSTMs for Target-Dependent Sentiment Classification}},
year = {2016}
}
@inproceedings{Roder2015,
abstract = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently stud-ied to remedy the problem that topic models give no guar-anty on the interpretablity of their output. Several bench-mark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining ele-mentary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. Finally, we outline how our results can be transferred to further appli-cations in the context of text mining, information retrieval and the world wide web.},
author = {R{\"{o}}der, Michael and Both, Andreas and Hinneburg, Alexander},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining - WSDM '15},
doi = {10.1145/2684822.2685324},
file = {::},
isbn = {9781450333177},
issn = {1937284115},
pages = {399--408},
title = {{Exploring the Space of Topic Coherence Measures}},
url = {http://dl.acm.org/citation.cfm?doid=2684822.2685324},
year = {2015}
}
@inproceedings{Bergstra2011,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
address = {Granada, Spain},
author = {Bergstra, James and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
booktitle = {NIPS'11 Proceedings of the 24th International Conference on Neural Information Processing Systems},
file = {::},
isbn = {9781618395993},
pages = {2546--2554},
publisher = {Neural Information Processing Systems},
title = {{Algorithms for hyper-parameter optimization}},
url = {https://dl.acm.org/citation.cfm?id=2986743},
year = {2011}
}
@article{Exposure2009,
author = {Exposure, Conclusions and Ed, Draft},
doi = {10.1080/03019233.2016.1218198},
isbn = {9781907026249},
issn = {1938-7228},
number = {September},
pmid = {1714571},
title = {{Financial Instruments : Classifi cation and Measurement}},
year = {2009}
}
@article{Reyes2012,
abstract = {The research described in this paper is focused on analyzing two playful domains of language: humor and irony, in order to identify key values components for their automatic processing. In particular, we are focused on describing a model for recognizing these phenomena in social media, such as “tweets". Our experiments are centered on five data sets retrieved from Twitter taking advantage of user-generated tags, such as “{\#}humor" and “{\#}irony". The model, which is based on textual features, is assessed on two dimensions: representativeness and relevance. The results, apart from providing some valuable insights into the creative and figurative usages of language, are positive regarding humor, and encouraging regarding irony.},
author = {Reyes, Antonio and Rosso, Paolo and Buscaldi, Davide},
doi = {https://doi.org/10.1016/j.datak.2012.02.005},
issn = {0169-023X},
journal = {Data {\&} Knowledge Engineering},
keywords = {Humor recognition,Irony detection,Natural language processing,Web text analysis},
pages = {1--12},
title = {{From humor recognition to irony detection: The figurative language of social media}},
url = {http://www.sciencedirect.com/science/article/pii/S0169023X12000237},
volume = {74},
year = {2012}
}
@unpublished{Jelodar2017b,
abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data, text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modeling, which Latent Dirichlet allocation (LDA) is one of the most popular methods in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper can be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated scholarly articles highly (between 2003 to 2016) related to Topic Modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. Also, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
archivePrefix = {arXiv},
arxivId = {1711.04305v1},
author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia},
booktitle = {arXiv:1711.04305v1 [cs.IR]},
doi = {10.16288/j.yczz.17-199},
eprint = {1711.04305v1},
file = {::},
isbn = {0000000345},
issn = {0960-8524},
keywords = {Gibbs sampling,LDA,Latent Dirichlet Allocation,expectation maximization,topic modelling},
title = {{Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey}},
url = {https://arxiv.org/abs/1711.04305v1},
year = {2017}
}
@misc{KantarTNSFinland2017,
author = {{Kantar TNS Finland}},
file = {::},
title = {{In your opinion , how do organic food products differ from other food products ?}},
urldate = {2018-04-11},
year = {2017}
}
@article{Brunet2004,
abstract = {We describe here the use of nonnegative matrix factorization (NMF), an algorithm based on decomposition by parts that can reduce the dimension of expression data from thousands of genes to a handful of metagenes. Coupled with a model selection mechanism, adapted to work for any stochastic clustering algorithm, NMF is an efficient method for identification of distinct molecular patterns and provides a powerful method for class discovery. We demonstrate the ability of NMF to recover meaningful biological information from cancer-related microarray data. NMF appears to have advantages over other methods such as hierarchical clustering or self-organizing maps. We found it less sensitive to a priori selection of genes or initial conditions and able to detect alternative or context-dependent patterns of gene expression in complex biological systems. This ability, similar to semantic polysemy in text, provides a general method for robust molecular pattern discovery.},
author = {Brunet, J P and Tamayo, P and Golub, T R and Mesirov, J P},
doi = {10.1073/pnas.0308531101},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proc Natl Acad Sci U S A},
keywords = {*Computational Biology,*Models, Genetic,Algorithms,Central Nervous System Neoplasms/classification/ge,Data Interpretation, Statistical,Leukemia/classification/genetics,Medulloblastoma/genetics,Neoplasms/*classification/genetics},
number = {12},
pages = {4164--4169},
pmid = {15016911},
title = {{Metagenes and molecular pattern discovery using matrix factorization}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15016911},
volume = {101},
year = {2004}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {::},
journal = {NIPS'13 Proceedings of the 26th International Conference on Neural Information Processing Systems},
keywords = {word2vec},
mendeley-tags = {word2vec},
month = {oct},
pages = {3111--3119},
title = {{Distributed Representations ofWords and Phrases and their Compositionality}},
volume = {2},
year = {2013}
}
@article{Bowman2015,
abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
archivePrefix = {arXiv},
arxivId = {1511.06349},
author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
eprint = {1511.06349},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:pdf},
month = {nov},
title = {{Generating Sentences from a Continuous Space}},
url = {http://arxiv.org/abs/1511.06349},
year = {2015}
}
@article{MacKay2003,
abstract = {Social networks show striking structural regularities, and both theory and evidence suggest that networks may have facilitated the development of large-scale cooperation in humans. Here, we characterize the social networks of the Hadza, a population of hunter-gatherers in Tanzania. We show that Hadza networks have important properties also seen in modernized social networks, including a skewed degree distribution, degree assortativity, transitivity, reciprocity, geographic decay and homophily. We demonstrate that Hadza camps exhibit high between-group and low within-group variation in public goods game donations. Network ties are also more likely between people who give the same amount, and the similarity in cooperative behaviour extends up to two degrees of separation. Social distance appears to be as important as genetic relatedness and physical proximity in explaining assortativity in cooperation. Our results suggest that certain elements of social network structure may have been present at an early point in human history. Also, early humans may have formed ties with both kin and non-kin, based in part on their tendency to cooperate. Social networks may thus have contributed to the emergence of cooperation.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {MacKay, D J C},
doi = {10.1038/nature10736},
eprint = {NIHMS150003},
file = {::},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Information Theory, Inference and Learning Algorithms},
pages = {284--292},
pmid = {22281599},
title = {{An Example Inference Task: Clustering}},
year = {2003}
}
@book{Fleiss1971,
abstract = {The statistic kappa was introduced to measure nominal scale agreement between a fixed pair of raters. In this paper kappa is generalized to the case where each of a sample of subjects is rated on a nominal scale by the same number of raters, but where the raters rating one subject are not necessarily the same as those rating another. Large sample standard errors are derived, and a numerical example is given.},
author = {Fleiss, Joseph L.},
booktitle = {Psychological Bulletin},
doi = {10.1037/h0031619},
isbn = {0033-2909},
issn = {00332909},
keywords = {nominal scale agreement measurement among many rat},
month = {nov},
number = {5},
pages = {378--382},
pmid = {14605196},
title = {{Measuring nominal scale agreement among many raters}},
volume = {76},
year = {1971}
}
@article{Griffiths2007,
abstract = {Slides},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Steyvers, M and Griffiths, T},
doi = {10.1109/TKDE.2009.122},
eprint = {1111.6189v1},
file = {::},
isbn = {0805854185},
issn = {10414347},
journal = {Handbook of Latent Semantic Analysis: A Road to Meaning},
pages = {424--440},
pmid = {9982802760707177758},
title = {{Probabilistic topic models}},
year = {2007}
}
@article{Hutter2009,
abstract = {The identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm configuration problem. More formally, we provide methods for optimizing a target algorithm's performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for SAT. We also present what is, to our knowledge, the first published work on automatically configuring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identified with considerable effort. Nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements.},
author = {Hutter, Frank and Hoos, Holger H and Ca, Kevinlb and Be, Stuetzle},
file = {::},
journal = {Journal of Artificial Intelligence Research},
pages = {267--306},
title = {{ParamILS: An Automatic Algorithm Configuration Framework Kevin Leyton-Brown Thomas St{\"{u}}tzleSt¨St{\"{u}}tzle}},
url = {http://www.ilog.com/products/cplex/},
volume = {36},
year = {2009}
}
@article{Rivard2000,
author = {Rivard, By Hugues and Member, Associate and Fenves, Steven J and Member, Honorary},
file = {::},
keywords = {transfer learning},
mendeley-tags = {transfer learning},
number = {July},
pages = {151--159},
title = {{REPRESENTATION STABILITY AS A REGULARIZER FOR IMPROVED TEXT ANALYTICS TRANSFER LEARNING}},
volume = {14},
year = {2000}
}
@inproceedings{Joulin2016,
abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\~{}}CPU, and classify half a million sentences among{\~{}}312K classes in less than a minute.},
address = {Valencia, Spain},
archivePrefix = {arXiv},
arxivId = {1607.01759},
author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
eprint = {1607.01759},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:pdf},
keywords = {()},
pages = {427--431},
publisher = {Association for Computational Linguistics},
title = {{Bag of Tricks for Efficient Text Classification}},
url = {https://github.com/facebookresearch/fastText http://arxiv.org/abs/1607.01759},
year = {2016}
}
@article{Weik2006,
abstract = {Representations for semantic information about words are necessary for many applications of neural networks in natural language processing. This paper describes an efficient, corpus-based method for inducing distributed semantic representations for a large number of words (50,000) from lexical coccurrence statistics by means of a large-scale linear regression. The representations are successfully applied to word sense disambiguation using a nearest neighbor method .},
author = {Weik, Martin H. and Weik, Martin H.},
doi = {10.1007/1-4020-0613-6_21189},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{u}}tze - 1992 - Word Space(3).pdf:pdf},
journal = {Computer Science and Communications Dictionary},
pages = {1930--1930},
title = {{Word Space}},
year = {2006}
}
@article{Hoffman2010,
abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time. 1},
author = {Hoffman, MD and Blei, DM and Bach, Francis},
doi = {10.1.1.187.1883},
file = {::},
isbn = {9781450300551},
issn = {08912017},
journal = {Nips},
pages = {1--9},
title = {{Online learning for latent dirichlet allocation}},
url = {http://videolectures.net/site/normal{\_}dl/tag=83534/nips2010{\_}1291.pdf},
year = {2010}
}
@article{Reinsel2018,
author = {Reinsel, David and Gantz, John and Rydning, John},
file = {::},
journal = {Idc},
keywords = {IDC, Seagate},
number = {November},
title = {{Data Age 2025: The Digitization of the World From Edge to Core}},
url = {https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf},
year = {2018}
}
@techreport{Gao2018,
author = {Gao, Yingqiang},
file = {::},
number = {October},
title = {{A Deep Learning Approach for Aspect-Based Sentiment Analysis}},
year = {2018}
}
@article{Shen2015,
abstract = {Sales forecasting is one of the most critical steps of business process. Since the forecasting accuracy of traditional techniques is generally unacceptable for products with irregular or non-seasonal sales trends, it is necessary to construct a new forecasting method. Past research shows that there is a strong relationship between online word-of-mouth and product sales, but that the extent of the impact of word-of-mouth varies with product category. This study aims to provide an understanding of how electronic word-of-mouth affects product sales by analyzing online review properties, reviewer characteristics and review influences. This new electronic word-of-mouth perspective contributes to sales forecasting research in two ways. First, a novel classification model involving polarity mining, intensity mining and influence analysis is proposed with a framework to elucidate the difference between review categories. Second, the influence of online reviews (i.e., electronic word-of-mouth) is estimated and then used to construct a sales forecasting model. The proposed online word-of-mouth-based sales forecasting method is evaluated by using real data from a well-known cosmetic retail chain in Taiwan. The experimental results demonstrate that the proposed method is especially suitable for products with abundant online reviews and outperforms traditional time series forecasting models for most consumer products examined.},
author = {Shen, Ching-chin Chern Chih-ping Wei Fang-yi and Fan, Yu-neng},
doi = {10.1007/s10257-014-0265-0},
file = {:Users/felix/Downloads/Chern2015{\_}Article{\_}ASalesForecastingModelForConsu.pdf:pdf},
isbn = {1025701402},
issn = {1617-9846},
journal = {Information Systems and e-Business Management},
keywords = {Electronic word-of-mouth,Online review,Sales forecasts,Text mining,Time series data},
number = {3},
pages = {445--473},
publisher = {Springer Berlin Heidelberg},
title = {{A sales forecasting model for consumer products based on the influence of online word-of-mouth}},
url = {http://dx.doi.org/10.1007/s10257-014-0265-0},
volume = {13},
year = {2015}
}
@article{Musat2012,
abstract = {This work outlines a novel system that automatically extracts conceptual labels for statistically obtained topics. By creating a projection of the topic, which is a distribution over all the vocabulary words, over the WordNet ontology we succeed in associating concepts to the said groups of words. The most important contributions of this paper are connected to the validation of the role of these concepts as topical labels and the determination of correlations that emerge between the utility of these labels and the strength of the relation between the concepts and the topics.},
author = {Muşat, Claudiu Cristian and Trǎuşan-Matu, Ştefan and Velcin, Julien and Rizoiu, Marian-Andrei},
file = {::},
issn = {1454234X},
journal = {UPB Scientific Bulletin, Series C: Electrical Engineering},
keywords = {Conceptual processing,Labels,Topic models,WordNet},
number = {2},
pages = {57--68},
title = {{Automatic extraction of conceptual labels from topic models}},
volume = {74},
year = {2012}
}
@article{Newman2010,
abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on point- wise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
author = {Newman, David and Lau, Jh and Grieser, Karl and Baldwin, Timothy},
doi = {10.3115/1220175.1220274},
file = {::},
isbn = {1932432655},
journal = {{\ldots} Language Technologies: The {\ldots}},
number = {June},
pages = {100--108},
title = {{Automatic evaluation of topic coherence}},
url = {http://dl.acm.org/citation.cfm?id=1858011},
year = {2010}
}
@article{Li2016,
abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
archivePrefix = {arXiv},
arxivId = {1606.09282},
author = {Li, Zhizhong and Hoiem, Derek},
doi = {10.1007/978-3-319-46493-0_37},
eprint = {1606.09282},
file = {::},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
pages = {1--13},
pmid = {4520227},
title = {{Learning without Forgetting}},
url = {http://arxiv.org/abs/1606.09282},
year = {2016}
}
@article{Vriens2018,
author = {Vriens, Marco and Chen, Song and Vidden, Chad},
doi = {10.1177/1470785318810106},
issn = {1470-7853},
journal = {International Journal of Market Research},
keywords = {additive trees,brand similarities,brand similarities,market structure analysis,multi,hierarchical clustering,market structure analysis,multidimensional scaling,online consumer data,perceptual maps,t-sne},
pages = {147078531881010},
title = {{Mapping brand similarities: Comparing consumer online comments versus survey data}},
url = {https://doi.org/10.1177/1470785318810106},
year = {2018}
}
@book{goodfellow2016deep,
author = {Goodfellow, I and Bengio, Y and Courville, A},
isbn = {9780262035613},
publisher = {MIT Press},
series = {Adaptive Computation and Machine Learning series},
title = {{Deep Learning}},
url = {https://books.google.de/books?id=Np9SDQAAQBAJ},
year = {2016}
}
@article{Carbonell1998,
abstract = {This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization. The latter are borne out by the recent results of the SUMMAC conference in the evaluation of summarization systems. However, the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection.},
author = {Carbonell, Jaime and Goldstein, Jade},
doi = {10.1145/290941.291025},
file = {::},
isbn = {1581130155},
issn = {01635840 (ISSN)},
journal = {Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval  - SIGIR '98},
number = {June},
pages = {335--336},
pmid = {18708246},
title = {{The use of MMR, diversity-based reranking for reordering documents and producing summaries}},
url = {http://portal.acm.org/citation.cfm?doid=290941.291025},
year = {1998}
}
@article{Conneau2017a,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
archivePrefix = {arXiv},
arxivId = {1705.02364},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
doi = {10.1.1.156.2685},
eprint = {1705.02364},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Conneau et al. - 2018 - Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.pdf:pdf},
isbn = {978-1-109-24088-7},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
url = {http://arxiv.org/abs/1705.02364},
year = {2017}
}
@inproceedings{Popescu2005,
abstract = {Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised information-extraction system which mines reviews in order to build a model of im- portant product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22{\%} higher precision (with only 3{\%} lower recall) on the feature extraction task. OPINE's novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.},
address = {Vancouver, Canada},
author = {Popescu, Ana-Maria and Etzioni, Oren},
booktitle = {Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP)},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Popescu, Etzioni - 2005 - OPINE Extracting product features and opinions from reviews.pdf:pdf},
number = {October},
pages = {339--346},
title = {{OPINE: Extracting product features and opinions from reviews}},
url = {https://www.aclweb.org/anthology/H05-1043 http://dl.acm.org/citation.cfm?id=1225750},
year = {2005}
}
@article{Ding2008,
abstract = {Non-negative Matrix Factorization (NMF) and Probabilistic Latent Semantic Indexing (PLSI) have been successfully applied to document clustering recently. In this paper, we show that PLSI and NMF (with the I-divergence objective function) optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoretical basis for a new hybrid method that runs PLSI and NMF alternatively, each jumping out of the local minima of the other method successively, thus achieving a better final solution. Extensive experiments on five real-life datasets show relations between NMF and PLSI, and indicate that the hybrid method leads to significant improvements over NMF-only or PLSI-only methods. We also show that at first-order approximation, NMF is identical to the ??2-statistic. ?? 2008.},
author = {Ding, Chris and Li, Tao and Peng, Wei},
doi = {10.1016/j.csda.2008.01.011},
isbn = {01679473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
number = {8},
pages = {3913--3927},
title = {{On the equivalence between Non-negative Matrix Factorization and Probabilistic Latent Semantic Indexing}},
volume = {52},
year = {2008}
}
@misc{IVW2018,
author = {IVW},
file = {::},
pages = {2018},
title = {{Verkaufte Auflage der {\"{u}}berregionalen Tageszeitungen in Deutschland im 3 . Quartal 2018}},
year = {2018}
}
@article{Blei2004,
abstract = {We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, gen- erating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior al- lows arbitrarily large branching factors and readily accommodates grow- ing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.},
archivePrefix = {arXiv},
arxivId = {arXiv:0710.0845v2},
author = {Blei, D. and Griffiths, T.L. and Jordan, M.I. and Tenenbaum, J.B.},
doi = {10.1016/0169-023X(89)90004-9},
eprint = {arXiv:0710.0845v2},
isbn = {0262201526},
issn = {0169023X},
journal = {Advances in neural information processing systems},
pmid = {25122015},
title = {{Hierarchical topic models and the nested Chinese restaurant process}},
year = {2004}
}
@article{Emnlp2018,
abstract = {absa;},
archivePrefix = {arXiv},
arxivId = {1808.09238},
author = {{Martin Schmitt}},
eprint = {1808.09238},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Schmitt et al. - 2018 - Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks.pdf:pdf},
pages = {1--5},
title = {{Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks}},
year = {2018}
}
@book{anton1998lineare,
author = {Anton, H},
isbn = {9783827403247},
publisher = {Spektrum Akademischer Verlag},
title = {{Lineare Algebra: Einf{\"{u}}hrung, Grundlagen, {\"{U}}bungen}},
url = {https://books.google.de/books?id={\_}qNtQwAACAAJ},
year = {1998}
}
@inproceedings{Vaswani2017c,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
annote = {Summary

Instead of RNN approach uses self attention to focus on important words


state of the art (is used in google translate (see https://youtu.be/rBCqOTEfxvg?t=1575 and https://translate.google.de/{\#}en/fr/The{\%}20city{\%}20councilmen{\%}20refused{\%}20the{\%}20female{\%}20demonstrators{\%}20a{\%}20permit{\%}20because{\%}20they{\%}20feared{\%}20violence) -{\textgreater} makes the same mistakes

training time: 3.5 days on 8 GPUs
state of the art translation 12h on 8 P100 GPUs (eq 7 days on one 1080Ti)

Problem with CNNs:
https://youtu.be/rBCqOTEfxvg?t=578

It's very positional. The last layer does not see every word but instead sees a subset of the sentence. For example word 13, 15, ... 
How do we make sure, that word 13 is actually important. It could also be word 12 that is important. This scheme is referencing by position while referencing by content would be much more natural

With attention one can look in the past and focus on the important parts. With CNNs this is also possible but it requires that you get the exact position you need by chance},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I and Luxburg, U. V. and Bengio, S and Wallach, H and Fergus, R. and Vishwanathan, S. and Garnett, R.},
eprint = {1706.03762},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
keywords = {attention},
mendeley-tags = {attention},
month = {jun},
pages = {5998----6008},
publisher = {Curran Associates, Inc.},
title = {{Attention Is All You Need}},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
year = {2017}
}
@article{Kim2015,
abstract = {The present study investigates topic coverage and sentiment dynamics of two different media sources, Twitter and news publications, on the hot health issue of Ebola. We conduct content and sentiment analysis by: (1) applying vocabulary control to collected datasets; (2) employing the n-gram LDA topic modeling technique; (3) adopting entity extraction and entity network; and (4) introducing the concept of topic-based sentiment scores. With the query term ‘Ebola' or ‘Ebola virus', we collected 16,189 news articles from 1006 different publications and 7,106,297 tweets with the Twitter stream API. The experiments indicate that topic coverage of Twitter is narrower and more blurry than that of the news media. In terms of sentiment dynamics, the life span and variance of sentiment on Twitter is shorter and smaller than in the news. In addition, we observe that news articles focus more on event-related entities such as person, organization and location, whereas Twitter covers more time-oriented entities. Based on the results, we report on the characteristics of Twitter and news media as two distinct news outlets in terms of content coverage and sentiment dynamics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kim, Erin Hea Jin and Jeong, Yoo Kyung and Kim, Yuyoung and Kang, Keun Young and Song, Min},
doi = {10.1177/0165551515608733},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0165551515608},
issn = {17416485},
journal = {Journal of Information Science},
keywords = {Ebola,sentiment analysis,text-mining,topic models},
number = {6},
pages = {763--781},
pmid = {24131618},
title = {{Topic-based content and sentiment analysis of Ebola virus on Twitter and in the news}},
volume = {42},
year = {2015}
}
@book{Manning2008,
abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar and Schutze, Hinrich},
doi = {10.1017/CBO9780511809071},
eprint = {0521865719 9780521865715},
file = {::},
isbn = {9780511809071},
issn = {08912017},
pmid = {3393},
title = {{Introduction to Information Retrieval}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511809071},
year = {2008}
}
@article{Pratt1993,
abstract = {Previously, we have introduced the idea of neural network transfer, where learning on a target problem is sped up by using the weights obtained from a network trained for a related source task. Here, we present a new algorithm. called Discriminability-Based Transfer (DBT), which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network, and rescales transferred weight magnitudes accordingly. Several experiments demonstrate that target networks initialized via DBT learn significantly faster than networks initialized randomly.},
author = {Pratt, L Y},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pratt - Unknown - Discriminability-Based Transfer between Neural Networks.pdf:pdf},
journal = {Advances in neural information processing systems},
pages = {204--211},
title = {{Discriminability-Based Transfer between Neural Networks}},
url = {http://papers.nips.cc/paper/641-discriminability-based-transfer-between-neural-networks.pdf},
year = {1993}
}
@article{Levy2014,
abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word simi-larity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Levy, Omer and Goldberg, Yoav},
doi = {10.1162/153244303322533223},
eprint = {1405.4053},
file = {::},
isbn = {9781634393973},
issn = {10495258},
journal = {Proceedings of Advances in Neural Information Processing Systems (NIPS-2014)},
pages = {2177--2185},
pmid = {1710995},
title = {{Neural Word Embedding as Implicit Matrix Factorization}},
url = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization},
year = {2014}
}
@inproceedings{McAuley2013b,
abstract = {Recommending products to consumers means not only understanding their tastes, but also understanding their level of experience. For example, it would be a mistake to recommend the iconic film Seven Samurai simply because a user enjoys other action movies; rather, we might conclude that they will eventually enjoy it -- once they are ready. The same is true for beers, wines, gourmet foods -- or any products where users have acquired tastes: the `best' products may not be the most `accessible'. Thus our goal in this paper is to recommend products that a user will enjoy now, while acknowledging that their tastes may have changed over time, and may change again in the future. We model how tastes change due to the very act of consuming more products -- in other words, as users become more experienced. We develop a latent factor recommendation system that explicitly accounts for each user's level of experience. We find that such a model not only leads to better recommendations, but also allows us to study the role of user experience and expertise on a novel dataset of fifteen million beer, wine, food, and movie reviews.},
address = {Rio de Janeiro, Brazil},
archivePrefix = {arXiv},
arxivId = {1303.4402},
author = {McAuley, Julian and Leskovec, Jure},
booktitle = {WWW '13 Proceedings of the 22nd international conference on World Wide Web},
doi = {10.1145/2488388.2488466},
eprint = {1303.4402},
file = {:Users/felix/Downloads/www13.pdf:pdf},
isbn = {9781450320351},
keywords = {H33 [Information Search and Retrieval]: Informatio,expertise,user modeling},
pages = {897--908},
publisher = {ACM New York, NY, USA},
title = {{From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews}},
url = {http://i.stanford.edu/{~}julian/pdfs/www13.pdf http://arxiv.org/abs/1303.4402},
year = {2013}
}
@misc{IVW2018,
author = {IVW},
file = {::},
title = {{Verkaufte Auflage der {\"{u}}berregionalen Tageszeitungen in Deutschland im 4. Quartal 2017}},
url = {http://de.statista.com/statistik/daten/studie/73448/umfrage/auflage-der-ueberregionalen-tageszeitungen/},
urldate = {2018-03-25},
year = {2018}
}
@article{Ireland2018,
abstract = {Advanced data analytics is one of the most revolutionary technological developments in the 21st century, which enables the discovery of underlining trends via sophisticated computational methods On various e-commerce and social platforms, millions of online product reviews are published by customers, which can potentially provide designers with invaluable insights into product design. This paper presents a design framework to analyze online product reviews. The objective is to use this machine-generated data to identify a series of customer needs. The framework aims to distill large volumes of qualitative data into quantitative insights on product features, so that designers can make more informed decisions. The framework combines the elements of online product reviews, design theory and methodology, and data analytics to reveal new insights. The effectiveness of the proposal framework is validated through a case study on product reviews from the e-commerce website, Amazon. The framework demonstrates a statistical approach for analyzing online product reviews. The framework acts as an interface between quantitative outputs and the qualitative and creative process of design. Further analysis of results identifies many of incorporating logical, computational methods into the highly subjective and creative process of design.},
author = {Ireland, Robert and Liu, Ang},
doi = {10.1016/J.CIRPJ.2018.06.003},
issn = {1755-5817},
journal = {CIRP Journal of Manufacturing Science and Technology},
month = {nov},
pages = {128--144},
publisher = {Elsevier},
title = {{Application of data analytics for product design: Sentiment analysis of online product reviews}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S1755581718300336},
volume = {23},
year = {2018}
}
@inproceedings{McAuley2012,
abstract = {The majority of online reviews consist of plain-text feedback together with a single numeric score. However, there are multiple dimensions to products and opinions, and understanding the `aspects' that contribute to users' ratings may help us to better understand their individual preferences. For example, a user's impression of an audiobook presumably depends on aspects such as the story and the narrator, and knowing their opinions on these aspects may help us to recommend better products. In this paper, we build models for rating systems in which such dimensions are explicit, in the sense that users leave separate ratings for each aspect of a product. By introducing new corpora consisting of five million reviews, rated with between three and six aspects, we evaluate our models on three prediction tasks: First, we use our model to uncover which parts of a review discuss which of the rated aspects. Second, we use our model to summarize reviews, which for us means finding the sentences that best explain a user's rating. Finally, since aspect ratings are optional in many of the datasets we consider, we use our model to recover those ratings that are missing from a user's evaluation. Our model matches state-of-the-art approaches on existing small-scale datasets, while scaling to the real-world datasets we introduce. Moreover, our model is able to `disentangle' content and sentiment words: we automatically learn content words that are indicative of a particular aspect as well as the aspect-specific sentiment words that are indicative of a particular rating.},
author = {McAuley, Julian and Leskovec, Jure and Jurafsky, Dan},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2012.110},
file = {:Users/felix/Downloads/icdm2012.pdf:pdf},
isbn = {9780769549057},
issn = {15504786},
keywords = {Machine learning,Segmentation,Sentiment analysis,Summarization},
pages = {1020--1025},
title = {{Learning attitudes and attributes from multi-aspect reviews}},
year = {2012}
}
@article{Zanzotto2010,
abstract = {In distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored. In this paper we propose a novel approach to estimate parameters for a class of compositional distributional models: the additive models. Our approach leverages on two main ideas. Firstly, a novel idea for extracting compositional distributional semantics examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics.},
author = {Zanzotto, F M and Korkontzelos, I and Fallucchi{\ldots}, F},
file = {::},
journal = {Proceedings of the 23rd {\ldots}},
number = {August},
pages = {1263--1271},
title = {{Estimating linear models for compositional distributional semantics}},
url = {http://dl.acm.org/citation.cfm?id=1873923{\%}5Cnpapers://5860649b-6292-421d-b3aa-1b17a5231ec5/Paper/p139382},
volume = {1},
year = {2010}
}
@article{Hammersley2009,
abstract = {Metadynamics is an enhanced sampling method designed to flatten free energy surfaces uniformly. However, the highest-energy regions are often irrelevant to study and dangerous to explore because systems often respond irreversibly in unforeseen ways in response to driving forces in these regions, spoiling the sampling. Introducing an on-the-fly domain restriction allows metadynamics to flatten only up to a specified energy level and no further, improving efficiency and safety while decreasing the pressure on practitioners to design collective variables that are robust to otherwise irrelevant high energy driving. This paper describes a new method that achieves this using sequential on-the-fly estimation of energy wells and redefinition of the metadynamics hill shape, termed metabasin metadynamics. The energy level may be defined {\{}$\backslash$em a priori{\}} or relative to unknown barrier energies estimated on the fly. Altering only the hill ensures that the method is compatible with many other advances in metadynamics methodology. The hill shape has a natural interpretation in terms of multiscale dynamics and the computational overhead in simulation is minimal when studying systems of any reasonable size, for instance proteins or other macromolecules. Three example applications show that the formula is accurate and robust to complex dynamics, making metadynamics significantly more forgiving with respect to CV quality and thus more feasible to apply to the most challenging biomolecular systems.$\backslash$nMetadynamics is an enhanced sampling method designed to flatten free energy surfaces uniformly. However, the highest-energy regions are often irrelevant to study and dangerous to explore because systems often respond irreversibly in unforeseen ways in response to driving forces in these regions, spoiling the sampling. Introducing an on-the-fly domain restriction allows metadynamics to flatten only up to a specified energy level and no further, improving efficiency and safety while decreasing the pressure on practitioners to design collective variables that are robust to otherwise irrelevant high energy driving. This paper describes a new method that achieves this using sequential on-the-fly estimation of energy wells and redefinition of the metadynamics hill shape, termed metabasin metadynamics. The energy level may be defined {\{}$\backslash$em a priori{\}} or relative to unknown barrier energies estimated on the fly. Altering only the hill ensures that the method is compatible with many other advances in metadynamics methodology. The hill shape has a natural interpretation in terms of multiscale dynamics and the computational overhead in simulation is minimal when studying systems of any reasonable size, for instance proteins or other macromolecules. Three example applications show that the formula is accurate and robust to complex dynamics, making metadynamics significantly more forgiving with respect to CV quality and thus more feasible to apply to the most challenging biomolecular systems.},
author = {Hammersley, Martin},
doi = {10.1021/acs.jctc.5b00907},
file = {::},
isbn = {9781595938596},
issn = {15499626},
journal = {Methodological Innovations Online},
number = {2},
pages = {1--11},
pmid = {26587809},
title = {{Open Research Online}},
volume = {4},
year = {2009}
}
@inproceedings{Blitzer2008,
abstract = {Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.},
author = {Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Wortman, Jennifer},
booktitle = {Advances in Neural Information Processing Systems 20 (NIPS 2007)},
editor = {{J. C. Platt and D. Koller and Y. Singer and S. T. Roweis}},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Blitzer et al. - 2008 - Learning Bounds for Domain Adaptation.pdf:pdf},
pages = {129----136},
publisher = {Curran Associates, Inc.},
title = {{Learning Bounds for Domain Adaptation}},
url = {https://papers.nips.cc/paper/3212-learning-bounds-for-domain-adaptation.pdf},
year = {2008}
}
@inproceedings{Hu2004,
abstract = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
address = {Seattle, WA, USA},
author = {Hu, Minqing and Liu, Bing},
booktitle = {Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '04},
doi = {10.1145/1014052.1014073},
file = {:Users/felix/Downloads/p168-hu.pdf:pdf},
isbn = {1581138889},
pages = {168--177},
title = {{Mining and summarizing customer reviews}},
url = {http://portal.acm.org/citation.cfm?doid=1014052.1014073},
year = {2004}
}
@article{Griffiths2004,
abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. {\&} Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying "hot topics" by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
author = {Griffiths, T. L. and Steyvers, M.},
doi = {10.1073/pnas.0307752101},
file = {::},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {Supplement 1},
pages = {5228--5235},
pmid = {14872004},
title = {{Finding scientific topics}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0307752101},
volume = {101},
year = {2004}
}
@article{Hallgren2012,
author = {Hallgren, Kevin A},
file = {::},
keywords = {behavioral observation,coding,inter-rater agreement,intra-class correlation,kappa,reliability},
number = {1},
pages = {23--34},
title = {{NIH Public Access}},
volume = {8},
year = {2012}
}
@article{Griffiths2002,
abstract = {Semantic networks produced from human data have statistical properties that cannot be easily captured by spatial representations. We explore a probabilistic approach to semantic representation that explicitly models the probability with which words occur in different contexts, and hence captures the proba- bilistic relationships between words. We show that this representation has statistical properties consistent with the large-scale structure of semantic networks constructed by humans, and trace the origins of these properties.},
author = {Griffiths, Thomas L. and Steyvers, Mark},
journal = {Proceedings of the 24th Annual Conference of the Cognitive Science Society},
pages = {381--386},
title = {{A probabilistic approach to semantic representation}},
year = {2002}
}
@article{Thet2010,
abstract = {In this article, a method for automatic sentiment analysis of movie reviews is proposed, implemented and evaluated. In contrast to most studies that focus on determining only sentiment orientation (positive versus negative), the proposed method performs fine-grained analysis to determine both the sentiment orientation and sentiment strength of the reviewer towards various aspects of a movie. Sentences in review documents contain independent clauses that express different sentiments toward different aspects of a movie. The method adopts a linguistic approach of computing the sentiment of a clause from the prior sentiment scores assigned to individual words, taking into consideration the grammatical dependency structure of the clause. The prior sentiment scores of about 32,000 individual words are derived from SentiWordNet with the help of a subjectivity lexicon. Negation is delicately handled. The output sentiment scores can be used to identify the most positive and negative clauses or sentences with respect to particular movie aspects.},
author = {Thet, Tun Thura and Na, Jin Cheon and Khoo, Christopher S.G.},
doi = {10.1177/0165551510388123},
file = {:Users/felix/Downloads/10.1.1.885.713.pdf:pdf},
issn = {01655515},
journal = {Journal of Information Science},
keywords = {discussion board,opinion mining,sentiment analysis},
number = {6},
pages = {823--848},
title = {{Aspect-based sentiment analysis of movie reviews on discussion boards}},
volume = {36},
year = {2010}
}
@article{Netzer2012,
abstract = {Web 2.0 provides gathering places for Internet users in blogs, forums, and chat rooms. These gathering places leave footprints in the form of colossal amounts of data regarding consumers' thoughts, beliefs, experiences, and even interactions. In this paper, we propose an approach for firms to explore online user-generated content and " listen " to what customers write about their and their competitors' products. Our objective is to convert the user-generated content to market structures and competitive landscape insights. The difficulty in obtaining such market-structure insights from online user-generated content is that consumers' postings are often not easy to syndicate. To address these issues, we employ a text-mining approach and combine it with semantic network analysis tools. We demonstrate this approach using two cases—sedan cars and diabetes drugs—generating market-structure perceptual maps and meaningful insights without interviewing a single consumer. We compare a market structure based on user-generated content data with a market structure derived from more traditional sales and survey-based data to establish validity and highlight meaningful differences. },
author = {Netzer, Oded and Feldman, Ronen and Goldenberg, Jacob and Fresko, Moshe},
doi = {10.1287/mksc.1120.0713},
issn = {0732-2399},
journal = {Marketing Science},
keywords = {2010,2012,accepted,and alan montgomery served,as associate editor for,history,january 20,january 30,market structure,marketing research,peter fader served as,received,text mining,the special issue editor,this article,user-generated content},
number = {3},
pages = {521--543},
title = {{Mine Your Own Business: Market-Structure Surveillance Through Text Mining}},
volume = {31},
year = {2012}
}
@article{Christensen2017,
abstract = {Online communities are attractive sources of ideas relevant for new product development and innovation.However,making sense of the ‘bigdata' in these communities is a complex analytical task.Asystematicway of dealing with these data is needed to exploit their potential for boosting companies' innovation performance.Wepropose amethod for analysing online community data with a special focus on identifying ideas. We employ a research designwhere two human raters classified 3,000 texts extractedfroman online community, accordingtowhether the text contained an idea. Among the 3,000, 137 idea texts and 2,666 non-idea texts were identified. The human raters could not agree on the remaining 197 texts. These texts were omitted from the analysis. The remaining 2,803 texts were processed by using text mining techniques and used to train a classificationmodel.Wedescribe howto tune themodel andwhich textmining steps to perform. We conclude that machine learning and text mining can be useful for detecting ideas in online communities. Themethod can help researchers and firms identify ideas hidden in large amounts of texts. Also, it is interesting in its own right thatmachine learning can be used to detect ideas.},
author = {Christensen, Kasper and N{\o}rskov, Sladjana and Frederiksen, Lars and Scholderer, Joachim},
doi = {10.1111/caim.12202},
isbn = {1467-8691},
issn = {14678691},
journal = {Creativity and Innovation Management},
number = {1},
pages = {17--30},
title = {{In Search of New Product Ideas: Identifying Ideas in Online Communities by Machine Learning and Text Mining}},
volume = {26},
year = {2017}
}
@inproceedings{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Weinberger, Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q.},
eprint = {1411.1792},
file = {::},
pages = {3320----3328},
publisher = {Curran Associates, Inc.},
title = {{How transferable are features in deep neural networks?}},
url = {https://arxiv.org/pdf/1411.1792.pdf http://arxiv.org/abs/1411.1792 http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf},
volume = {27},
year = {2014}
}
@article{El-Assady2018,
abstract = {IEEE Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.},
author = {El-Assady, Mennatallah and Sevastjanova, Rita and Sperrle, Fabian and Keim, Daniel and Collins, Christopher},
doi = {10.1109/TVCG.2017.2745080},
isbn = {1077-2626 VO - PP},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Feature Detection and Tracking,Iterative Optimization,Reinforcement Learning,Topic Model Configuration},
number = {1},
pages = {382--391},
pmid = {28866566},
title = {{Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework}},
volume = {24},
year = {2018}
}
@article{Socher2013,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
archivePrefix = {arXiv},
arxivId = {1690219.1690245‎},
author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y. and Chuang, Jason and Manning, Christopher D. and Ng, Andrew Y and Potts, Christopher},
doi = {10.1371/journal.pone.0073791},
eprint = {1690219.1690245‎},
file = {:Users/felix/Downloads/EMNLP2013{\_}RNTN.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {1631--1642},
pmid = {24086296},
title = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
volume = {8},
year = {2013}
}
@inproceedings{Baccianella2010,
abstract = {In this work we present SENTIWORDNET 3.0, a lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide. Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET 2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores. We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0. We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20{\%} with respect to SENTIWORDNET 1.0.},
address = {Valletta, Malta},
author = {Baccianella, Stefano and Esuli, Andrea and Sebastiani, Fabrizio},
booktitle = {Proceedings of the International Conference on Language Resources and Evaluation},
file = {:Users/felix/Downloads/LREC10.pdf:pdf},
pages = {2200 -- 2204},
title = {{SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining}},
volume = {1},
year = {2010}
}
@article{Zhang2017a,
abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.},
archivePrefix = {arXiv},
arxivId = {1707.08114},
author = {Zhang, Yu and Yang, Qiang},
eprint = {1707.08114},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Yang - Unknown - A Survey on Multi-Task Learning.pdf:pdf},
keywords = {Artificial Intelligence !,Index Terms-Multi-Task Learning,Machine Learning},
title = {{A Survey on Multi-Task Learning}},
url = {https://arxiv.org/pdf/1707.08114.pdf http://arxiv.org/abs/1707.08114},
year = {2017}
}
@article{Zielinski2018,
abstract = {Topological data analysis, such as persistent homology has shown beneficial properties for machine learning in many tasks. Topological representations, such as the persistence diagram (PD), however, have a complex structure (multiset of intervals) which makes it difficult to combine with typical machine learning workflows. We present novel compact fixed-size vectorial representations of PDs based on clustering and bag of words encodings that cope well with the inherent sparsity of PDs. Our novel representations outperform state-of-the-art approaches from topological data analysis and are computationally more efficient.},
archivePrefix = {arXiv},
arxivId = {1802.04852},
author = {Zielinski, Bartosz and Juda, Mateusz and Zeppelzauer, Matthias},
eprint = {1802.04852},
file = {::},
title = {{Persistence Codebooks for Topological Data Analysis}},
url = {http://arxiv.org/abs/1802.04852},
year = {2018}
}
@article{Blei2006a,
abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.1486v1},
author = {Blei, David M. and Lafferty, John D.},
doi = {10.1145/1143844.1143859},
eprint = {arXiv:0712.1486v1},
isbn = {1595933832},
issn = {19326157},
journal = {Advances in Neural Information Processing Systems 18},
pages = {147--154},
pmid = {9013932},
title = {{Correlated Topic Models}},
url = {papers2://publication/uuid/1191CDB8-6BB3-4201-8EFB-6F7B8CBA0E8F},
year = {2006}
}
@article{Li2016,
abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
archivePrefix = {arXiv},
arxivId = {1606.09282},
author = {Li, Zhizhong and Hoiem, Derek},
doi = {10.1007/978-3-319-46493-0_37},
eprint = {1606.09282},
file = {::},
isbn = {978-3-319-46447-3},
issn = {0302-9743},
pages = {1--13},
pmid = {4520227},
title = {{Learning without Forgetting}},
url = {http://arxiv.org/abs/1606.09282},
year = {2016}
}
@article{Alec2018,
abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9{\%} on commonsense reasoning (Stories Cloze Test), 5.7{\%} on question answering (RACE), and 1.5{\%} on textual entailment (MultiNLI).},
author = {Alec, Radford and Karthik, Narasimhan and Tim, Salimans and Openai, Ilya Sutskever},
doi = {10.1093/aob/mcp031},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Alec et al. - 2018 - Improving Language Understanding by Generative Pre-Training.pdf:pdf},
issn = {1095-8290},
journal = {OpenAI},
pages = {12},
pmid = {19218577},
title = {{Improving Language Understanding by Generative Pre-Training}},
url = {https://gluebenchmark.com/leaderboard},
year = {2018}
}
@incollection{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Computer Vision – ECCV 2014. ECCV 2014. Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-10590-1_53},
file = {::},
pages = {818--833},
publisher = {Springer, Cham},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53},
year = {2014}
}
@article{Hercig2016,
abstract = {This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
doi = {10.18653/v1/s16-1055},
eprint = {NIHMS150003},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pontiki et al. - Unknown - SemEval-2016 Task 5 Aspect Based Sentiment Analysis.pdf:pdf},
isbn = {978-1-941643-95-2},
issn = {03063674},
keywords = {SemEval,SemEval Winners,Sentiment,Winner},
mendeley-tags = {SemEval,SemEval Winners,Sentiment,Winner},
pages = {342--349},
pmid = {17390532},
title = {{SemEval-2016 Task 5: Aspect Based Sentiment Analysis}},
year = {2016}
}
@article{Hofmann1999,
abstract = {Probabilistic Latente Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrievaland filtering, natural language processing, machine learning fromtext,and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular VAlue Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. Inorder to avoid overfitting, we propose a widely applicable generalization of maximumlikelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.3900v2},
author = {Hofmann, Thomas},
doi = {10.1.1.33.1187},
eprint = {arXiv:1212.3900v2},
file = {::},
isbn = {1581130961},
issn = {15206882},
journal = {Uncertainity in Artifitial Intelligence - UAI'99},
pages = {8},
pmid = {18989936},
title = {{Probabilistic Latent Semantic Analysis}},
year = {1999}
}
@article{Yang2016,
abstract = {Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.},
archivePrefix = {arXiv},
arxivId = {1707.00896},
author = {Pappas, Nikolaos and Popescu-Belis, Andrei},
doi = {10.18653/v1/N16-1174},
eprint = {1707.00896},
file = {::},
isbn = {9781941643914},
issn = {1606.02393},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1480--1489},
title = {{Multilingual Hierarchical Attention Networks for Document Classification}},
url = {http://arxiv.org/abs/1707.00896},
year = {2017}
}
@article{Kanungo2002,
abstract = {In k-means clustering, we are given a set of n data points in d-dimensional space Rd and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {Kanungo, Tapas and Mount, David M. and Netanyahu, Nathan S. and Piatko, Christine D. and Silverman, Ruth and Wu, Angela Y.},
doi = {10.1109/TPAMI.2002.1017616},
eprint = {arXiv:0711.0189v1},
file = {::},
isbn = {978-1-4673-0892-2},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computational geometry,Data mining,Knowledge discovery,Machine learning,Nearest-neighbor searching,Pattern recognition,k-d tree,k-means clustering},
number = {7},
pages = {881--892},
pmid = {19784854},
title = {{An efficient k-means clustering algorithms: Analysis and implementation}},
volume = {24},
year = {2002}
}
@article{Danner2019,
author = {Danner, Hannah and Menapace, Luisa},
file = {::},
keywords = {consumer beliefs,organic food,social media},
pages = {1--34},
title = {{Using Online Comments to Explore Consumer Beliefs regarding Organic Food in German-Speaking Countries and the United States}},
year = {2019}
}
@article{Asuncion2009,
abstract = {Latent Dirichlet Analysis, or topic modeling, is a flexible latent variable framework for modeling high-dimensional sparse count data. Various learning algorithms have been developed in recent years, including collapsed Gibbs sampling, variational inference, and maximum a posteriori estimation, and this variety motivates the need for careful empirical comparisons. In this paper, we highlight the close connections between these approaches. We find that the main differences are attributable to the amount of smoothing applied to the counts. When the hyperparameters are optimized, the differences in performance among the algorithms diminish significantly. The ability of these algorithms to achieve solutions of comparable accuracy gives us the freedom to select computationally efficient approaches. Using the insights gained from this comparative study, we show how accurate topic models can be learned in several seconds on text corpora with thousands of documents.},
archivePrefix = {arXiv},
arxivId = {1205.2662},
author = {Asuncion, Arthur and Welling, Max and Smyth, Padraic and Teh, Yee Whye},
eprint = {1205.2662},
isbn = {978-0-9749039-5-8},
journal = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
keywords = {Computational,Information Retrieval {\&} Textual Information Access,Information-Theoretic Learning with,Learning/Statistics {\&} Optimisation,Natural Language Processing,Theory {\&} Algorithms},
number = {Ml},
pages = {27--34},
title = {{On Smoothing and Inference for Topic Models}},
url = {http://eprints.pascal-network.org/archive/00006729/},
year = {2009}
}
@techreport{Kaiser2018,
abstract = {Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. We present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neu-ral machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.},
archivePrefix = {arXiv},
arxivId = {1803.03382v6},
author = {Kaiser, {\L}ukasz and Roy, Aurko and Vaswani, Ashish and Parmar, Niki and Bengio, Samy and Uszkoreit, Jakob and Shazeer, Noam},
eprint = {1803.03382v6},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Kaiser et al. - 2018 - Fast Decoding in Sequence Models Using Discrete Latent Variables.pdf:pdf},
title = {{Fast Decoding in Sequence Models Using Discrete Latent Variables}},
url = {https://arxiv.org/pdf/1803.03382.pdf},
year = {2018}
}
@misc{Pleple2013,
author = {Plepl{\'{e}}, Quentin},
title = {{Perplexity To Evaluate Topic Models}},
url = {http://qpleple.com/perplexity-to-evaluate-topic-models/},
urldate = {2018-03-28},
year = {2013}
}
@article{Schwarm2005,
abstract = {Reading proficiency is a fundamen- tal component of language competency. However, finding topical texts at an appro- priate reading level for foreign and sec- ond language learners is a challenge for teachers. This task can be addressed with natural language processing technology to assess reading level. Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the bene- fit of using statistical language models. In this paper, we also use support vector machines to combine features from tradi- tional reading level measures, statistical language models, and other language pro- cessing tools to produce a better method of assessing reading level. 1},
author = {Schwarm, Sarah E. and Ostendorf, Mari},
doi = {10.3115/1219840.1219905},
file = {::},
isbn = {1932432515},
issn = {0167-6369},
journal = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics  - ACL '05},
number = {June},
pages = {523--530},
title = {{Reading level assessment using support vector machines and statistical language models}},
url = {http://portal.acm.org/citation.cfm?doid=1219840.1219905},
year = {2005}
}
@article{Brokos2016,
abstract = {We propose a document retrieval method for question answering that represents documents and questions as weighted centroids of word embeddings and reranks the retrieved documents with a relaxation of Word Mover's Distance. Using biomedical questions and documents from BIOASQ, we show that our method is competitive with PUBMED. With a top-k approximation, our method is fast, and easily portable to other domains and languages.},
archivePrefix = {arXiv},
arxivId = {1608.03905},
author = {Brokos, Georgios-Ioannis and Malakasiotis, Prodromos and Androutsopoulos, Ion},
doi = {10.18653/v1/W16-2915},
eprint = {1608.03905},
isbn = {9780387496160},
title = {{Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering}},
url = {http://arxiv.org/abs/1608.03905},
year = {2016}
}
@article{Exposure2009,
author = {Exposure, Conclusions and Ed, Draft},
doi = {10.1080/03019233.2016.1218198},
isbn = {9781907026249},
issn = {1938-7228},
number = {September},
pmid = {1714571},
title = {{Financial Instruments : Classifi cation and Measurement}},
year = {2009}
}
@inproceedings{Collobert2008,
address = {New York, New York, USA},
annote = {Multitask for NLP},
author = {Collobert, Ronan and Weston, Jason},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390177},
file = {:Users/felix/Downloads/p160-collobert.pdf:pdf},
isbn = {9781605582054},
pages = {160--167},
publisher = {ACM Press},
title = {{A unified architecture for natural language processing: Deep Neural Networks with Multitask Learning}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390177},
year = {2008}
}
@article{Ireland2018,
abstract = {Advanced data analytics is one of the most revolutionary technological developments in the 21st century, which enables the discovery of underlining trends via sophisticated computational methods On various e-commerce and social platforms, millions of online product reviews are published by customers, which can potentially provide designers with invaluable insights into product design. This paper presents a design framework to analyze online product reviews. The objective is to use this machine-generated data to identify a series of customer needs. The framework aims to distill large volumes of qualitative data into quantitative insights on product features, so that designers can make more informed decisions. The framework combines the elements of online product reviews, design theory and methodology, and data analytics to reveal new insights. The effectiveness of the proposal framework is validated through a case study on product reviews from the e-commerce website, Amazon. The framework demonstrates a statistical approach for analyzing online product reviews. The framework acts as an interface between quantitative outputs and the qualitative and creative process of design. Further analysis of results identifies many of incorporating logical, computational methods into the highly subjective and creative process of design.},
author = {Ireland, Robert and Liu, Ang},
doi = {10.1016/J.CIRPJ.2018.06.003},
issn = {1755-5817},
journal = {CIRP Journal of Manufacturing Science and Technology},
month = {nov},
pages = {128--144},
publisher = {Elsevier},
title = {{Application of data analytics for product design: Sentiment analysis of online product reviews}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S1755581718300336},
volume = {23},
year = {2018}
}
@article{Nguyen2016,
abstract = {We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66-0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex-999, and on distinguishing antonyms from synonyms.},
archivePrefix = {arXiv},
arxivId = {1605.07766},
author = {Nguyen, Kim Anh and im Walde, Sabine Schulte and Vu, Ngoc Thang},
doi = {10.18653/v1/P16-2074},
eprint = {1605.07766},
file = {::},
isbn = {9781510827592},
number = {2014},
title = {{Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction}},
url = {http://arxiv.org/abs/1605.07766},
year = {2016}
}
@techreport{Li2017,
abstract = {In this paper, drawing intuition from the Turing test, we propose using ad-versarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a genera-tive model to produce response sequences, and a discriminator-analagous to the human evaluator in the Turing test-to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.},
archivePrefix = {arXiv},
arxivId = {1701.06547v5},
author = {Li, Jiwei and Monroe, Will and Shi, Tianlin and Jean, S{\'{e}}bastien and Ritter, Alan and Jurafsky, Dan},
eprint = {1701.06547v5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2017 - Adversarial Learning for Neural Dialogue Generation.pdf:pdf},
title = {{Adversarial Learning for Neural Dialogue Generation}},
url = {https://arxiv.org/pdf/1701.06547.pdf},
year = {2017}
}
@inproceedings{Hinton1986,
author = {Hinton, Geoffrey E.},
booktitle = {Proceedings of the Eighth Annual Conference of the Cognitive Science Society},
file = {:Users/felix/Downloads/families.pdf:pdf},
pages = {1--12},
title = {{Learning Distributed Representations of Concepts}},
year = {1986}
}
@inproceedings{Pauca2004,
abstract = {This study involves a methodology for the automatic identification of semantic features and document clusters in a heterogeneous text collection. The methodology is based upon encoding the data using low rank non-negative matrix factorization algorithms to preserve natural data non-negativity and thus avoid subtractive basis vector and encoding interactions present in techniques such as principal component analysis. Some existing non-negative matrix factorization techniques are reviewed and some new ones are proposed. Numerical experiments are reported on the use of a hybrid NMF algorithm to produce a parts-based approximation of a sparse term-by-document matrix. The resulting basis vectors and matrix projection can be used to identify underlying semantic features (topics) and document clusters of the corresponding text collection.},
author = {Pauca, V.P. and Shahnaz, F. and Berry, M.W. and Plemmons, R.J.},
booktitle = {Proceedings SIAM International Conference on Data Mining (SDM)},
pages = {452--456},
title = {{Text mining using non-negative matrix factorizations}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=gcJVK9a9RR0C{\&}oi=fnd{\&}pg=PA452{\&}dq=Text+Mining+using+Non-Negative+Matrix+Factorization{\&}ots=mOnb2Xzn3r{\&}sig=WUKWJei57sxh2QUqkGDfeRB3iqI},
year = {2004}
}
@misc{AGOF2018,
author = {AGOF},
file = {::},
title = {{Nettoreichweite der Top 15 Nachrichtenseiten (ab 10 Jahre) nach Unique Usern im Februar 2018 (in Millionen)}},
url = {http://de.statista.com/statistik/daten/studie/165258/umfrage/reichweite-der-meistbesuchten-nachrichtenwebsites/},
urldate = {2018-03-25},
year = {2018}
}
@article{Hoyer2004,
abstract = {Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of `sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.},
archivePrefix = {arXiv},
arxivId = {cs/0408058},
author = {Hoyer, Patrik O.},
doi = {10.1109/ICMLC.2011.6016966},
eprint = {0408058},
file = {::},
isbn = {0780395174},
issn = {1532-4435},
keywords = {data-adaptive representations,non-negative matrix factorization,sparseness},
pages = {1457--1469},
pmid = {1000253614},
primaryClass = {cs},
title = {{Non-negative matrix factorization with sparseness constraints}},
url = {http://arxiv.org/abs/cs/0408058},
volume = {5},
year = {2004}
}
@techreport{Plank,
abstract = {Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence mod-eling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed.},
annote = {Predicting POS-tags along with word frequencies},
archivePrefix = {arXiv},
arxivId = {1604.05529v3},
author = {Plank, Barbara and S{\o}gaard, Anders and Goldberg, Yoav},
eprint = {1604.05529v3},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Plank, S{\o}gaard, Goldberg - Unknown - Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary.pdf:pdf},
title = {{Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss}},
url = {https://github.com/clab/cnn}
}
@article{Hsiao2015,
abstract = {In this paper, we propose a novel Topic Facet Model (TFM), a probabilistic topic model that assumes all words in single sentence are generated from one topic facet. The model is applied to automatically extract forum posts semantics for uncovering the content latent structures. We further prototype a visual analytics interface to present online discussion forum semantics. We hypothesize that the semantic modeling through analytics on open online discussion forums can help users examine the post content by viewing the summarized topic facets. Our preliminary results demonstrated that TFM can be a promising method to extract topic specificity from conversational and relatively short texts in online programming discussion forums.},
author = {Hsiao, I-Han and Awasthi, Piyush},
doi = {10.1145/2723576.2723613},
isbn = {9781450334174},
journal = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge - LAK '15},
keywords = {LDA,SLDA,TFM,automated assessment,discourse analytics,discussion forums,learning analytics,programming,topic modeling},
pages = {231--235},
title = {{Topic facet modeling: semantic visual analytics for online discussion forums}},
url = {http://dl.acm.org/citation.cfm?id=2723576.2723613},
year = {2015}
}
@book{2018optimization,
publisher = {Oakland University},
title = {{Optimization of Word Embeddings in Text Categorization}},
url = {https://books.google.de/books?id=1ApEuwEACAAJ},
year = {2018}
}
@article{Mei2007,
abstract = {Multinomial distributions over words are frequently used to model topics in text collections. A common, major chal-lenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automat-ically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information be-tween a label and a topic model. Experiments with user study have been done on two text data sets with different genres. The results show that the proposed labeling meth-ods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.},
author = {Mei, Qiaozhu and Shen, Xuehua and Zhai, ChengXiang},
doi = {10.1145/1281192.1281246},
isbn = {9781595936097},
issn = {9781595936097},
journal = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining  - KDD '07},
keywords = {1 is a topic,col-,database literature,lection of abstracts of,left side of table,model extracted from a,multinomial distribu-,statistical topic models,this model gives,tion,topic model labeling},
number = {January 2007},
pages = {490},
title = {{Automatic labeling of multinomial topic models}},
url = {http://portal.acm.org/citation.cfm?doid=1281192.1281246},
year = {2007}
}
@article{Byrt1993,
abstract = {Since the introduction of Cohen's kappa as a chance-adjusted measure of agreement between two observers, several “paradoxes” in its interpretation have been pointed out. The difficulties occur because kappa not only measures agreement but is also affected in complex ways by the presence of bias between observers and by the distributions of data across the categories that are used (“prevalence”). In this paper, new indices that provide independent measures of bias and prevalence, as well as of observed agreement, are defined and a simple formula is derived that expresses kappa in terms of these three indices. When comparisons are made between agreement studies it can be misleading to report kappa values alone, and it is recommended that researchers also include quantitative indicators of bias and prevalence.},
author = {Byrt, Ted and Bishop, Janet and Carlin, John B.},
doi = {10.1016/0895-4356(93)90018-V},
issn = {0895-4356},
journal = {Journal of Clinical Epidemiology},
month = {may},
number = {5},
pages = {423--429},
publisher = {Pergamon},
title = {{Bias, prevalence and kappa}},
url = {https://www.sciencedirect.com/science/article/pii/089543569390018V},
volume = {46},
year = {1993}
}
@article{Allahyari2015,
abstract = {—Topic models, which frequently represent topics as multinomial distributions over words, have been extensively used for discovering latent topics in text corpora. Topic labeling, which aims to assign meaningful labels for discovered topics, has recently gained significant attention. In this paper, we argue that the quality of topic labeling can be improved by considering ontology concepts rather than words alone, in contrast to previous works in this area, which usually represent topics via groups of words selected from topics. We have created: (1) a topic model that integrates ontological concepts with topic models in a single framework, where each topic and each concept are represented as a multinomial distribution over concepts and over words, respectively, and (2) a topic labeling method based on the ontological meaning of the concepts included in the discovered topics. In selecting the best topic labels, we rely on the semantic relatedness of the concepts and their ontological classifications. The results of our experiments conducted on two different data sets show that introducing concepts as additional, richer features between topics and words and describing topics in terms of concepts offers an effective method for generating meaningful labels for the discovered topics.},
author = {Allahyari, Mehdi and Kochut, Krys},
file = {::},
keywords = {DBpedia ontology,topic model labeling,topic modeling,—Statistical learning},
title = {{Automatic Topic Labeling using Ontology-based Topic Models}},
year = {2015}
}
@article{Ma2018,
abstract = {{\textcopyright} 2018 Springer Science+Business Media, LLC, part of Springer Nature Sentiment analysis has emerged as one of the most popular natural language processing (NLP) tasks in recent years. A classic setting of the task mainly involves classifying the overall sentiment polarity of the inputs. However, it is based on the assumption that the sentiment expressed in a sentence is unified and consistent, which does not hold in the reality. As a fine-grained alternative of the task, analyzing the sentiment towards a specific target and aspect has drawn much attention from the community for its more practical assumption that sentiment is dependent on a particular set of aspects and entities. Recently, deep neural models have achieved great successes on sentiment analysis. As a functional simulation of the behavior of human brains and one of the most successful deep neural models for sequential data, long short-term memory (LSTM) networks are excellent in learning implicit knowledge from data. However, it is impossible for LSTM to acquire explicit knowledge such as commonsense facts from the training data for accomplishing their specific tasks. On the other hand, emerging knowledge bases have brought a variety of knowledge resources to our attention, and it has been acknowledged that incorporating the background knowledge is an important add-on for many NLP tasks. In this paper, we propose a knowledge-rich solution to targeted aspect-based sentiment analysis with a specific focus on leveraging commonsense knowledge in the deep neural sequential model. To explicitly model the inference of the dependent sentiment, we augment the LSTM with a stacked attention mechanism consisting of attention models for the target level and sentence level, respectively. In order to explicitly integrate the explicit knowledge with implicit knowledge, we propose an extension of LSTM, termed Sentic LSTM. The extended LSTM cell includes a separate output gate that interpolates the token-level memory and the concept-level input. In addition, we propose an extension of Sentic LSTM by creating a hybrid of the LSTM and a recurrent additive network that simulates sentic patterns. In this paper, we are mainly concerned with a joint task combining the target-dependent aspect detection and targeted aspect-based polarity classification. The performance of proposed methods on this joint task is evaluated on two benchmark datasets. The experiment shows that the combination of proposed attention architecture and knowledge-embedded LSTM could outperform state-of-the-art methods in two targeted aspect sentiment tasks. We present a knowledge-rich solution for the task of targeted aspect-based sentiment analysis. Our model can effectively incorporate the commonsense knowledge into the deep neural network and be trained in an end-to-end manner. We show that the two-step attentive neural architecture as well as the proposed Sentic LSTM and H-Sentic-LSTM can achieve an improved performance on resolving the aspect categories and sentiment polarity for a targeted entity in its context over state-of-the-art systems.},
author = {Ma, Yukun and Peng, Haiyun and Khan, Tahir and Cambria, Erik and Hussain, Amir},
doi = {10.1007/s12559-018-9549-x},
file = {:Users/felix/Downloads/Ma2018{\_}Article{\_}SenticLSTMAHybridNetworkForTar.pdf:pdf},
issn = {18669964},
journal = {Cognitive Computation},
keywords = {Attention model,Commonsense knowledge,Neural network,Sentiment analysis},
number = {4},
pages = {639--650},
publisher = {Cognitive Computation},
title = {{Sentic LSTM: a Hybrid Network for Targeted Aspect-Based Sentiment Analysis}},
volume = {10},
year = {2018}
}
@article{Kim2002,
abstract = {The majority of machine learning research has been fo- cused on building models and inference techniques with sound mathematical properties and cutting edge perfor- mance. Little attention has been devoted to the develop- ment of data representation that can be used to improve a user's ability to interpret the data and machine learn- ing models to solve real-world problems. In this paper, we quantitatively and qualitatively evaluate an efficient, accurate and scalable feature-compression method us- ing latent Dirichlet allocation for discrete data. This representation can effectively communicate the charac- teristics of high-dimensional, complex data points. We show that the improvement of a user's interpretability through the use of a topic modeling-based compres- sion technique is statistically significant, according to a number of metrics, when compared with other repre- sentations. Also, we find that this representation is scal- able — it maintains alignment with human classifica- tion accuracy as an increasing number of data points are shown. In addition, the learned topic layer can semanti- cally deliver meaningful information to users that could potentially aid human reasoning about data characteris- tics in connection with compressed topic space.},
author = {Kim, Been and Patel, Kayur and Rostamizadeh, Afshin and Shah, Julie},
file = {::},
isbn = {9781577357018},
journal = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1034},
title = {{Scalable and interpretable data representation for high-dimensional, complex data}},
url = {https://dl.acm.org/citation.cfm?id=2886565},
year = {2002}
}
@inproceedings{Schmitt2018,
abstract = {In this work, we propose a new model for aspect-based sentiment analysis. In contrast to previous approaches, we jointly model the detection of aspects and the classification of their polarity in an end-to-end trainable neural network. We conduct experiments with different neural architectures and word representations on the recent GermEval 2017 dataset. We were able to show considerable performance gains by using the joint modeling approach in all settings compared to pipeline approaches. The combination of a convolutional neural network and fasttext embeddings outperformed the best submission of the shared task in 2017, establishing a new state of the art.},
address = {Brussels, Belgium},
archivePrefix = {arXiv},
arxivId = {1808.09238},
author = {Schmitt, Martin and Steinheber, Simon and Schreiber, Konrad and Roth, Benjamin},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
eprint = {1808.09238},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Schmitt et al. - 2018 - Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks.pdf:pdf},
month = {aug},
pages = {1109--1114},
publisher = {Association for Computational Linguistics},
title = {{Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks}},
url = {http://arxiv.org/abs/1808.09238},
year = {2018}
}
@article{Chuang2012,
abstract = {Topic models aid analysis of text corpora by identifying la- tent topics based on co-occurring words. Real-world de- ployments of topic models, however, often require intensive expert verification and model refinement. In this paper we present Termite, a visual analysis tool for assessing topic model quality. Termite uses a tabular layout to promote comparison of terms both within and across latent topics. We contribute a novel saliency measure for selecting relevant terms and a seriation algorithm that both reveals clustering structure and promotes the legibility of related terms. In a series of examples, we demonstrate how Termite allows analysts to identify coherent and significant themes.},
author = {Chuang, Jason and Manning, Christopher D. and Heer, Jeffrey},
doi = {10.1145/2254556.2254572},
file = {::},
isbn = {9781450312875},
journal = {Proceedings of the International Working Conference on Advanced Visual Interfaces - AVI '12},
keywords = {seriation,text visualization,topic models},
pages = {74},
title = {{Termite : Visualization Techniques for Assessing Textual Topic Models}},
url = {http://dl.acm.org/citation.cfm?doid=2254556.2254572},
year = {2012}
}
@inproceedings{Lakkaraju2014,
abstract = {This paper focuses on the problem of aspect-specific sentiment analysis. The goal here is to not only extract aspects of a product or service, but also to identify specific sentiments being expressed about them. Most existing algorithms address this problem by treating aspect extraction and sentiment analysis as separate phases or by enforcing explicit modeling assumptions on how these two phases should overlap and interact. In this paper, we propose a novel approach based on a hierarchical deep learning framework which overcomes the aforementioned drawbacks. We experiment with various models of semantic compositionality within this framework. Experimental results on real world datasets show that the proposed framework outperforms other state-of-the-art techniques. In addition, we also demonstrate how domain adaptation using word vectors can benefit the task of aspect specific sentiment analyis.},
address = {Montreal},
author = {Lakkaraju, Himabindu and Socher, Richard and Manning, Chris},
booktitle = {NIPS Workshop on deep learning and representation learning},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Lakkaraju, Metamind, Manning - Unknown - Aspect Specific Sentiment Analysis using Hierarchical Deep Learning.pdf:pdf},
title = {{Aspect Specific Sentiment Analysis using Hierarchical Deep Learning}},
url = {https://pdfs.semanticscholar.org/4500/68221da8297ac0a0e1524b1e196900c61b2e.pdf},
year = {2014}
}
@inproceedings{Sivic2008,
abstract = {Objects in the world can be arranged into a hierarchy based on their semantic meaning (e.g. organism - animal - feline - cat). What about defining a hierarchy based on the visual appearance of objects? This paper investigates ways to automatically discover a hierarchical structure for the visual world from a collection of unlabeled images. Previous approaches for unsupervised object and scene discovery focused on partitioning the visual data into a set of non-overlapping classes of equal granularity. In this work, we propose to group visual objects using a multi-layer hierarchy tree that is based on common visual elements. This is achieved by adapting to the visual domain the generative hierarchical latent Dirichlet allocation (hLDA) model previously used for unsupervised discovery of topic hierarchies in text. Images are modeled using quantized local image regions as analogues to words in text. Employing the multiple segmentation framework of Russell et al. [22], we show that meaningful object hierarchies, together with object segmentations, can be automatically learned from unlabeled and unsegmented image collections without supervision. We demonstrate improved object classification and localization performance using hLDA over the previous non-hierarchical method on the MSRC dataset [33].},
author = {Sivic, Josef and Russell, Bryan C. and Zisserman, Andrew and Freeman, William T. and Efros, Alexei A.},
booktitle = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
doi = {10.1109/CVPR.2008.4587622},
isbn = {9781424422432},
issn = {1063-6919},
title = {{Unsupervised discovery of visual object class hierarchies}},
year = {2008}
}
@article{Salton1975a,
abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
author = {Salton, G. and Wong, A. and Yang, C. S.},
doi = {10.1145/361219.361220},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {613--620},
pmid = {15142973},
title = {{A vector space model for automatic indexing}},
url = {http://portal.acm.org/citation.cfm?doid=361219.361220},
volume = {18},
year = {1975}
}
@article{Conneau2017b,
abstract = {State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.},
archivePrefix = {arXiv},
arxivId = {1710.04087},
author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'{e}}gou, Herv{\'{e}}},
doi = {http://dx.doi.org/10.1111/j.1540-4560.2007.00543.x},
eprint = {1710.04087},
file = {::},
issn = {0022-4537 1540-4560},
pages = {1--14},
title = {{Word Translation Without Parallel Data}},
url = {http://arxiv.org/abs/1710.04087},
year = {2017}
}
@article{He2018,
abstract = {Aspect-level sentiment classification aims to determine the sentiment polarity of a review sentence towards an opinion target. A sentence could contain multiple sentiment-target pairs; thus the main challenge of this task is to separate different opinion contexts for different targets. To this end, attention mechanism has played an important role in previous state-of-the-art neural models. The mechanism is able to capture the importance of each context word towards a target by modeling their semantic associations. We build upon this line of research and propose two novel approaches for improving the effectiveness of attention. First, we propose a method for target representation that better captures the semantic meaning of the opinion target. Second, we introduce an attention model that incorporates syntactic information into the attention mechanism. We experiment on attention-based LSTM (Long Short-Term Memory) models using the datasets from SemEval 2014, 2015, and 2016. The experimental results show that the conventional attention-based LSTM can be substantially improved by incorporating the two approaches.},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2018 - Effective Attention Modeling for Aspect-Level Sentiment Classification.pdf:pdf},
pages = {1121--1131},
title = {{Effective Attention Modeling for Aspect-Level Sentiment Classification}},
url = {http://aclweb.org/anthology/C18-1096},
year = {2018}
}
@inproceedings{Gaussier2005,
abstract = {Non-negative Matrix Factorization (NMF, [5]) and Probabilistic Latent Semantic Analysis (PLSA, [4]) have been successfully applied to a number of text analysis tasks such as document clustering. Despite their different inspirations, both methods are instances of multinomial PCA [1]. We further explore this relationship and first show that PLSA solves the problem of NMF with KL divergence, and then explore the implications of this relationship.},
author = {Gaussier, Eric and Goutte, Cyril},
booktitle = {Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval - SIGIR '05},
doi = {10.1145/1076034.1076148},
isbn = {1595930345},
issn = {1595930345},
pages = {601},
title = {{Relation between PLSA and NMF and implications}},
url = {http://dl.acm.org/citation.cfm?doid=1076034.1076148},
year = {2005}
}
@article{Tirunillai2014a,
abstract = {Online chatter, or user-generated content, constitutes an excellent emerging source for marketers to mine meaning at a high temporal frequency. This article posits that this meaning consists of extracting the key latent dimensions of consumer satisfaction with quality and ascertaining the valence, labels, validity, importance, dynamics, and heterogeneity of those dimensions. The authors propose a unified framework for this purpose using unsupervised latent Dirichlet allocation. The sample of user-generated content consists of rich data on product reviews across 15 firms in five markets over four years. The results suggest that a few dimensions with good face validity and external validity are enough to capture quality. Dynamic analysis enables marketers to track dimensions' importance over time and allows for dynamic mapping of competitive brand positions on those dimensions over time. For vertically differentiated markets (e.g., mobile phones, computers), objective dimensions dominate and are similar across markets, heterogeneity is low across dimensions, and stability is high over time. For horizontally differentiated markets (e.g., shoes, toys), subjective dimensions dominate but vary across markets, heterogeneity is high across dimensions, and stability is low over time.},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {Tirunillai, Seshadri and Tellis, Gerard J.},
doi = {10.1509/jmr.12.0106},
eprint = {0803.1716},
isbn = {00222437},
issn = {0022-2437},
journal = {Journal of Marketing Research},
keywords = {big data,brand mapping,consumer satisfaction,dimensions,latent Dirichlet allocation,quality,user-generated content},
month = {aug},
number = {4},
pages = {463--479},
pmid = {1552442562},
publisher = { American Marketing Association },
title = {{Mining Marketing Meaning from Online Chatter: Strategic Brand Analysis of Big Data Using Latent Dirichlet Allocation}},
url = {http://journals.ama.org/doi/abs/10.1509/jmr.12.0106},
volume = {51},
year = {2014}
}
@article{Fei-Fei2005,
abstract = {We propose a novel approach to learn and recognize nat- ural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We repre- sent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previ- ous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes with- out supervision. We report satisfactory categorization per- formances on a large set of 13 categories of complex scenes.},
author = {Fei-Fei, Li and Perona, Pietro},
doi = {10.1109/CVPR.2005.16},
isbn = {0-7695-2372-2},
issn = {10636919},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
keywords = {ayesian hierarchical model for,learning natural scene categories},
pages = {524--531},
title = {{A Bayesian Hierarchical Model for Learning Natural Scene Categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467486},
volume = {2},
year = {2005}
}
@article{Blei2012a,
abstract = {Topic modeling provides a suite of algorithms to discover hidden thematic structure in large collections of texts. The results of topic modeling algorithms can be used to summarize, visualize, explore, and theorize about a corpus. A topic model takes a collection of texts as input. It discovers a set of “topics” — recurring themes that are discussed in the collection — and the degree to which each document exhibits those topics. Figure 1 illustrates topics found by running a topic model on 1.8 million articles from the New York Times. The model gives us a framework in which to explore and analyze the texts, but we did not need to decide on the topics in advance or painstakingly code each document according to them. The model algorithmically finds a way of representing documents that is useful for navigating and understanding the collection. In this essay I will discuss topic models and how they relate to digital humanities. I will describe latent Dirichlet allocation, the simplest topic model. I will explain what a “topic” is from the mathematical perspective and why algorithms can discover topics from collections of texts.[1] I will then discuss the broader field of probabilistic modeling, which gives a flexible language for expressing assumptions about data and a set of algorithms for computing under those assumptions. With probabilistic modeling for the humanities, the scholar can build a statistical lens that encodes her specific knowledge, theories, and assumptions about texts. She can then use that lens to examine and explore large archives of real sources.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Blei, David M.},
doi = {10.1613/jair.301},
eprint = {9605103},
file = {::},
isbn = {0-7803-3213-X},
issn = {10769757},
journal = {Journal of Digital Humanities},
number = {1},
pages = {8--11},
primaryClass = {cs},
title = {{Topic modeling and digital humanities}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Topic+Modeling+and+Digital+Humanities{\#}1},
volume = {2},
year = {2012}
}
@incollection{Zhang2017,
abstract = {With the rapid growth of social media, sentiment analysis, also called opinion mining, has become one of the most active research areas in natural language processing. Its application is also widespread, from business services to political campaigns. This article gives an introduction to this important area and presents some recent developments.},
address = {Boston, MA},
author = {Zhang, Lei and Liu, Bing},
booktitle = {Encyclopedia of Machine Learning and Data Mining},
doi = {10.1007/978-1-4899-7687-1_907},
editor = {Sammut, Claude and Webb, Geoffrey I},
isbn = {978-1-4899-7687-1},
pages = {1152--1161},
publisher = {Springer US},
title = {{Sentiment Analysis and Opinion Mining}},
url = {https://doi.org/10.1007/978-1-4899-7687-1{\_}907},
year = {2017}
}
@article{JyotiBora2014,
abstract = {K-means algorithm is a very popular clustering algorithm which is famous for its simplicity. Distance measure plays a very important rule on the performance of this algorithm. We have different distance measure techniques available. But choosing a proper technique for distance calculation is totally dependent on the type of the data that we are going to cluster. In this paper an experimental study is done in Matlab to cluster the iris and wine data sets with different distance measures and thereby observing the variation of the performances shown.},
author = {{Jyoti Bora}, Dibya and {Kumar Gupta}, Anil},
file = {::},
journal = {International Journal of Computer Science and Information Technologies},
keywords = {-Clustering,Iris,K Means,Matlab,Wine},
number = {2},
pages = {2501--2506},
title = {{Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab}},
url = {https://arxiv.org/ftp/arxiv/papers/1405/1405.7471.pdf},
volume = {5},
year = {2014}
}
@inproceedings{Li2010,
abstract = {A semantically meaningful image hierarchy can ease the human effort in organizing thousands and millions of pictures (e.g., personal albums), and help to improve performance of end tasks such as image annotation and classification. Previous work has focused on using either low-level image features or textual tags to build image hierarchies, resulting in limited success in their general usage. In this paper, we propose a method to automatically discover the {\&}{\#}x201C;semantivisual{\&}{\#}x201D; image hierarchy by incorporating both image and tag information. This hierarchy encodes a general-to-specific image relationship. We pay particular attention to quantifying the effectiveness of the learned hierarchy, as well as comparing our method with others in the end-task applications. Our experiments show that humans find our semantivisual image hierarchy more effective than those solely based on texts or low-level visual features. And using the constructed image hierarchy as a knowledge ontology, our algorithm can perform challenging image classification and annotation tasks more accurately.},
author = {Li, Li Jia and Wang, Chong and Lim, Yongwhan and Blei, David M. and Fei-Fei, Li},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540027},
isbn = {9781424469840},
issn = {10636919},
pages = {3336--3343},
title = {{Building and using a Semantivisual image hierarchy}},
year = {2010}
}
@article{Howard2018,
abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
archivePrefix = {arXiv},
arxivId = {1801.06146},
author = {Howard, Jeremy and Ruder, Sebastian},
doi = {arXiv:1801.06146v3},
eprint = {1801.06146},
file = {::},
keywords = {transfer learning},
mendeley-tags = {transfer learning},
title = {{Universal Language Model Fine-tuning for Text Classification}},
url = {http://arxiv.org/abs/1801.06146},
year = {2018}
}
@article{Xu2019,
abstract = {Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions.{\~{}}We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective. The datasets and code are available at https://www.cs.uic.edu/{\~{}}hxu/.},
archivePrefix = {arXiv},
arxivId = {1904.02232},
author = {Xu, Hu and Liu, Bing and Shu, Lei and Yu, Philip S},
eprint = {1904.02232},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - Unknown - BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis.pdf:pdf},
title = {{BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis}},
url = {https://www. http://arxiv.org/abs/1904.02232},
year = {2019}
}
@inproceedings{McAuley2013,
abstract = {In order to recommend products to users we must ultimately predict how a user will respond to a new product. To do so we must uncover the implicit tastes of each user as well as the properties of each product. For example, in order to predict whether a user will enjoy Harry Potter, it helps to identify that the book is about wiz- ards, as well as the user's level of interest in wizardry. User feedback is required to discover these latent product and user dimensions. Such feedback often comes in the form of a numeric rating accompanied by review text. However, traditional methods often discard review text, which makes user and product latent dimensions difficult to interpret, since they ignore the very text that justifies a user's rating. In this paper, we aim to combine latent rating dimensions (such as those of latent-factor recommender systems) with latent review topics (such as those learned by topic models like LDA). Our approach has several advantages. Firstly, we ob- tain highly interpretable textual labels for latent rating dimensions, which helps us to ‘justify' ratings with text. Secondly, our approach more accurately predicts product ratings by harnessing the informa- tion present in review text; this is especially true for new products and users, who may have too few ratings to model their latent fac- tors, yet may still provide substantial information from the text of even a single review. Thirdly, our discovered topics can be used to facilitate other tasks such as automated genre discovery, and to identify useful and representative reviews.},
address = {Hong Kong, China},
author = {McAuley, Julian and Leskovec, Jure},
booktitle = {RecSys '13 Proceedings of the 7th ACM conference on Recommender systems},
doi = {10.1145/2507157.2507163},
file = {::},
isbn = {9781450324090},
keywords = {recommender systems,topic models},
pages = {165--172},
publisher = {ACM},
title = {{Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text}},
url = {http://dx.doi.org/10.1145/2507157.2507163.},
year = {2013}
}
@inproceedings{Turney2007,
abstract = {This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., "subtle nuances") and a negative semantic orientation when it has bad associations (e.g., "very cavalier"). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word "excellent" minus the mutual information between the given phrase and the word "poor". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74{\%} when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84{\%} for automobile reviews to 66{\%} for movie reviews.},
address = {Philadelphia, USA},
archivePrefix = {arXiv},
arxivId = {cs/0212032},
author = {Turney, Peter D.},
booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)},
doi = {10.3115/1073083.1073153},
eprint = {0212032},
file = {:Users/felix/Downloads/P02-1053.pdf:pdf},
issn = {0738467X},
number = {July},
pages = {417--424},
pmid = {16715730},
primaryClass = {cs},
title = {{Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews}},
url = {http://link.springer.com/10.1007/978-3-642-19460-3},
year = {2007}
}
@inproceedings{Yano2009,
abstract = {In this paper we model discussions in online political weblogs (blogs). To do this, we extend Latent Dirichlet Allocation, introduced by Blei et al. (2003), in various ways to capture different characteristics of the data. Our models jointly describe the generation of the primary documents (“posts”) as well as the authorship and, optionally, the contents of the blog community's verbal reactions to each post (“comments”). We evaluate our model on a novel “comment prediction” task where the models are used to predict comment activity on a given post. We also provide a qualitative discussion about what the models discover.},
author = {Yano, Tae and Cohen, William W. and Smith, Noah A.},
booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on - NAACL '09},
doi = {10.3115/1620754.1620824},
isbn = {9781932432411},
issn = {1351-3249},
pages = {477},
title = {{Predicting response to political blog posts with topic models}},
url = {http://portal.acm.org/citation.cfm?doid=1620754.1620824},
year = {2009}
}
@article{Mikolov2013b,
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, "King-Man + Woman" results in a vector very close to "Queen." We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably , this method outperforms the best previous systems.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Yih, Wen-Tau and Zweig, Geoffrey},
doi = {10.3109/10826089109058901},
eprint = {1301.3781},
file = {::},
isbn = {9781937284473},
issn = {9781937284473},
number = {June},
pages = {9--14},
pmid = {1938007},
title = {{O'Sullivan - Classification of Lumbopelvic Pain Disorders.pdf}},
url = {http://research.microsoft.com/en-},
year = {2013}
}
@article{Bergstra2012a,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {Bergstra, James and Bengio, Yoshua},
file = {::},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
url = {http://scikit-learn.sourceforge.net. papers3://publication/uuid/1190E1AB-0319-40C5-81CD-7207784965DE},
volume = {13},
year = {2012}
}
@inproceedings{Brun2016,
abstract = {This paper presents our contribution to the SemEval 2016 task 5: Aspect-Based Sentiment Analysis. We have addressed Subtask 1 for the restaurant domain, in English and French, which implies opinion target expression detection, aspect category and polarity classiﬁcation. We describe the different components of the system, based on composite models combining sophisticated linguistic features with Machine Learning algorithms, and report the results obtained for both languages.},
address = {San Diego, USA},
author = {Brun, Caroline and Perez, Julien and Roux, Claude},
booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
doi = {10.18653/v1/s16-1044},
file = {:Users/felix/Downloads/S16-1044.pdf:pdf},
pages = {277--281},
title = {{XRCE at SemEval-2016 Task 5: Feedbacked Ensemble Modeling on Syntactico-Semantic Knowledge for Aspect Based Sentiment Analysis}},
year = {2016}
}
@article{Lau2011,
abstract = {We propose a method for automatically la- belling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia ar- ticles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates us- ing a combination of association measures and lexical features, optionally fed into a super- vised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.},
author = {Lau, Jey Han and Grieser, Karl and Newman, David and Baldwin, Timothy},
isbn = {978-1-932432-87-9},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics},
pages = {1536--1545},
title = {{Automatic Labelling of Topic Models}},
year = {2011}
}
@inproceedings{Bhatia2017,
abstract = {Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.},
archivePrefix = {arXiv},
arxivId = {1706.05140},
author = {Bhatia, Shraey and Lau, Jey Han and Baldwin, Timothy},
booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning, CoNLL},
eprint = {1706.05140},
pages = {206--215},
title = {{An Automatic Approach for Document-level Topic Model Evaluation}},
url = {http://arxiv.org/abs/1706.05140},
year = {2017}
}
@article{Tirunillai2012,
abstract = {This study examines whether user-generated content (UGC) is related to stock market performance, which metric of UGC has the strongest relationship, and what the dynamics of the relationship are. We aggregate UGC from multiple websites over a four-year period across 6 markets and 15 firms. We derive multiple metrics of UGC and use multivariate time-series models to assess the relationship between UGC and stock market performance. Volume of chatter significantly leads abnormal returns by a few days (supported by Granger causality tests). Of all the metrics of UGC, volume of chatter has the strongest positive effect on abnormal returns and trading volume. The effect of negative and positive metrics of UGC on abnormal returns is asymmetric. Whereas negative UGC has a significant negative effect on abnormal returns with a short “wear-in” and long “wear-out,” positive UGC has no significant effect on these metrics. The volume of chatter and negative chatter have a significant positive effect on trading volume....},
author = {Tirunillai, Seshadri and Tellis, Gerard J.},
doi = {10.1287/mksc.1110.0682},
issn = {0732-2399},
journal = {Marketing Science},
keywords = {computational text processing,online word of mouth,stock returns,user-generated content (UGC),vector autoregression (VAR)},
month = {mar},
number = {2},
pages = {198--215},
publisher = { INFORMS },
title = {{Does Chatter Really Matter? Dynamics of User-Generated Content and Stock Performance}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mksc.1110.0682},
volume = {31},
year = {2012}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {::},
journal = {NIPS'13 Proceedings of the 26th International Conference on Neural Information Processing Systems},
keywords = {word2vec},
mendeley-tags = {word2vec},
month = {oct},
pages = {3111--3119},
title = {{Distributed Representations ofWords and Phrases and their Compositionality}},
volume = {2},
year = {2013}
}
@article{Das2007,
abstract = {Extracting sentiment from text is a hard semantic problem. We develop a methodology for extracting small investor sentiment from stock message boards. The algorithm comprises different classifier algorithms coupled together by a voting scheme. Accuracy levels are similar to widely used Bayes classifiers, but false positives are lower and sentiment accuracy higher. Time series and cross-sectional aggregation of message information improves the quality of the resultant sentiment index, particularly in the presence of slang and ambiguity. Empirical applications evidence a relationship with stock values—tech-sector postings are related to stock index levels, and to volumes and volatility. The algorithms may be used to assess the impact on investor opinion of management announcements, press releases, third-party news, and regulatory changes.},
author = {Das, Sanjiv R. and Chen, Mike Y.},
doi = {10.1287/mnsc.1070.0704},
file = {:Users/felix/Downloads/10.1.1.202.6418-2.pdf:pdf},
issn = {0025-1909},
journal = {Management Science},
number = {9},
pages = {1375--1388},
title = {{Yahoo! for Amazon: Sentiment Extraction from Small Talk on the Web}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1070.0704},
volume = {53},
year = {2007}
}
@article{Ringeval2017,
abstract = {The Audio/Visual Emotion Challenge and Workshop (AVEC 2017) "Real-life depression, and aaect" will be the seventh competition event aimed at comparison of multimedia processing and machine learning methods for automatic audiovisual depression and emotion analysis, with all participants competing under strictly the same conditions. .e goal of the Challenge is to provide a common benchmark test set for multimodal information processing and to bring together the depression and emotion recognition communities, as well as the audiovisual processing communities, to compare the relative merits of the various approaches to depression and emotion recognition from real-life data. .is paper presents the novelties introduced this year, the challenge guidelines, the data used, and the performance of the baseline system on the two proposed tasks: dimensional emotion recognition (time and value-continuous), and dimensional depression estimation (value-continuous).},
author = {Ringeval, Fabien and Schuller, Bj{\"{o}}rn and Valstar, Michel and Gratch, Jonathan and Cowie, Roddy and Scherer, Stefan and Mozgai, Sharon and Cummins, Nicholas and Schmii, Maximilian and Pantic, Maja},
doi = {10.1145/3133944.3133953},
file = {::},
isbn = {9781450355025},
keywords = {evaluator weighted estimator,ewe,statistics,•Computing methodologies  Biometrics,•General and reference  Performance},
mendeley-tags = {evaluator weighted estimator,ewe,statistics},
title = {{AVEC 2017-Real-life Depression, and AAect Recognition Workshop and Challenge}},
url = {http://delivery.acm.org/10.1145/3140000/3133953/p3-ringeval.pdf?ip=150.65.249.59{\&}id=3133953{\&}acc=ACTIVE SERVICE{\&}key=D2341B890AD12BFE.FC02734FB516017D.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1536202826{\_}65ce5331fb4c3a577a21529d3223b817},
year = {2017}
}
@inproceedings{Pontiki2016,
abstract = {This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams.},
address = {San Diego, USA},
author = {Pontiki, Maria and Galanis, Dimitrios and Papageorgiou, Haris and Androutsopoulos, Ion and Manandhar, Suresh and Al-Smadi, Mohammad and Al-Ayyoub, Mahmoud and Zhao, Yanyan and Qin, Bing and {De Clercq}, Orph{\'{e}}e and Hoste, V{\'{e}}ronique and Apidianaki, Marianna and Tannier, Xavier and Loukachevitch, Natalia and Kotelnikov, Evgeny and Bel, Nuria and {Mar{\'{i}}a Jim{\'{e}}nez-Zafra}, Salud and Eryiğit, G{\"{u}}lşen},
booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pontiki et al. - Unknown - SemEval-2016 Task 5 Aspect Based Sentiment Analysis.pdf:pdf},
pages = {19--30},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2016 Task 5: Aspect Based Sentiment Analysis}},
url = {http://alt.qcri.org/semeval2014/task4/},
year = {2016}
}
@article{Bhatia2016a,
abstract = {Topics generated by topic models are typically represented as list of terms. To reduce the cognitive overhead of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels.},
archivePrefix = {arXiv},
arxivId = {1612.05340},
author = {Bhatia, Shraey and Lau, Jey Han and Baldwin, Timothy},
eprint = {1612.05340},
file = {::},
number = {1},
pages = {953--963},
title = {{Automatic Labelling of Topics with Neural Embeddings}},
url = {http://arxiv.org/abs/1612.05340},
year = {2016}
}
@article{Conneau2017c,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
archivePrefix = {arXiv},
arxivId = {1705.02364},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
doi = {10.1.1.156.2685},
eprint = {1705.02364},
file = {::},
isbn = {978-1-109-24088-7},
issn = {0176-1617},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
year = {2017}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
doi = {10.1017/S0140525X16001837},
eprint = {1706.03762},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
isbn = {9781577357384},
issn = {0140-525X},
pmid = {1000303116},
title = {{Attention Is All You Need}},
year = {2017}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Berard2016,
abstract = {We present MultiVec, a new toolkit for computing continuous representations for text at different granularity levels (word-level or sequences of words). MultiVec includes word2vec's features, paragraph vector (batch and online) and bivec for bilingual distributed representations. MultiVec also includes different distance measures between words and sequences of words. The toolkit is written in C++ and is aimed at being fast (in the same order of magnitude as word2vec), easy to use, and easy to extend. It has been evaluated on several NLP tasks: the analogical reasoning task, sentiment analysis, and crosslingual document classification.},
author = {B{\'{e}}rard, Alexandre and Servan, Christophe and Pietquin, Olivier and Besacier, Laurent},
file = {::},
isbn = {9782951740891},
journal = {The 10th edition of the Language Resources and Evaluation Conference (LREC 2016)},
keywords = {bilingual word embeddings,crosslingual document classification,paragraph vector,word embeddings},
number = {1},
pages = {1--5},
title = {{MultiVec: a Multilingual and Multilevel Representation Learning Toolkit for NLP}},
volume = {1},
year = {2016}
}
@article{Lee1999,
abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Lee, Daniel D. and Seung, H. Sebastian},
doi = {10.1038/44565},
eprint = {arXiv:1408.1149},
file = {::},
isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
issn = {00280836},
journal = {Nature},
number = {6755},
pages = {788--791},
pmid = {10548103},
title = {{Learning the parts of objects by non-negative matrix factorization}},
volume = {401},
year = {1999}
}
@inproceedings{Hassan2013,
abstract = {Twitter sentiment analysis has become widely popular. However, stable Twitter sentiment classification performance remains elusive due to several issues: heavy class imbalance in a multi-class problem, representational richness issues for sentiment cues, and the use of diverse colloquial linguistic patterns. These issues are problematic since many forms of social media analytics rely on accurate underlying Twitter sentiments. Accordingly, a text analytics framework is proposed for Twitter sentiment analysis. The framework uses an elaborate bootstrapping ensemble to quell class imbalance, sparsity, and representational richness issues. Experiment results reveal that the proposed approach is more accurate and balanced in its predictions across sentiment classes, as compared to various comparison tools and algorithms. Consequently, the bootstrapping ensemble framework is able to build sentiment time series that are better able to reflect events eliciting strong positive and negative sentiments from users. Considering the importance of Twitter as one of the premiere social media platforms, the results have important implications for social media analytics and social intelligence.},
annote = {"Since SA tools need to be customized for each domain"},
author = {Hassan, Ammar and Abbasi, Ahmed and Zeng, Daniel},
booktitle = {Proceedings - SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013},
doi = {10.1109/SocialCom.2013.56},
file = {:Users/felix/Downloads/06693353.pdf:pdf},
isbn = {9780769551371},
keywords = {Machine learning,Opinion mining,Sentiment analysis,Social media analytics,Text mining},
month = {sep},
pages = {357--364},
publisher = {IEEE},
title = {{Twitter sentiment analysis: A bootstrap ensemble framework}},
url = {http://ieeexplore.ieee.org/document/6693353/},
year = {2013}
}
@techreport{Conneau2018,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an un-supervised manner on large corpora, as base features. Efforts to obtain embed-dings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsu-pervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsu-pervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features , which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available 1 .},
archivePrefix = {arXiv},
arxivId = {1705.02364v5},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and {Lo¨ıc Barrault}, Lo¨ıc and Bordes, Antoine},
eprint = {1705.02364v5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Conneau et al. - 2018 - Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.pdf:pdf},
keywords = {Infersent,LSTM,NLI,Natural language inference,facebook,sentence embeddings,sentence encoding,supervised learning},
mendeley-tags = {Infersent,LSTM,NLI,Natural language inference,facebook,sentence embeddings,sentence encoding,supervised learning},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
url = {https://www.github.com/facebookresearch/InferSent},
year = {2018}
}
@article{Berry2007,
abstract = {The development and use of low-rank approximate nonnegative matrix factorization (NMF) algorithms for feature extraction and identification in the fields of text mining and spectral data analysis are presented. The evolution and convergence properties of hybrid methods based on both sparsity and smoothness constraints for the resulting nonnegative matrix factors are discussed. The interpretability of NMF outputs in specific contexts are provided along with opportunities for future work in the modification of NMF algorithms for large-scale and time-varying data sets. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Berry, Michael W. and Browne, Murray and Langville, Amy N. and Pauca, V. Paul and Plemmons, Robert J.},
doi = {10.1016/j.csda.2006.11.006},
file = {::},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Conjugate gradient,Constrained least squares,Email surveillance,Nonnegative matrix factorization,Spectral data analysis,Text mining},
number = {1},
pages = {155--173},
pmid = {9126737},
title = {{Algorithms and applications for approximate nonnegative matrix factorization}},
volume = {52},
year = {2007}
}
@article{Blei2007,
abstract = {Correction to Annals of Applied Statistics 1 (2007) 17--35 [doi:10.1214/07-AOAS114]},
archivePrefix = {arXiv},
arxivId = {0712.1486},
author = {Blei, David M. and Lafferty, John D.},
doi = {10.1214/07-AOAS114},
eprint = {0712.1486},
isbn = {1595933832},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
number = {1},
pages = {17--35},
pmid = {9013932},
title = {{A correlated topic model of Science}},
url = {http://projecteuclid.org/euclid.aoas/1183143727},
volume = {1},
year = {2007}
}
@article{Guo2018,
abstract = {This paper presents an effective approach for parallel corpus mining using bilingual sentence embeddings. Our embedding models are trained to produce similar representations exclusively for bilingual sentence pairs that are translations of each other. This is achieved using a novel training method that introduces hard negatives consisting of sentences that are not translations but that have some degree of semantic similarity. The quality of the resulting embeddings are evaluated on parallel corpus reconstruction and by assessing machine translation systems trained on gold vs. mined sentence pairs. We find that the sentence embeddings can be used to reconstruct the United Nations Parallel Corpus at the sentence level with a precision of 48.9{\%} for en-fr and 54.9{\%} for en-es. When adapted to document level matching, we achieve a parallel document matching accuracy that is comparable to the significantly more computationally intensive approach of [Jakob 2010]. Using reconstructed parallel data, we are able to train NMT models that perform nearly as well as models trained on the original data (within 1-2 BLEU).},
archivePrefix = {arXiv},
arxivId = {1807.11906},
author = {Guo, Mandy and Shen, Qinlan and Yang, Yinfei and Ge, Heming and Cer, Daniel and Abrego, Gustavo Hernandez and Stevens, Keith and Constant, Noah and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
doi = {arXiv:1807.11906v2},
eprint = {1807.11906},
file = {::},
issn = {1938-7228},
title = {{Effective Parallel Corpus Mining using Bilingual Sentence Embeddings}},
url = {http://arxiv.org/abs/1807.11906},
year = {2018}
}
@article{Powers2010,
abstract = {Recently I have been intrigued by the reappearance of an old friend, George Kingsley Zipf, in a number of not entirely expected places. The law named for him is ubiquitous, but Zipf did not actually discover the law so much as provide a plausible explanation. Others have proposed modifications to Zipf's Law, and closer examination uncovers systematic deviations from its normative form. We demonstrate how Zipf's analysis can be extended to include some of these phenomena.},
author = {Powers, David M. W.},
doi = {10.3115/1603899.1603924},
file = {::},
pages = {151},
title = {{Applications and explanations of Zipf's law}},
year = {2010}
}
@article{Caruana1993,
author = {Caruana, Richard},
file = {:Users/felix/Downloads/10.1.1.57.3196.pdf:pdf},
journal = {PROCEEDINGS OF THE TENTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING},
pages = {41----48},
title = {{Multitask Learning: A Knowledge-Based Source of Inductive Bias}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.3196},
year = {1993}
}
@article{Dr.S.Kannan2014,
author = {{Dr. S. Kannan} and Gurusamy, Vairaprakash},
file = {::},
journal = {Rtrics},
number = {October 2014},
pages = {7--16},
title = {{Preprocessing Techniques for Text Mining}},
year = {2014}
}
@article{Kotzias2014,
abstract = {We present a new approach for transferring knowledge from groups to individuals that comprise them. We evaluate our method in text, by inferring the ratings of individual sentences using full-review ratings. This approach, which combines ideas from transfer learning, deep learning and multi-instance learning, reduces the need for laborious human labelling of fine-grained data when abundant labels are available at the group level.},
archivePrefix = {arXiv},
arxivId = {1411.3128},
author = {Kotzias, Dimitrios and Denil, Misha and Blunsom, Phil and de Freitas, Nando},
eprint = {1411.3128},
file = {::},
pages = {1--9},
title = {{Deep Multi-Instance Transfer Learning}},
url = {http://arxiv.org/abs/1411.3128},
year = {2014}
}
@article{Conneau2017,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
archivePrefix = {arXiv},
arxivId = {1705.02364},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
doi = {10.1.1.156.2685},
eprint = {1705.02364},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Conneau et al. - 2018 - Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.pdf:pdf},
isbn = {978-1-109-24088-7},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
url = {http://arxiv.org/abs/1705.02364},
year = {2017}
}
@book{wu2009top,
author = {Wu, X and Kumar, V},
isbn = {9781420089653},
publisher = {CRC Press},
series = {Chapman {\&} Hall/CRC Data Mining and Knowledge Discovery Series},
title = {{The Top Ten Algorithms in Data Mining}},
url = {https://books.google.de/books?id={\_}kcEn-c9kYAC},
year = {2009}
}
@article{Nielsen2005a,
abstract = {We present a general method for automatic meta-analyses in neuroscience and apply it on text data from published functional imaging studies to extract main functions associated with a brain area-the posterior cingulate cortex (PCC). Abstracts from PubMed are downloaded, words extracted and converted to a bag-of-words matrix representation. The combined data are analyzed with hierarchical non-negative matrix factorization. We find that the prominent themes in the PCC corpus are episodic memory retrieval and pain. We further characterize the distribution in PCC of the Talairach coordinates available in some of the articles. This shows a tendency to functional segregation between memory and pain components where memory activations are predominantly in the caudal part and pain in the rostral part of PCC. {\textcopyright} 2005 Elsevier Inc. All rights reserved.},
author = {Nielsen, Finn {\AA}rup and Balslev, Daniela and Hansen, Lars Kai},
doi = {10.1016/j.neuroimage.2005.04.034},
isbn = {1053-8119 (Print)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Alzheimer's disease,Automatic data processing,Bibliometrics,Brain mapping,Cingulate gyrus,Magnetic resonance imaging,Memory,Meta-analysis,Pain,Positron emission tomography,PubMed},
number = {3},
pages = {520--532},
pmid = {15946864},
title = {{Mining the posterior cingulate: Segregation between memory and pain components}},
volume = {27},
year = {2005}
}
@article{Marquez2018,
abstract = {Social Media is playing a key role in today's society. Many of the events that are taking place in diverse human activities could be explained by the study of these data. Big Data is a relatively new parading in Computer Science that is gaining increasing interest by the scientific community. Big Data Predictive Analytics is a Big Data discipline that is mostly used to analyze what is in the huge amounts of data and then perform predictions based on such analysis using advanced mathematics and computing techniques. The study of Social Media Data involves disciplines like Natural Language Processing, by the integration of this area to academic studies, useful findings have been achieved. Social Network Rating Systems are online platforms that allow users to know about goods and services, the way in how users review and rate their experience is a field of evolving research. This paper presents a deep investigation in the state of the art of these areas to discover and analyze the current status of the research that has been developed so far by academics of diverse background.},
author = {Marquez, J L Jimenez and Carrasco, I Gonzalez and Cuadrado, J L Lopez},
doi = {10.1109/TLA.2018.8327417},
issn = {1548-0992 VO  - 16},
journal = {IEEE Latin America Transactions},
keywords = {Big Data,Big Data analytics,Big Data predictive,Big Data predictive analytics,Computer science,IEEE transactions,Natural Language Processing,Natural language processing,Social network rating systems,Social network services,Web 2.0,analytic-predictive environments,computer science,data analysis,natural language processing,social media data,social network rating systems,social networking (online)},
number = {2},
pages = {592--597},
title = {{Challenges And Opportunities In Analytic-Predictive Environments Of Big Data And Natural Language Processing For Social Network Rating Systems}},
volume = {16},
year = {2018}
}
@article{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
archivePrefix = {arXiv},
arxivId = {1606.04671},
author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
eprint = {1606.04671},
file = {::},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@misc{tuv2016,
author = {{T{\"{U}}V S{\"{u}}d}},
file = {::},
title = {{Was verbinden Sie mit dem Begriff "Bio" auf Lebensmittelverpackungen?}},
url = {https://de.statista.com/statistik/daten/studie/12279/umfrage/einstellungen-zu-biolebensmitteln-in-deutschland/},
urldate = {2018-03-16},
year = {2016}
}
@article{ALEKSEJEVA2014,
abstract = {The aim of this investigation is to present the results obtained during the survey of Latvian consumers in order to elicit subjective and objective knowledge about genetically modified organisms and genetic modification. The main task was to develop the core questions so that to elicit the objective knowledge of Latvian consumers on genetically modified organisms. The questions were elaborated in cooperation with Latvian scientists in the field of biology and further will be incorporated in the survey on Latvian consumers' attitude regarding the use of genetically modified organisms in food production and other industries where also relationship between the level of the consumers' knowledge and attitude to acceptance of genetically modified products will be evaluated. (English) [ABSTRACT FROM AUTHOR]},
author = {Aleksejeva, Inese},
issn = {13921142},
journal = {Latvijos Vartotoju {\v{Z}}inios Apie Geneti{\v{s}}kai Modifikuotus Organizmus.},
keywords = {Consumers,Food production,Latvians,Scientists,Transgenic organisms,consumer attitude,genetically modified,geneti{\v{s}}kai modifikuotas,knowledge,vartotojų po{\v{z}}iūris,{\v{z}}inios},
number = {71},
pages = {7--16},
title = {{Latvian Consumers' Knowledge about Genetically Modified Organisms.}},
url = {http://10.0.28.52/MOSR.2335.8750.2014.71.1{\%}0Ahttp://search.ebscohost.com/login.aspx?direct=true{\&}db=bth{\&}AN=99584276{\&}site=ehost-live{\&}scope=site},
volume = {1142},
year = {2014}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {::},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
number = {3/1/2003},
pages = {993----1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
url = {http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf},
volume = {3},
year = {2003}
}
@article{Blei2007a,
abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A lim- itation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet dis- tribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139–177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We ap- ply the CTM to the articles from Science published from 1990–1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections.},
archivePrefix = {arXiv},
arxivId = {0708.3601},
author = {Blei, David M. and Lafferty, John D.},
doi = {10.1214/07-AOAS114},
eprint = {0708.3601},
issn = {1932-6157},
journal = {Annals of Applied Statistics},
keywords = {Hierarchical models,approximate posterior inferen},
number = {1},
pages = {17--35},
title = {{A correlated topic model of Science}},
url = {http://projecteuclid.org/euclid.aoas/1183143727},
volume = {1},
year = {2007}
}
@article{Christensen2018,
abstract = {Online communities can be an attractive source of ideas for product and process innovations. However, innovative user‐contributed ideas may be few. From a perspective of harnessing “big data” for inbound open innovation, the detection of good ideas in online communities is a problem of detecting rare events. Recent advances in text analytics and machine learning have made it possible to screen vast amounts of online information and automatically detect user‐contributed ideas. However, it is still uncertain whether the ideas identified by such systems will also be regarded as sufficiently novel, feasible and valuable by firms who might decide to develop them further. A validation study is reported in which 200 posts from an online home brewing community were extracted by an automatic idea detection system. Two professionals from a brewing company evaluated the posts in terms of idea ,idea ,idea  and idea. The results suggest that the automatic idea detection system is sufficiently valid to be deployed for the harvesting and initial screening of ideas, and that the profile of the identified ideas (in terms of novelty, feasibility and value) follows the same pattern identified in studies of user ideation in general.},
author = {Christensen, Kasper and Scholderer, Joachim and Hersleth, Stine Alm and N{\ae}s, Tormod and Kvaal, Knut and Mollestad, Torulf and Veflen, Nina and Risvik, Einar},
doi = {10.1111/caim.12260},
issn = {14678691},
journal = {Creativity and Innovation Management},
number = {1},
pages = {23--31},
title = {{How good are ideas identified by an automatic idea detection system?}},
volume = {27},
year = {2018}
}
@article{10.1086/266577,
author = {Scott, William A.},
doi = {10.1086/266577},
issn = {0033-362X},
journal = {Public Opinion Quarterly},
number = {3},
pages = {321--325},
title = {{Reliability of Content Analysis:The Case of Nominal Scale Coding}},
url = {https://dx.doi.org/10.1086/266577},
volume = {19},
year = {1955}
}
@phdthesis{Abey2015,
author = {Abey, Rebecca Katherine},
file = {::},
school = {University of Canterbury},
title = {{THE STATISTICS OF TOPIC MODELLING}},
type = {Master's Thesis},
year = {2015}
}
@article{Zhao2010,
abstract = {Discovering and summarizing opinions from online reviews is an important and challeng- ing task. A commonly-adopted framework generates structured review summaries with aspects and opinions. Recently topic mod- els have been used to identify meaningful re- view aspects, but existing topic models do not identify aspect-specific opinion words. In this paper, we propose a MaxEnt-LDA hy- brid model to jointly discover both aspects and aspect-specific opinion words. We show that with a relatively small amount of train- ing data, our model can effectively identify as- pect and opinion words simultaneously. We also demonstrate the domain adaptability of our model.},
author = {Zhao, Wayne Xin and Jiang, Jing and Yan, Hongfei and Li, Xiaoming},
doi = {10.1145/2623330.2623758},
isbn = {1932432868},
journal = {Computational Linguistics},
number = {October},
pages = {56--65},
title = {{Jointly Modeling Aspects and Opinions with a MaxEnt-LDA Hybrid}},
url = {http://www.aclweb.org/anthology/D10-1006},
volume = {16},
year = {2010}
}
@article{Subramanian2018,
abstract = {A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.},
archivePrefix = {arXiv},
arxivId = {1804.00079},
author = {Subramanian, Sandeep and Trischler, Adam and Bengio, Yoshua and Pal, Christopher J},
doi = {10.1074/jbc.M115.641787},
eprint = {1804.00079},
file = {::},
issn = {0021-9258},
number = {2016},
pages = {1--16},
title = {{Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning}},
url = {http://arxiv.org/abs/1804.00079},
year = {2018}
}
@article{Vaswani2017a,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
doi = {10.1017/S0952523813000308},
eprint = {1706.03762},
file = {::},
isbn = {1469-8714},
issn = {1469-8714},
number = {Nips},
pmid = {24016424},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
@article{Grimmer2010,
abstract = {Political scientists lack methods to efficiently measure the priorities political actors emphasize in statements. To address this limitation, I introduce a statistical model that attends to the structure of political rhetoric when measuring expressed priorities: statements are naturally organized by author. The expressed agenda model exploits this structure to simultaneously estimate the topics in the texts, as well as the attention political actors allocate to the estimated topics. I apply the method to a collection of over 24,000 press releases from senators from 2007, which I demonstrate is an ideal medium to measure how senators explain their work in Washington to constituents. A set of examples validates the estimated priorities and demonstrates their usefulness for testing theories of how members of Congress communicate with constituents. The statistical model and its extensions will be made available in a forthcoming free software package for the R computing language.},
author = {Grimmer, Justin},
doi = {10.1093/pan/mpp034},
file = {::},
issn = {10471987},
journal = {Political Analysis},
number = {1},
pages = {1--35},
title = {{A Bayesian hierarchical topic model for political texts: Measuring expressed agendas in senate press releases}},
volume = {18},
year = {2010}
}
@inproceedings{Fang2012,
abstract = {This paper presents a novel opinion mining research prob- lem, which is called Contrastive Opinion Modeling (COM). Given any query topic and a set of text collections from multiple perspectives, the task of COM is to present the opinions of the individual perspectives on the topic, and furthermore to quantify their difference. This general prob- lem subsumes many interesting applications, including opin- ion summarization and forecasting, government intelligence and cross-cultural studies. We propose a novel unsupervised topic model for contrastive opinion modeling. It simulates the generative process of how opinion words occur in the doc- uments of different collections. The ad hoc opinion search process can be efficiently accomplished based on the learned parameters in the model. The difference of perspectives can be quantified in a principled way by the Jensen-Shannon di- vergence among the individual topic-opinion distributions. An extensive set of experiments have been conducted to evaluate the proposed model on two datasets in the politi- cal domain: 1) statement records of U.S. senators; 2) world news reports from three representative media in U.S., China and India, respectively. The experimental results with both qualitative and quantitative analysis have shown the effec- tiveness of the proposed model.},
author = {Fang, Yi and Si, Luo and Somasundaram, Naveen and Yu, Zhengtao},
booktitle = {Proceedings of the fifth ACM international conference on Web search and data mining - WSDM '12},
doi = {10.1145/2124295.2124306},
isbn = {9781450307475},
issn = {9781450307475},
pages = {63},
title = {{Mining contrastive opinions on political texts using cross-perspective topic model}},
url = {http://dl.acm.org/citation.cfm?doid=2124295.2124306},
year = {2012}
}
@article{Wu2010,
abstract = {In the form of topic discussions, users interact with each other to share knowledge and exchange information in online forums. Modeling the evolution of topic discussion reveals how information propagates on Internet and can thus help understand sociological phenomena and improve the performance of applications such as recommendation systems. In this paper, we argue that a user's participation in topic discussions is motivated by either her friends or her own preferences. Inspired by the theory of information flow, we propose dynamic topic discussion models by mining influential relationships between users and individual preferences. Reply relations of users are exploited to construct the fundamental influential social network. The property of discussed topics and time lapse factor are also considered in our modeling. Furthermore, we propose a novel measure called ParticipationRank to rank users according to how important they are in the social network and to what extent they prefer to participate in the discussion of a certain topic. The experiments show our model can simulate the evolution of topic discussions well and predict the tendency of user's participation accurately. Copyright {\textcopyright} 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
author = {Wu, Hao and Bu, Jiajun and Chen, Chun and Wang, Can and Qiu, Guang and Zhang, Lijun and Shen, Jianfeng},
isbn = {9781577354666},
journal = {Proc. of AAAI 2010},
keywords = {Special Track -- AI and the Web},
pages = {1455--1460},
title = {{Modeling dynamic multi-topic discussions in online forums}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1834/2232},
year = {2010}
}
@article{Chu2004,
abstract = {The notion of low rank approximations arises from many important applications. When the low rank data are further required to comprise nonnegative values only, the approach by nonnegative matrix factorization is particularly appealing. This paper intends to bring about three points. First, the theoretical Kuhn-Tucker optimality condition is described in explicit form. Secondly, a number of numerical techniques, old and new, are suggested for the nonnegative matrix factorization problems. Thirdly, the techniques are employed to two real-world applications to demonstrate the difficulty in interpreting the factorizations. Key},
author = {Chu, M and Diele, F and Plemmons, R and Ragni, S},
journal = {Siam Journal on Matrix Analysis},
keywords = {ellipsoid method,gradient method,kuhn-,least squares,linear model,mass balance,newton method,nonnegative matrix factorization,quadratic programming,reduced quadratic model,tucker condition},
pages = {4--8030},
title = {{Optimality, computation, and interpretations of nonnegative matrix factorizations}},
url = {http://scholar.google.com/scholar?q=intitle:OPTIMALITY,+COMPUTATION,+AND+INTERPRETATION+OF+NONNEGATIVE+MATRIX+FACTORIZATIONS+(VERSION:+October+18,+2004){\#}0},
year = {2004}
}
@inproceedings{Pang2012,
abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.1515/9783110239171.151},
file = {:Users/felix/Downloads/W02-1011.pdf:pdf},
issn = {0003-5696},
number = {July},
pages = {79--86},
publisher = {Association for Computational Linguistics},
title = {{Thumbs up? Sentiment Classification using Machine Learning Techniques}},
year = {2012}
}
@techreport{Iyyer,
author = {Iyyer, Mohit and Boyd-Graber, Jordan and Iii, Hal Daum{\'{e}}},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Iyyer, Boyd-Graber, Iii - Unknown - Generating Sentences from Semantic Vector Space Representations.pdf:pdf},
title = {{Generating Sentences from Semantic Vector Space Representations}},
url = {https://people.cs.umass.edu/{~}miyyer/pubs/2014{\_}nips{\_}generation.pdf}
}
@article{Wallach2009,
abstract = {Implementations of topic models typically use symmetric Dirichlet priors with fixed concentration parameters, with the implicit assumption that such smoothing parameters have little practical effect. In this paper, we explore several classes of structured priors for topic models. We find that an asymmetric Dirichlet prior over the documenttopic distributions has substantial advantages over a symmet- ric prior, while an asymmetric prior over the topicword distributions provides no real benefit. Approximation of this prior structure through simple, efficient hy- perparameter optimization steps is sufficient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word fre- quency distributions common in natural language. Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.},
author = {Wallach, Hanna M and Mimno, David and Mccallum, Andrew},
doi = {10.1007/s10708-008-9161-9},
file = {::},
isbn = {9781615679119},
issn = {02699370},
journal = {Advances in Neural Information Processing Systems 22},
number = {2},
pages = {1973--1981},
pmid = {2508715},
title = {{Rethinking LDA : Why Priors Matter}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.185{\&}rep=rep1{\&}type=pdf},
volume = {22},
year = {2009}
}
@article{Chang2009a,
abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
doi = {10.1.1.100.1089},
eprint = {arXiv:1011.1669v3},
isbn = {9781615679119},
issn = {1098-6596},
journal = {Advances in Neural Information Processing Systems 22},
pages = {288----296},
pmid = {25246403},
title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
url = {http://www.umiacs.umd.edu/{~}jbg/docs/nips2009-rtl.pdf},
year = {2009}
}
@article{vanDerMaaten2008,
author = {van der Maaten, Laurens and Hinton, Geoffrey},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality{\_}reduction tSNE visualization},
pages = {2579--2605},
title = {{Visualizing Data using {\{}t-SNE{\}}}},
url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
volume = {9},
year = {2008}
}
@article{Chidambaram2018,
abstract = {Neural language models have been shown to achieve an impressive level of performance on a number of language processing tasks. The majority of these models, however, are limited to producing predictions for only English texts due to limited amounts of labeled data available in other languages. One potential method for overcoming this issue is learning cross-lingual text representations that can be used to transfer the performance from training on English tasks to non-English tasks, despite little to no task-specific non-English data. In this paper, we explore a natural setup for learning cross-lingual sentence representations: the dual-encoder. We provide a comprehensive evaluation of our cross-lingual representations on a number of monolingual, cross-lingual, and zero-shot/few-shot learning tasks, and also give an analysis of different learned cross-lingual embedding spaces.},
archivePrefix = {arXiv},
arxivId = {1810.12836},
author = {Chidambaram, Muthuraman and Yang, Yinfei and Cer, Daniel and Yuan, Steve and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
doi = {arXiv:1810.12836v1},
eprint = {1810.12836},
file = {::},
pages = {1--14},
title = {{Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model}},
url = {http://arxiv.org/abs/1810.12836},
year = {2018}
}
@inproceedings{Pennington2014a,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1162},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - Glove Global Vectors for Word Representation(2).pdf:pdf},
pages = {1532--1543},
title = {{Glove: Global Vectors for Word Representation}},
url = {http://nlp. http://aclweb.org/anthology/D14-1162},
year = {2014}
}
@article{Angelidis2018,
abstract = {We present a neural framework for opinion summarization from online product reviews which is knowledge-lean and only requires light supervision (e.g., in the form of product domain labels and user-provided ratings). Our method combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews: an aspect extractor trained under a multi-task objective, and a sentiment predictor based on multiple instance learning. We introduce an opinion summarization dataset that includes a training set of product reviews from six diverse domains and human-annotated development and test sets with gold standard aspect annotations, salience labels, and opinion summaries. Automatic evaluation shows significant improvements over baselines, and a large-scale study indicates that our opinion summaries are preferred by human judges according to multiple criteria.},
archivePrefix = {arXiv},
arxivId = {1808.08858},
author = {Angelidis, Stefanos and Lapata, Mirella},
eprint = {1808.08858},
file = {::},
isbn = {1808.08858v1},
title = {{Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised}},
url = {http://arxiv.org/abs/1808.08858},
year = {2018}
}
@article{Chen2014,
abstract = {Topic modeling has been commonly used to discover topics from document collections. How-ever, unsupervised models can generate many incoherent topics. To address this problem, several knowledge-based topic models have been proposed to incorporate prior domain knowledge from the user. This work advances this research much further and shows that without any user input, we can mine the prior knowledge automatically and dynamically from topics already found from a large number of domains. This paper first proposes a novel method to mine such prior knowledge dynamically in the modeling process, and then a new topic model to use the knowledge to guide the model inference. What is also interesting is that this approach offers a novel lifelong learning algorithm for topic discovery, which exploits the big (past) data and knowledge gained from such data for subsequent modeling. Our experimental results using product reviews from 50 domains demonstrate the effectiveness of the proposed approach.},
annote = {Contains information about topic models with domain knowledge. How t[o learn this knowledge from the documents. 

Contains refernces to opinion mining using topic models:
Mei et al., 2007; Titov {\&} McDonald, 2008; Zhao et al., 2010).},
author = {Chen, Zhiyuan and Liu, Bing},
file = {::},
isbn = {9781634393973},
journal = {Icml},
pages = {703--711},
pmid = {1869980},
title = {{Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data}},
volume = {32},
year = {2014}
}
@article{Kriegel2017,
abstract = {Any paper proposing a new algorithm should come with an evaluation of efficiency and scalability (particularly when we are designing methods for “big data”). However, there are several (more or less serious) pitfalls in such evaluations. We would like to point the attention of the community to these pitfalls. We substantiate our points with extensive experiments, using clustering and outlier detection methods with and without index acceleration. We discuss what we can learn from evaluations, whether experiments are properly designed, and what kind of conclusions we should avoid. We close with some general recommendations but maintain that the design of fair and conclusive experiments will always remain a challenge for researchers and an integral part of the scientific endeavor.},
author = {Kriegel, Hans-Peter and Schubert, Erich and Zimek, Arthur},
doi = {10.1007/s10115-016-1004-2},
issn = {0219-3116},
journal = {Knowledge and Information Systems},
number = {2},
pages = {341--378},
title = {{The (black) art of runtime evaluation: Are we comparing algorithms or implementations?}},
url = {https://doi.org/10.1007/s10115-016-1004-2},
volume = {52},
year = {2017}
}
@inproceedings{Wei2006,
abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
author = {Wei, Xing and Croft, W. Bruce},
booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval  - SIGIR '06},
doi = {10.1145/1148170.1148204},
isbn = {1595933697},
issn = {00295515},
pages = {178},
title = {{LDA-based document models for ad-hoc retrieval}},
url = {http://portal.acm.org/citation.cfm?doid=1148170.1148204},
year = {2006}
}
@article{SanjeevAroraYingyuLiang2017,
abstract = {The success of neural network methods for computing word embeddings has mo- tivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embed- dings and basic linear regression. The method ofWieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsuper- vised sentence embedding is a formidable baseline: Use word embeddings com- puted using one of the popular methods on unlabeled corpus like Wikipedia, rep- resent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10{\%} to 30{\%} in textual similarity tasks, and beats sophisticated supervised methods in- cluding RNN's and LSTM's. It even improves Wieting et al.'s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsu- pervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new “smoothing” terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts. 1},
author = {{Sanjeev Arora, Yingyu Liang}, Tengyu Ma Princeton},
doi = {10.1016/B978-0-12-401688-0.00001-X},
file = {::},
isbn = {0167-2738},
issn = {0022-1899},
pages = {1--16},
title = {{A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS}},
url = {https://openreview.net/pdf?id=SyK00v5xx},
year = {2017}
}
@book{Pedregosa2011,
abstract = {Abstract Scikit - learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high- ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1201.0490v2},
author = {Pedregosa, Fabian and Varoquaux, G},
booktitle = {{\ldots} of Machine Learning {\ldots}},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1201.0490v2},
isbn = {9781783281930},
issn = {15324435},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195},
volume = {12},
year = {2011}
}
@article{Mackay1875,
abstract = {PURPOSE: We aimed to determine whether there was a difference in post-operative symptomatic control and quality of life (QoL) between patients who were obese (BMI {\textgreater}30) and non-obese (BMI {\textless}30) pre-operatively. This information may inform the decision making of Physicians and patients whether to proceed to surgery for management of symptomatic lumbar disc prolapse. METHODS: We conducted a prospective questionnaire-based study of QoL and symptom control in 120 patients with postal follow-up at 3 and 12 months after lumbar disc surgery. This study was conducted in two United Kingdom regional neurosurgical units, with ethical approval from the North of Scotland Research Ethics Service (09/S0801/7). RESULTS: 120 patients were recruited; 37 (34.5{\%}) were obese. Follow up was 71{\%} at 3 months and 57{\%} at 12 months. At recruitment, both obese and non-obese patient groups had similar functional status and pain scores. At 3 and 12 months, non-obese and obese patients reported similar and significant benefits from surgery (e.g. 12 month SF-36 80.5 vs. 68.8, respectively). In non-obese and obese patients, time to return to work was 47.5 days and 53.8 days, respectively, (p = .345). After 12 months all QoL scores were significantly improved from pre-operative levels in both groups. CONCLUSIONS: Obese patients derive significant benefit from lumbar discectomy that it is similar to the benefit experienced by non-obese patients. Obese individuals may achieve excellent results from discectomy and these patients should not be refused surgery on the basis of BMI alone.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Mackay, Charles},
doi = {10.1093/nq/s5-IV.96.346-c},
eprint = {1504.06654},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - Glove Global Vectors for Word Representation(2).pdf:pdf},
isbn = {9781937284961},
issn = {00293970},
journal = {Notes and Queries},
number = {96},
pages = {346},
pmid = {28722516},
title = {{"Glove."}},
volume = {s5-IV},
year = {1875}
}
@article{Blei2012b,
abstract = {AS OUR COLLECTIVE knowledge continues to be digitized and stored—in the form of news, blogs, Web pages, scientific articles, books, images, sound, video, and social networks—it becomes more difficult to find and discover what we are looking for. We need new computational tools to help organize, search, and understand these vast amounts of information. Right now, we work with online information using two main tools—search and links. We type keywords into a search engine and find a set of documents related to them. We look at the documents in that set, possibly navigating to other linked documents. This is a powerful way of interacting with our online archive, but something is missing. Imagine searching and exploring documents based on the themes that run through them. We might “zoom in” and “zoom out” to find specific or broader themes; we might look at how those themes changed through time or how they are connected to each other. Rather than finding documents through keyword search alone, we might first find the theme that we are interested in, and then examine the documents related to that theme.},
archivePrefix = {arXiv},
arxivId = {1003.4916},
author = {Blei, David M.},
doi = {10.1145/2133806.2133826},
eprint = {1003.4916},
file = {::},
isbn = {0805854185},
issn = {00010782},
journal = {Communications of the ACM},
number = {4},
pages = {77},
pmid = {7789277},
title = {{Probabilistic topic models}},
url = {http://dl.acm.org/citation.cfm?doid=2133806.2133826},
volume = {55},
year = {2012}
}
@article{Rokach2010,
abstract = {The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving prediction performance. Researchers from various disciplines such as statistics and AI considered the use of ensemble methodology. This paper, review existing ensemble techniques and can be served as a tutorial for practitioners who are interested in building ensemble based systems.},
author = {Rokach, Lior},
doi = {10.1007/s10462-009-9124-7},
issn = {1573-7462},
journal = {Artificial Intelligence Review},
number = {1},
pages = {1--39},
title = {{Ensemble-based classifiers}},
url = {https://doi.org/10.1007/s10462-009-9124-7},
volume = {33},
year = {2010}
}
@inproceedings{Fried2015,
abstract = {We investigate the predictive power behind the language of food on social media. We collect a corpus of over three million food-related posts from Twitter and demonstrate that many latent population characteristics can be directly predicted from this data: overweight rate, diabetes rate, political leaning, and home geographical location of authors. For all tasks, our language-based models significantly outperform the majority-class baselines. Performance is further improved with more complex natural language processing, such as topic modeling. We analyze which textual features have most predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics. Lastly, we design and implement an online system for real-time query and visualization of the dataset. Visualization tools, such as geo-referenced heatmaps, semantics-preserving wordclouds and temporal histograms, allow us to discover more complex, global patterns mirrored in the language of food.},
archivePrefix = {arXiv},
arxivId = {1409.2195},
author = {Fried, Daniel and Surdeanu, Mihai and Kobourov, Stephen and Hingle, Melanie and Bell, Dane},
booktitle = {Proceedings - 2014 IEEE International Conference on Big Data, IEEE Big Data 2014},
doi = {10.1109/BigData.2014.7004305},
eprint = {1409.2195},
file = {::},
isbn = {9781479956654},
pages = {778--783},
title = {{Analyzing the language of food on social media}},
year = {2015}
}
@article{Aghajanyan2018,
abstract = {When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in,even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics and formalizing Universal Grammar as an optimization process (Chomsky, 2014; Montague, 1970). We demonstrate the capabilities of these representations by showing that the models trained on a single language using language agnostic representations achieve very similar accuracies in other languages.},
archivePrefix = {arXiv},
arxivId = {1809.08510},
author = {Aghajanyan, Armen and Song, Xia and Tiwary, Saurabh},
eprint = {1809.08510},
pages = {1--10},
title = {{Towards Language Agnostic Universal Representations}},
url = {http://arxiv.org/abs/1809.08510},
year = {2018}
}
@inproceedings{Luong2013,
abstract = {Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologically-aware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.},
author = {Luong, Thang and Socher, Richard and Manning, Christopher},
booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
doi = {10.1007/BF02579642},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Luong, Socher, Manning - Unknown - Better Word Representations with Recursive Neural Networks for Morphology(2).pdf:pdf},
issn = {11241845},
pages = {104--113},
title = {{Better word representations with recursive neural networks for morphology}},
url = {http://nlp.stanford.edu/},
year = {2013}
}
@phdthesis{Emmery2014,
author = {Emmery, Chris},
file = {::},
school = {Tilburg University},
title = {{Topic Modelling in Online Discussions}},
year = {2014}
}
@article{Kim2002,
abstract = {The majority of machine learning research has been fo- cused on building models and inference techniques with sound mathematical properties and cutting edge perfor- mance. Little attention has been devoted to the develop- ment of data representation that can be used to improve a user's ability to interpret the data and machine learn- ing models to solve real-world problems. In this paper, we quantitatively and qualitatively evaluate an efficient, accurate and scalable feature-compression method us- ing latent Dirichlet allocation for discrete data. This representation can effectively communicate the charac- teristics of high-dimensional, complex data points. We show that the improvement of a user's interpretability through the use of a topic modeling-based compres- sion technique is statistically significant, according to a number of metrics, when compared with other repre- sentations. Also, we find that this representation is scal- able — it maintains alignment with human classifica- tion accuracy as an increasing number of data points are shown. In addition, the learned topic layer can semanti- cally deliver meaningful information to users that could potentially aid human reasoning about data characteris- tics in connection with compressed topic space.},
author = {Kim, Been and Patel, Kayur and Rostamizadeh, Afshin and Shah, Julie},
file = {::},
isbn = {9781577357018},
journal = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1034},
title = {{Scalable and interpretable data representation for high-dimensional, complex data}},
url = {https://dl.acm.org/citation.cfm?id=2886565},
year = {2002}
}
@article{Vriens2018a,
author = {Vriens, Marco and Chen, Song and Vidden, Chad},
doi = {10.1177/1470785318810106},
issn = {1470-7853},
journal = {International Journal of Market Research},
keywords = {additive trees,brand similarities,brand similarities,market structure analysis,multi,hierarchical clustering,market structure analysis,multidimensional scaling,online consumer data,perceptual maps,t-sne},
pages = {147078531881010},
title = {{Mapping brand similarities: Comparing consumer online comments versus survey data}},
url = {https://doi.org/10.1177/1470785318810106},
year = {2018}
}
@inproceedings{Ahmed2017,
abstract = {Sentiment Analysis tools, developed for analyzing social media text or product reviews, work poorly on a Software Engineering (SE) dataset. Since prior studies have found devel- opers expressing sentiments during various SE activities, there is a need for a customized sentiment analysis tool for the SE domain. On this goal, we manually labeled 2000 review comments to build a training dataset and used our dataset to evaluate seven popular sentiment analysis tools. The poor performances of the existing sentiment analysis tools motivated us to build SentiCR, a sentiment analysis tool especially designed for code review comments. We evaluated SentiCR using one hundred 10- fold cross-validations of eight supervised learning algorithms. We found a model, trained using the Gradient Boosting Tree (GBT) algorithm, providing the highest mean accuracy (83{\%}), the highest mean precision (67.8{\%}), and the highest mean recall (58.4{\%}) in identifying negative review comments. I.},
annote = {Data preprocessing and contraction expansion},
author = {Ahmed, Toufique and Bosu, Amiangshu and Iqbal, Anindya and Rahimi, Shahram},
booktitle = {ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
doi = {10.1109/ASE.2017.8115623},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Ahmed et al. - Unknown - SentiCR A Customized Sentiment Analysis Tool for Code Review Interactions.pdf:pdf},
isbn = {9781538626849},
pages = {106--111},
title = {{SentiCR: A customized sentiment analysis tool for code review interactions}},
url = {https://github.com/senticr/SentiCR/},
year = {2017}
}
@inproceedings{Erik2003,
abstract = {We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance .},
address = {Edmonton, Canada},
author = {Erik, F and Kim, Sang Tjong and {De Meulder}, Fien},
booktitle = {Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003},
doi = {10.3115/1119176.1119195},
file = {::},
pages = {142--147},
title = {{Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition}},
url = {http://lcg-www.uia.ac.be/conll2003/ner/ http://dl.acm.org/citation.cfm?id=1119176.1119195},
volume = {4},
year = {2003}
}
@book{L.Fleiss1971,
author = {{L. Fleiss}, Joseph},
booktitle = {Psychological Bulletin},
doi = {10.1037/h0031619},
month = {nov},
pages = {378},
title = {{Measuring Nominal Scale Agreement Among Many Raters}},
volume = {76},
year = {1971}
}
@techreport{Bahuleyan2018,
abstract = {Probabilistic generation of natural language sentences is an important task in NLP. Existing models such as variational autoencoders (VAE) for sequence generation are extremely difficult to train due to the issues associated with the Kullback-Leibler (KL) loss collapsing to zero. One has to implement various heuristics such as KL weight annealing and word dropout in a carefully engineered manner to successfully train a text VAE. In this paper, we propose the use of Wasserstein au-toencoders (WAE) for probabilistic natural language sentence generation. We show that sequence-to-sequence WAEs are more robust towards hyperparameters and can be trained in a straightforward manner without the need for any weight an-nealing. Empirical evidence shows that the latent space learned by WAEs exhibits properties of continuity and smoothness as in VAEs, while simultaneously achieving much higher BLEU scores for sentence reconstruction .},
archivePrefix = {arXiv},
arxivId = {1806.08462v1},
author = {Bahuleyan, Hareesh and Mou, Lili and Vamaraju, Kartik and Zhou, Hao and Vechtomova, Olga},
eprint = {1806.08462v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bahuleyan et al. - 2018 - Probabilistic Natural Language Generation with Wasserstein Autoencoders.pdf:pdf},
keywords = {Language Generation,VAR,Variational Autoencoder,WAE,Wasserstein Autoencoder},
mendeley-tags = {Language Generation,VAR,Variational Autoencoder,WAE,Wasserstein Autoencoder},
title = {{Probabilistic Natural Language Generation with Wasserstein Autoencoders}},
url = {https://arxiv.org/pdf/1806.08462.pdf},
year = {2018}
}
@inproceedings{Xue2018,
abstract = {Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.},
address = {Melbourne, Australia},
archivePrefix = {arXiv},
arxivId = {1805.07043},
author = {Xue, Wei and Li, Tao},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
eprint = {1805.07043},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Xue, Li - Unknown - Aspect Based Sentiment Analysis with Gated Convolutional Networks.pdf:pdf},
pages = {2514--2523},
publisher = {Association for Computational Linguistics},
title = {{Aspect Based Sentiment Analysis with Gated Convolutional Networks}},
url = {https://github. http://arxiv.org/abs/1805.07043},
year = {2018}
}
@inproceedings{Haghighi2009,
abstract = {We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our final model, HierSum, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, HierSum yields state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)'s state-of-the-art discriminative system. We also explore HierSum's capacity to produce multiple 'topical summaries' in order to facilitate content discovery and navigation.},
author = {Haghighi, Aria and Vanderwende, Lucy},
booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on - NAACL '09},
doi = {10.3115/1620754.1620807},
isbn = {9781932432411},
issn = {1351-3249},
pages = {362},
title = {{Exploring content models for multi-document summarization}},
url = {http://portal.acm.org/citation.cfm?doid=1620754.1620807},
year = {2009}
}
@article{Bojanowski2016,
abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character {\$}n{\$}-grams. A vector representation is associated to each character {\$}n{\$}-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
archivePrefix = {arXiv},
arxivId = {1607.04606},
author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
doi = {10.1007/BF01959819},
eprint = {1607.04606},
file = {::},
isbn = {9781577357384},
issn = {00385298},
pmid = {1000303116},
title = {{Enriching Word Vectors with Subword Information}},
url = {http://arxiv.org/abs/1607.04606},
year = {2016}
}
@article{Lample2017,
abstract = {Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.},
archivePrefix = {arXiv},
arxivId = {1711.00043},
author = {Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
doi = {10.2507/26th.daaam.proceedings.070},
eprint = {1711.00043},
file = {::},
isbn = {9781479915729},
issn = {16113349},
number = {2011},
pages = {1--14},
pmid = {1000303116},
title = {{Unsupervised Machine Translation Using Monolingual Corpora Only}},
url = {http://arxiv.org/abs/1711.00043},
year = {2017}
}
@article{Pappas2017,
abstract = {Hierarchical attention networks have recently achieved remarkable performance for document classification in a given language. However, when multilingual document collections are considered, training such models separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using multi-task learning and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer.},
archivePrefix = {arXiv},
arxivId = {1707.00896},
author = {Pappas, Nikolaos and Popescu-Belis, Andrei},
doi = {10.18653/v1/N16-1174},
eprint = {1707.00896},
file = {::},
title = {{Multilingual Hierarchical Attention Networks for Document Classification}},
url = {http://arxiv.org/abs/1707.00896},
year = {2017}
}
@article{Brokos2016,
abstract = {We propose a document retrieval method for question answering that represents documents and questions as weighted centroids of word embeddings and reranks the retrieved documents with a relaxation of Word Mover's Distance. Using biomedical questions and documents from BIOASQ, we show that our method is competitive with PUBMED. With a top-k approximation, our method is fast, and easily portable to other domains and languages.},
archivePrefix = {arXiv},
arxivId = {1608.03905},
author = {Brokos, Georgios-Ioannis and Malakasiotis, Prodromos and Androutsopoulos, Ion},
doi = {10.18653/v1/W16-2915},
eprint = {1608.03905},
isbn = {9780387496160},
title = {{Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering}},
url = {http://arxiv.org/abs/1608.03905},
year = {2016}
}
@techreport{Funk2016,
abstract = {Differing views on benefits and risks of organic foods, GMOs as Americans report higher priority for healthy eating},
author = {Funk, Cary and Kennedy, Brian},
booktitle = {Pew research and American Association for Advancement of Science},
pages = {1--99},
title = {{The new food fights: U.S. public divides over food science}},
url = {http://www.pewinternet.org/2016/12/01/the-new-food-fights/},
volume = {1},
year = {2016}
}
@inproceedings{Pan2010,
abstract = {A survey on transfer learning. IEEE Trans. Knowl. Data Eng},
address = {Piscataway, NJ, USA},
author = {Pan, Sinno Jialin and Yang, Qiang},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2009.191},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Pan, Yang - 2009 - A Survey on Transfer Learning.pdf:pdf},
issn = {10414347},
keywords = {Transfer learning,data mining.,machine learning,survey},
number = {10},
pages = {1345--1359},
publisher = {IEEE Educational Activities Department},
title = {{A survey on transfer learning}},
url = {http://socrates.acadiau.ca/courses/comp/dsilver/NIPS95},
volume = {22},
year = {2010}
}
@misc{umsatz2016,
author = {FiBL},
booktitle = {The World of Organic Agriculture 2018},
file = {::},
publisher = {Statista},
title = {{Umsatz mit Bio-Lebensmitteln weltweit in den Jahren 1999 bis 2015(in Milliarden US-Dollar)}},
url = {http://de.statista.com/statistik/daten/studie/187590/umfrage/weltweiter-umsatz-mit-bio-lebensmitteln-seit-1999/},
urldate = {2018-04-15},
year = {2018}
}
@incollection{Zhang2017,
abstract = {With the rapid growth of social media, sentiment analysis, also called opinion mining, has become one of the most active research areas in natural language processing. Its application is also widespread, from business services to political campaigns. This article gives an introduction to this important area and presents some recent developments.},
address = {Boston, MA},
author = {Zhang, Lei and Liu, Bing},
booktitle = {Encyclopedia of Machine Learning and Data Mining},
doi = {10.1007/978-1-4899-7687-1_907},
editor = {Sammut, Claude and Webb, Geoffrey I},
isbn = {978-1-4899-7687-1},
pages = {1152--1161},
publisher = {Springer US},
title = {{Sentiment Analysis and Opinion Mining}},
url = {https://doi.org/10.1007/978-1-4899-7687-1{\_}907},
year = {2017}
}
@inproceedings{Mikolov2013d,
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, "King-Man + Woman" results in a vector very close to "Queen." We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably , this method outperforms the best previous systems.},
address = {Atlanta, USA},
author = {Mikolov, Tomas and Yih, Wen-Tau and Zweig, Geoffrey},
booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov, Yih, Zweig - 2013 - Linguistic Regularities in Continuous Space Word Representations(2).pdf:pdf},
pages = {9--14},
publisher = {Association for Computational Linguistics},
title = {{Linguistic Regularities in Continuous Space Word Representations}},
url = {http://research.microsoft.com/en-},
year = {2013}
}
@article{He2017,
abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
doi = {10.18653/v1/P17-1036},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2017 - An Unsupervised Neural Attention Model for Aspect Extraction.pdf:pdf},
isbn = {9781945626753},
journal = {Proceedings of the 55th Annual Meeting of the Association for
          Computational Linguistics (Volume 1: Long Papers)},
pages = {388--397},
title = {{An Unsupervised Neural Attention Model for Aspect Extraction}},
url = {http://aclweb.org/anthology/P17-1036},
year = {2017}
}
@article{Boisen2000,
abstract = {Trained systems for NE extraction have shown significant promise because of their robustness to errorful input and rapid adaptability. However, these learning algorithms have transferred the cost of development from skilled computational linguistic expertise to data annotation, putting a new premium on effective ways to produce high-quality annotated resources at minimal cost. The paper reflects on BBNs four years of experience in the annotation of training data for Named Entity (NE) extraction systems discussing useful techniques for maximizing data quality and quantity.},
author = {Boisen, Sean and Crystal, Michael R. and Schwartz, Richard and Stone, Rebecca and Weischedel, Ralph},
file = {::},
journal = {Language Resources and Evaluation},
pages = {1211----1214},
title = {{Annotating Resources for Information Extraction}},
url = {http://gandalf.aksis.uib.no/non/lrec2000/pdf/263.pdf},
year = {2000}
}
@inproceedings{Wojatzki,
abstract = {This paper describes the GermEval 2017 shared task on Aspect-Based Sentiment Analysis that consists of four subtasks: rel-evance, document-level sentiment polarity, aspect-level polarity ad opinion target ex-traction. System performance is measured on two evaluation sets – one from the same time period as the training and development set, and a second one, which contains data from a later time period. We describe the subtasks and the data in detail and provide the shared task results. Overall, the shared task attracted over 50 system runs from 8 teams.},
author = {Wojatzki, Michael and Ruppert, Eugen and Holschneider, Sarah and Zesch, Torsten and Biemann, Chris},
booktitle = {Proceedings of the GermEval 2017 – Shared Task on Aspect-based Sentiment in Social Media Customer Feedback},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Wojatzki et al. - Unknown - GermEval 2017 Shared Task on Aspect-based Sentiment in Social Media Customer Feedback(2).pdf:pdf},
pages = {1--12},
title = {{GermEval 2017: Shared Task on Aspect-based Sentiment in Social Media Customer Feedback}},
url = {http://www.ltl.uni-due.de/http://lt.informatik.uni-hamburg.de https://www.ltl.uni-due.de/wp-content/uploads/germeval-2017.pdf},
year = {2017}
}
@article{Schwenk2017,
abstract = {In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics. We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.},
archivePrefix = {arXiv},
arxivId = {1704.04154},
author = {Schwenk, Holger and Douze, Matthijs},
doi = {10.18653/v1/W17-2619},
eprint = {1704.04154},
file = {::},
isbn = {9781450304610},
pages = {157--167},
title = {{Learning Joint Multilingual Sentence Representations with Neural Machine Translation}},
url = {http://arxiv.org/abs/1704.04154},
year = {2017}
}
@article{Meza2016,
abstract = {Although there is a growing body of research on social media, only few studies have considered organic products. Therefore, this study mapped the diffusion path of the social media resources for organic products in Mexico and South Korea through Twitter and compared the contents of tweets about organic products in terms of their semantic and hyperlink networks using webometric methods. The results indicate that for organic products, Koreans sent tweets much more frequently than Mexicans. Mexican tweets focused on basic food products in street markets, whereas Korean tweets highlighted promotions and firms, revealing the corporatist structure of its economy. In both cases, the findings support Twitter as a useful tool for Word-of-Mouth Communication on the online environment, among product consumers, and between consumers and enterprises.},
author = {Meza, Xanat Vargas and Park, Han Woo},
doi = {10.1007/s10551-014-2345-y},
file = {::},
isbn = {0167-4544},
issn = {15730697},
journal = {Journal of Business Ethics},
keywords = {Marketing,Mexico,Organic products,Social media,South Korea,Twitter,Webometrics},
number = {3},
pages = {587--603},
publisher = {Springer Netherlands},
title = {{Organic Products in Mexico and South Korea on Twitter}},
volume = {135},
year = {2016}
}
@article{Meza2016,
abstract = {Although there is a growing body of research on social media, only few studies have considered organic products. Therefore, this study mapped the diffusion path of the social media resources for organic products in Mexico and South Korea through Twitter and compared the contents of tweets about organic products in terms of their semantic and hyperlink networks using webometric methods. The results indicate that for organic products, Koreans sent tweets much more frequently than Mexicans. Mexican tweets focused on basic food products in street markets, whereas Korean tweets highlighted promotions and firms, revealing the corporatist structure of its economy. In both cases, the findings support Twitter as a useful tool for Word-of-Mouth Communication on the online environment, among product consumers, and between consumers and enterprises.},
author = {Meza, Xanat Vargas and Park, Han Woo},
doi = {10.1007/s10551-014-2345-y},
file = {::},
isbn = {0167-4544},
issn = {15730697},
journal = {Journal of Business Ethics},
keywords = {Marketing,Mexico,Organic products,Social media,South Korea,Twitter,Webometrics},
number = {3},
pages = {587--603},
publisher = {Springer Netherlands},
title = {{Organic Products in Mexico and South Korea on Twitter}},
volume = {135},
year = {2016}
}
@article{He2018a,
abstract = {Attention-based long short-term memory (LSTM) networks have proven to be useful in aspect-level sentiment classification. However, due to the difficulties in annotating aspect-level data, existing public datasets for this task are all relatively small, which largely limits the effectiveness of those neural models. In this paper, we explore two approaches that transfer knowledge from document- level data, which is much less expensive to obtain, to improve the performance of aspect-level sentiment classification. We demonstrate the effectiveness of our approaches on 4 public datasets from SemEval 2014, 2015, and 2016, and we show that attention-based LSTM benefits from document-level knowledge in multiple ways.},
archivePrefix = {arXiv},
arxivId = {1806.04346},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
eprint = {1806.04346},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2018 - Exploiting Document Knowledge for Aspect-level Sentiment Classification.pdf:pdf},
title = {{Exploiting Document Knowledge for Aspect-level Sentiment Classification}},
url = {http://arxiv.org/abs/1806.04346},
year = {2018}
}
@inproceedings{Wang2011,
abstract = {Researchers have access to large online archives of scientific arti- cles. As a consequence, finding relevant papers has become more difficult. Newly formed online communities of researchers sharing citations provides a new way to solve this problem. In this paper, we develop an algorithm to recommend scientific articles to users of an online community. Our approach combines the merits of traditional collaborative filtering and probabilistic topic modeling. It provides an interpretable latent structure for users and items, and can form recommendations about both existing and newly published articles. We study a large subset of data from CiteULike, a bibliography shar- ing service, and show that our algorithm provides a more effective recommender system than traditional collaborative filtering.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2581v1},
author = {Wang, Chong and Blei, David M.},
booktitle = {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11},
doi = {10.1145/2020408.2020480},
eprint = {arXiv:1411.2581v1},
isbn = {9781450308137},
issn = {14710072},
pages = {448},
pmid = {16990852},
title = {{Collaborative topic modeling for recommending scientific articles}},
url = {http://dl.acm.org/citation.cfm?doid=2020408.2020480},
year = {2011}
}
@article{Bengio2001,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
annote = {why word embeddings are powerful},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1039/c5ra24597d},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2001 - A neural probabilistic language model.pdf:pdf},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
title = {{A neural probabilistic language model}},
url = {http://www.iro.umontreal.ca/{~}lisa/publications2/index.php/attachments/single/74 https://dl.acm.org/citation.cfm?id=944919.944966},
volume = {3},
year = {2001}
}
@article{Mimno2011c,
abstract = {Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.},
author = {Mimno, David and Blei, David},
file = {::},
isbn = {978-1-937284-11-4},
journal = {Emnlp},
keywords = {topic models},
pages = {227--237},
title = {{Bayesian Checking for Topic Models}},
url = {http://dl.acm.org/citation.cfm?id=2145432.2145459},
year = {2011}
}
@techreport{Marx2015,
abstract = {In this paper, we address the problem of aspect-specific sentiment analysis. Given product reviews, our goal is to extract not only the general sentiment of the review , but the aspects mentioned in the review and the sentiments specific to these aspects. We approach this problem by both jointly and sequentially predicting the aspects and sentiments of a review. Within these frameworks, we explore forms of both recursive and recurrent neural nets. To handle sentences with multiple aspect-sentiment pairs, we develop approaches to predict multiple classes. On our dataset with 17 classes (and multiple classes per example), we achieve 51.8{\%} accuracy in predicting aspect-sentiment pairs, a vast improvement over our baseline using Naive Bayes and tf-idf features with 37.3{\%} accuracy.},
author = {Marx, Elliot and Yellin-Flaherty, Zachary},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Marx, Yellin-Flaherty - Unknown - Aspect Specific Sentiment Analysis of Unstructured Online Reviews.pdf:pdf},
institution = {Standford},
pages = {1--9},
title = {{Aspect Specific Sentiment Analysis of Unstructured Online Reviews}},
url = {https://cs224d.stanford.edu/reports/MarxElliot.pdf},
year = {2015}
}
@article{article,
author = {Landis, J and {G. Koch}, Gary},
doi = {10.2307/2529310},
journal = {Biometrics},
pages = {159--174},
title = {{The Measurement Of Observer Agreement For Categorical Data}},
volume = {33},
year = {1977}
}
@inproceedings{Toutanova2007,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
doi = {10.3115/1073445.1073478},
file = {::},
pages = {173--180},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {https://nlp.stanford.edu/{~}manning/papers/tagging.pdf},
year = {2007}
}
@article{Rusu2016,
abstract = {Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1610.04286},
author = {Rusu, Andrei A. and Vecerik, Mel and Roth{\"{o}}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
eprint = {1610.04286},
file = {::},
keywords = {corl,progressive networks,robot learning,sim-to-real,transfer,transfer learning},
mendeley-tags = {transfer learning},
number = {CoRL},
pages = {1--9},
title = {{Sim-to-Real Robot Learning from Pixels with Progressive Nets}},
url = {http://arxiv.org/abs/1610.04286},
year = {2016}
}
@article{Conneau2018a,
abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. "Downstream" tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
annote = {(probing) Tasks to test sentence embeddings

here is a general consensus in the field that the simple approach of directly averaging a sentence's word vectors (so-called Bag-of-Word approach) gives a strong baseline for many downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1805.01070},
author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"{i}}c and Baroni, Marco},
eprint = {1805.01070},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Conneau et al. - 2018 - What you can cram into a single vector Probing sentence embeddings for linguistic properties.pdf:pdf},
keywords = {sentence embeddings},
mendeley-tags = {sentence embeddings},
month = {may},
title = {{What you can cram into a single vector: Probing sentence embeddings for linguistic properties}},
url = {http://arxiv.org/abs/1805.01070},
year = {2018}
}
@article{Akhundov2018,
abstract = {We take a practical approach to solving sequence labeling problem assuming unavailability of domain expertise and scarcity of informational and computational resources. To this end, we utilize a universal end-to-end Bi-LSTM-based neural sequence labeling model applicable to a wide range of NLP tasks and languages. The model combines morphological, semantic, and structural cues extracted from data to arrive at informed predictions. The model's performance is evaluated on eight benchmark datasets (covering three tasks: POS-tagging, NER, and Chunking, and four languages: English, German, Dutch, and Spanish). We observe state-of-the-art results on four of them: CoNLL-2012 (English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus (German POS-tagging), and competitive performance on the rest.},
annote = {Summary

Sequence labeling with bi-lstms using byte- and word embeddings together},
archivePrefix = {arXiv},
arxivId = {1808.03926},
author = {Akhundov, Adnan and Trautmann, Dietrich and Groh, Georg},
eprint = {1808.03926},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Akhundov, Trautmann, Groh - 2018 - Sequence Labeling A Practical Approach.pdf:pdf},
keywords = {NER,POS,chunking,lstm,named entity recognition,part-of-speech,shallow parsing,supervised learning},
mendeley-tags = {NER,POS,chunking,lstm,named entity recognition,part-of-speech,shallow parsing,supervised learning},
title = {{Sequence Labeling: A Practical Approach}},
url = {http://arxiv.org/abs/1808.03926},
year = {2018}
}
@article{Tolstikhin2017,
archivePrefix = {arXiv},
arxivId = {1711.01558},
author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
eprint = {1711.01558},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Tolstikhin et al. - 2017 - Wasserstein Auto-Encoders.pdf:pdf},
month = {nov},
title = {{Wasserstein Auto-Encoders}},
url = {https://arxiv.org/abs/1711.01558},
year = {2017}
}
@book{Pennington2014,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christoper},
booktitle = {EMNLP},
doi = {10.3115/v1/D14-1162},
month = {jan},
pages = {1532--1543},
title = {{Glove: Global Vectors for Word Representation}},
volume = {14},
year = {2014}
}
@article{Levenshtein1966,
author = {Levenshtein, V.I.},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Levenshtein - 1966 - Binary codes capable of correcting deletions.pdf:pdf},
journal = {Insertions and Reversals. Sov},
pages = {707--710},
title = {{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}},
url = {https://nymity.ch/sybilhunting/pdf/Levenshtein1966a.pdf},
volume = {6},
year = {1966}
}
@misc{FrancoiseBeaufays2017,
author = {{Francoise Beaufays}, Google},
title = {{I/O '17 Guide - Interview with}},
url = {https://www.youtube.com/watch?v=jUFetIK1whg},
year = {2017}
}
@book{howard2015pearson,
author = {{for Research in Social Science}, Howard W Odum Institute and Institute, Odum},
isbn = {9781473937901},
publisher = {SAGE},
title = {{Pearson's Correlation Coefficient and the Integrated Postsecondary Education Data System (2012): Graduation Rates by Type of College}},
url = {https://books.google.de/books?id=TQ2kAQAACAAJ},
year = {2015}
}
@misc{M.ChidambaramY.YangD.CerS.YuanY.-H.SungB.Strope,
author = {{M. Chidambaram, Y. Yang, D. Cer, S. Yuan, Y.-H. Sung, B. Strope}, and R. Kurzweil},
title = {{TensorFlow Hub}},
url = {https://tfhub.dev/google/universal-sentence-encoder-xling/en-de/1},
urldate = {2019-01-10}
}
@article{Perone2018,
abstract = {Despite the fast developmental pace of new sentence embedding methods, it is still challenging to find comprehensive evaluations of these different techniques. In the past years, we saw significant improvements in the field of sentence embeddings and especially towards the development of universal sentence encoders that could provide inductive transfer to a wide variety of downstream tasks. In this work, we perform a comprehensive evaluation of recent methods using a wide variety of downstream and linguistic feature probing tasks. We show that a simple approach using bag-of-words with a recently introduced language model for deep context-dependent word embeddings proved to yield better results in many tasks when compared to sentence encoders trained on entailment datasets. We also show, however, that we are still far away from a universal encoder that can perform consistently across several downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1806.06259},
author = {Perone, Christian S. and Silveira, Roberto and Paula, Thomas S.},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1806.06259},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Perone, Silveira, Paula - Unknown - Evaluation of sentence embeddings in downstream and linguistic probing tasks(2).pdf:pdf},
isbn = {9781577357384},
issn = {10495258},
pmid = {18244602},
title = {{Evaluation of sentence embeddings in downstream and linguistic probing tasks}},
url = {http://arxiv.org/abs/1806.06259},
year = {2018}
}
@article{Teh2007,
abstract = {Latent Dirichlet allocation (LDA) is a Bayesian network that has recently gained much popularity in applications ranging from document modeling to computer vision. Due to the large scale nature of these applications, current inference procedures like variational Bayes and Gibbs sampling have been found lacking. In this paper we propose the collapsed variational Bayesian inference algorithm for LDA, and show that it is computationally efficient, easy to implement and significantly more accurate than standard variational Bayesian inference for LDA. 1},
author = {Teh, Yw and Newman, D and Welling, M},
isbn = {0-262-19568-2},
issn = {1049-5258},
journal = {Nips},
keywords = {bayesian methodology,topic models,variational inference},
pages = {1353--1360},
title = {{A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation}},
url = {http://books.nips.cc/papers/files/nips19/NIPS2006{\_}0511.pdf},
volume = {19},
year = {2007}
}
@article{Sang2003,
abstract = {We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.},
archivePrefix = {arXiv},
arxivId = {cs/0306050},
author = {Sang, Erik F. Tjong Kim and {De Meulder}, Fien},
eprint = {0306050},
file = {::},
primaryClass = {cs},
title = {{Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition}},
url = {http://lcg-www.uia.ac.be/conll2003/ner/ http://arxiv.org/abs/cs/0306050},
year = {2003}
}
@misc{Parr2018,
abstract = {Gradient boosting machines (GBMs) are currently very popular and so it's a good idea for machine learning practitioners to understand how GBMs work. The problem is that understanding all of the mathematical machinery is tricky and, unfortunately, these details are needed to tune the hyper-parameters. (Tuning the hyper-parameters is required to get a decent GBM model unlike, say, Random Forests.) Our goal in this article is to explain the intuition behind gradient boosting, provide visualizations for model construction, explain the mathematics as simply as possible, and answer thorny questions such as why GBM is performing “gradient descent in function space.”},
author = {Parr, Terence and Howard, Jeremy},
booktitle = {Explained.ai},
pages = {1--2},
title = {{How to explain gradient boosting}},
url = {http://explained.ai/gradient-boosting/index.html},
year = {2018}
}
@article{Cohen1960,
annote = {doi: 10.1177/001316446002000104},
author = {Cohen, Jacob},
doi = {10.1177/001316446002000104},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
month = {apr},
number = {1},
pages = {37--46},
publisher = {SAGE Publications Inc},
title = {{A Coefficient of Agreement for Nominal Scales}},
url = {https://doi.org/10.1177/001316446002000104},
volume = {20},
year = {1960}
}
@phdthesis{Ayyad2018,
author = {Ayyad, Ahmed},
file = {::},
pages = {12},
school = {Techische Universit{\"{a}}t M{\"{u}}nchen},
title = {{Aspect-based Sentiment Analysis}},
type = {Guided Research},
year = {2018}
}
@book{Manning:2008:IIR:1394399,
address = {New York, NY, USA},
author = {Manning, Christopher D and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
isbn = {0521865719, 9780521865715},
publisher = {Cambridge University Press},
title = {{Introduction to Information Retrieval}},
year = {2008}
}
@phdthesis{Widmer,
abstract = {Comment sections of editorial news sites, blogs, and forums are becoming a increasingly popular place for users and consumers to exchange their opinions. In this thesis we perform topic modeling with Latent Dirichlet Allocation (LDA) and Non-negative Matrix factorization to identify the main topics of German and U.S. discussions regarding organic food. With the help of a domain expert the identified topics were compared to consumer responses from qualitative surveys. Our results show that the issues that are important to the consumers when making consumption decisions are also expressed in online discussions. Further, we show that topic modeling can be used to identify events and trends that had an significant impact on online discussions.},
author = {Widmer, Christian},
file = {::},
school = {Technische Universit{\"{a}}t M{\"{u}}nchen},
title = {{Topic Modeling for Opinion Mining}}
}
@phdthesis{Wittmann2018,
abstract = {For the past few decades, relational databases have been the default choice for data storage, especially for enterprise applications. Currently, many other database technologies are grabbing more attention due to their high performance, scalability, and availability options, such as Not Only SQL (NoSQL) technologies. Choosing the right database technology for applications among a plethora of database options is a challenging task. This research aims to provide a systematic and experimental evaluation of four NoSQL databases across the spectrum of different NoSQL categories. The investigated databases are Redis, MongoDB, Neo4j, and Cassandra. We study multiple aspects such as the database transaction support, query options, data layout, scalability, availability, security, and durability. Besides, we analyze the data modelling of each database using a data model from the Transaction Processing Council Ad-hoc (TPCH) benchmark. Furthermore, we evaluate the query capabilities of each database using three complex queries from the same benchmark. Based on the examination, we capture the strengths and weaknesses each database has. Finally, we present a set of factors that influence the adoption of a NoSQL solution. These factors assist software architects and developers to choose the proper database technology for their specific application needs.},
author = {Wittmann, Elisabeth},
file = {::},
isbn = {1253978748590},
title = {{Transfer Learning for Emotion Recognition on Multimodal Data from Children with Autism Spectrum Condition}},
url = {http://www.ii.uib.no/publikasjoner/texrap/pdf/2008-367.pdf},
year = {2018}
}
@article{Akhundov2018,
abstract = {We take a practical approach to solving sequence labeling problem assuming unavailability of domain expertise and scarcity of informational and computational resources. To this end, we utilize a universal end-to-end Bi-LSTM-based neural sequence labeling model applicable to a wide range of NLP tasks and languages. The model combines morphological, semantic, and structural cues extracted from data to arrive at informed predictions. The model's performance is evaluated on eight benchmark datasets (covering three tasks: POS-tagging, NER, and Chunking, and four languages: English, German, Dutch, and Spanish). We observe state-of-the-art results on four of them: CoNLL-2012 (English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus (German POS-tagging), and competitive performance on the rest.},
archivePrefix = {arXiv},
arxivId = {1808.03926},
author = {Akhundov, Adnan and Trautmann, Dietrich and Groh, Georg},
eprint = {1808.03926},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Akhundov, Trautmann, Groh - 2018 - Sequence Labeling A Practical Approach.pdf:pdf},
title = {{Sequence Labeling: A Practical Approach}},
url = {http://arxiv.org/abs/1808.03926},
year = {2018}
}
@article{Wallach2009a,
abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is in- tractable, several estimators for this prob- ability have been used in the topic model- ing literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held- out documents, and propose two alternative methods that are both accurate and efficient.},
author = {Wallach, Hanna M and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
doi = {10.1145/1553374.1553515},
file = {::},
isbn = {978-1-60558-516-1},
issn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
number = {4},
pages = {1105--1112},
title = {{Evaluation Methods for Topic Models}},
url = {http://doi.acm.org/10.1145/1553374.1553515},
year = {2009}
}
@phdthesis{Caruana1997a,
author = {Caruana, Rich},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(3).pdf:pdf},
school = {Carnegie Melon University},
title = {{Multitask Learning}},
type = {Ph.D thesis},
url = {http://www8.cs.umu.se/research/ifor/dl/LEARNING/multitask learning.pdf},
year = {1997}
}
@book{Singhal2001,
author = {Singhal, Amit and Google, I},
booktitle = {IEEE Data Engineering Bulletin},
month = {jan},
title = {{Modern Information Retrieval: A Brief Overview}},
volume = {24},
year = {2001}
}
@inproceedings{Blei2006,
abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.1486v1},
author = {Blei, David M. and Lafferty, John D.},
booktitle = {Proceedings of the 23rd international conference on Machine learning  - ICML '06},
doi = {10.1145/1143844.1143859},
eprint = {arXiv:0712.1486v1},
isbn = {1595933832},
issn = {1932-6157},
pages = {113--120},
pmid = {9013932},
title = {{Dynamic topic models}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143859},
year = {2006}
}
@article{Zhang2016,
abstract = {This article offers an empirical exploration on the use of character-level convolu- tional networks (ConvNets) for text classification. We constructed several large- scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag ofwords, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {1502.01710},
author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
doi = {10.1093/ndt/gfv436},
eprint = {1502.01710},
file = {::},
isbn = {0123456789},
issn = {14602385},
journal = {arxiv.org},
pmid = {19724569},
title = {{Character-level Convolutional Networks for Text Classificatio}},
year = {2016}
}
@article{Radford2017,
abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
archivePrefix = {arXiv},
arxivId = {1704.01444},
author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
eprint = {1704.01444},
file = {::},
title = {{Learning to Generate Reviews and Discovering Sentiment}},
url = {http://arxiv.org/abs/1704.01444},
year = {2017}
}
@book{Zheng2015,
abstract = {Whenever we want to teach the computer to perform a particular task, we distinguish between two phases: training and testing. In the training phase, we feed the computer labeled examples, which the computer can use to associate patterns in the data with the output labels it needs to learn to classify . In the testing phase , we then give the computer unseen examples which it needs to classify correctly . Inourmatchmakingexamplethismeansthatthecomputerlearnstheoptimaldecisiontreeinthetrainingphaseandappliesthisdecisiontreetonew,unseenexamplesinthetestingphase.Wewouldofcourseliketomakesurethatwecanactuallycheckwhetherornotthecomputerpredictedthecorrectanswers—yesornoinourmatchmakingcase.Thismeansweneedtohavethecorrectanswersforourtestcases.Astandardwayofevaluatingmachinelearningalgorithmsisbydividingourdatasetupintoatrainingsetandatestset.Wewantthetrainingsettobesufficientlylarge,sowecanbesurethealgorithmhasseenmanydifferentexamplesandthedecisiontreeisabletogeneralizewellenough.Forexample,wecouldreserve80{\%}ofourdatasetfortraining,withtheremaining20{\%}usedfortesting.Thetrainingsetisfedtothecomputertolearnadecisiontreeon.The10{\%}testsetthenservesasoursetofunseenexamples.Tosimulatethatwehaveneverseentheseinstancesbefore,weactasifwedon'tknowthefinalclassification.Then,afterthecomputerhasgeneratedpredictionsforeachtestinstance,wecomparethistotheoriginallabelsweknowtobetrue.Bycheckinghowmanylabelsthealgorithmmanagedtopredictcorrectly,weareabletosaysomethingabouthowwellamachinelearningalgorithmhaslearnedthetask.},
author = {Zheng, Alice},
booktitle = {Springer},
file = {::},
isbn = {9781491932469},
pages = {59},
title = {{Evaluating Machine Learning Algorithms}},
year = {2015}
}
@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
author = {Miller, George A.},
doi = {10.1145/219717.219748},
file = {::},
isbn = {1558602720},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
pmid = {17081734},
title = {{WordNet: a lexical database for English}},
url = {http://portal.acm.org/citation.cfm?doid=219717.219748},
volume = {38},
year = {1995}
}
@article{Pauca2006,
abstract = {Data analysis is pervasive throughout business, engineering and science. Very often the data to be analyzed is nonnegative, and it is often preferable to take this constraint into account in the analysis process. Here we are concerned with the application of analyzing data obtained using astronomical spectrometers, which provide spectral data, which is inherently nonnegative. The identification and classification of space objects that cannot be imaged in the normal way with telescopes is an important but difficult problem for tracking thousands of objects, including satellites, rocket bodies, debris, and asteroids, in orbit around the earth. In this paper, we develop an effective nonnegative matrix factorization algorithm with novel smoothness constraints for unmixing spectral reflectance data for space object identification and classification purposes. Promising numerical results are presented using laboratory and simulated datasets. {\textcopyright} 2005 Elsevier Inc. All rights reserved.},
author = {Pauca, V. Paul and Piper, J. and Plemmons, Robert J.},
doi = {10.1016/j.laa.2005.06.025},
isbn = {0024-3795},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
keywords = {Blind source separation,Data mining,Nonnegative matrix factorization,Space object identification and classification,Spectral data},
number = {1},
pages = {29--47},
title = {{Nonnegative matrix factorization for spectral data analysis}},
volume = {416},
year = {2006}
}
@article{Ke2017,
abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algo- rithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-yan},
doi = {10.1145/1731903.1731925},
file = {::},
isbn = {9781605587332},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
number = {Nips},
pages = {3149--3157},
title = {{LightGBM : A Highly Efficient Gradient Boosting Decision Tree}},
year = {2017}
}
@article{Lee2001,
abstract = {Lee D D, Seung H S. Algorithms for non-negative matrix factorization[C]//Advances in neural information processing systems. 2001: 556-562.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0408058v1},
author = {Lee, D D and Seung, H S},
doi = {10.1109/IJCNN.2008.4634046},
eprint = {0408058v1},
file = {::},
isbn = {9781424418206},
issn = {10987576},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {556--562},
pmid = {10548103},
primaryClass = {arXiv:cs},
title = {{Algorithms for Non-negative Matrix Factorization}},
url = {http://papers.nips.cc/paper/1861-alg},
year = {2001}
}
@book{mackay2003information,
author = {MacKay, D J C and Kay, D.J.C.M. and Press, Cambridge University},
isbn = {9780521642989},
publisher = {Cambridge University Press},
title = {{Information Theory, Inference and Learning Algorithms}},
url = {https://books.google.de/books?id=AKuMj4PN{\_}EMC},
year = {2003}
}
@article{Caruana1997,
abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need ofsupervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems. Keywords:},
author = {Caruana, Rich},
doi = {10.1023/A:1007379606734},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Caruana - 1997 - Multitask Learning.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Primary},
mendeley-tags = {Primary},
number = {1},
pages = {41--75},
publisher = {Kluwer Academic Publishers},
title = {{Multitask Learning}},
url = {http://link.springer.com/10.1023/A:1007379606734},
volume = {28},
year = {1997}
}
@article{Rosen-Zvi2004,
abstract = {We introduce the author-topic model, a gen- erative model for documents that extends La- tent Dirichlet Allocation (LDA; Blei, Ng, {\&} Jordan, 2003) to include authorship informa- tion. Each author is associated with a multi- nomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple au- thors is modeled as a distribution over topics that is a mixture of the distributions associ- ated with the authors. We apply the model to a collection of 1,700 NIPS conference pa- pers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative mod- els for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each au- thor is associated with a distribution over words rather than a distribution over top- ics. We show topics recovered by the author- topic model, and demonstrate applications to computing similarity between authors and entropy of author output.},
archivePrefix = {arXiv},
arxivId = {1207.4169},
author = {Rosen-Zvi, M. and Griffiths, T. and Steyvers, M. and Smyth, P.},
doi = {10.1016/j.nima.2010.11.062},
eprint = {1207.4169},
isbn = {0-9749039-0-6},
issn = {01689002},
journal = {Proceedings of the 20th conference on Uncertainty in artificial intelligence},
pages = {487--494},
title = {{The author-topic model for authors and documents}},
url = {http://portal.acm.org/citation.cfm?id=1036902},
year = {2004}
}
@misc{Adamic2000,
abstract = {Many man made and naturally occurring phenomena, including city sizes, incomes, word frequencies, and earthquake magnitudes, are distributed according to a power-law distribution. A power-law implies that small occurrences are extremely common, whereas large instances are extremely rare. This regularity or 'law' is sometimes also referred to as Zipf and sometimes Pareto. To add to the confusion, the laws alternately refer to ranked and unranked distributions. Here we show that all three terms, Zipf, power-law, and Pareto, can refer to the same thing, and how to easily move from the ranked to the unranked distributions and relate their exponents.},
author = {Adamic, Lada A},
booktitle = {International Journal},
doi = {10.1.1.221.8427},
isbn = {0738-3991 (Print)},
issn = {0002-9513},
pages = {2--5},
pmid = {10705063},
title = {{Zipf , Power-laws , and Pareto - a ranking tutorial}},
url = {http://www.hpl.hp.com/research/idl/papers/ranking/ranking.html},
year = {2000}
}
@article{McAuley2015,
abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
archivePrefix = {arXiv},
arxivId = {1506.04757},
author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
eprint = {1506.04757},
file = {::},
pages = {1--11},
title = {{Image-based Recommendations on Styles and Substitutes}},
url = {http://arxiv.org/abs/1506.04757},
year = {2015}
}
@article{Zhao2011,
abstract = {Summarizing and analyzing Twitter content is an important and challenging task. In this paper, we propose to extract topical keyphrases as one way to summarize Twitter. We propose a context-sensitive topical PageRank method for keyword ranking and a probabilistic scoring function that considers both relevance and interestingness of keyphrases for keyphrase ranking. We evaluate our proposed methods on a large Twitter data set. Experiments show that these methods are very effective for topical keyphrase extraction.},
author = {Zhao, Wayne Xin and Jiang, Jing and He, Jing and Song, Yang and Achananuparp, Palakorn and Lim, Ee-Peng and Li, Xiaoming},
file = {::},
isbn = {978-1-932432-87-9},
issn = {1909-230X},
journal = {HLT '11 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
pages = {379--388},
title = {{Topical keyphrase extraction from Twitter}},
url = {http://dl.acm.org/citation.cfm?id=2002472.2002521},
year = {2011}
}
@article{Stevens2012a,
abstract = {We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation of documents and words in a corpus.},
author = {Stevens, Keith and Kegelmeyer, Philip and Andrzejewski, David and Buttler, David},
file = {::},
isbn = {9781937284435},
journal = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
number = {July},
pages = {952--961},
title = {{Exploring Topic Coherence over Many Models and Many Topics}},
url = {http://aclanthology.info/papers/exploring-topic-coherence-over-many-models-and-many-topics},
year = {2012}
}
@article{Zapf2016,
abstract = {BACKGROUND Reliability of measurements is a prerequisite of medical research. For nominal data, Fleiss' kappa (in the following labelled as Fleiss' K) and Krippendorff's alpha provide the highest flexibility of the available reliability measures with respect to number of raters and categories. Our aim was to investigate which measures and which confidence intervals provide the best statistical properties for the assessment of inter-rater reliability in different situations. METHODS We performed a large simulation study to investigate the precision of the estimates for Fleiss' K and Krippendorff's alpha and to determine the empirical coverage probability of the corresponding confidence intervals (asymptotic for Fleiss' K and bootstrap for both measures). Furthermore, we compared measures and confidence intervals in a real world case study. RESULTS Point estimates of Fleiss' K and Krippendorff's alpha did not differ from each other in all scenarios. In the case of missing data (completely at random), Krippendorff's alpha provided stable estimates, while the complete case analysis approach for Fleiss' K led to biased estimates. For shifted null hypotheses, the coverage probability of the asymptotic confidence interval for Fleiss' K was low, while the bootstrap confidence intervals for both measures provided a coverage probability close to the theoretical one. CONCLUSIONS Fleiss' K and Krippendorff's alpha with bootstrap confidence intervals are equally suitable for the analysis of reliability of complete nominal data. The asymptotic confidence interval for Fleiss' K should not be used. In the case of missing data or data or higher than nominal order, Krippendorff's alpha is recommended. Together with this article, we provide an R-script for calculating Fleiss' K and Krippendorff's alpha and their corresponding bootstrap confidence intervals.},
author = {Zapf, Antonia and Castell, Stefanie and Morawietz, Lars and Karch, Andr{\'{e}}},
doi = {10.1186/s12874-016-0200-9},
isbn = {1471-2288},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Bootstrap,Confidence interval,Fleiss' K,Fleiss' kappa,Inter-rater heterogeneity,Krippendorff's alpha},
pmid = {27495131},
title = {{Measuring inter-rater reliability for nominal data - Which coefficients and confidence intervals are appropriate?}},
year = {2016}
}
@book{markov2007data,
author = {Markov, Z and Larose, D T},
isbn = {9780471666554},
publisher = {Wiley},
series = {Wiley series on methods and applications in data mining},
title = {{Data Mining the Web: Uncovering Patterns in Web Content, Structure, and Usage}},
url = {https://books.google.de/books?id=DGjbibiS-S0C},
year = {2007}
}
@article{Das2014b,
abstract = {Social networks serve as important platforms for users to express, exchange and form opinions on various topics. Several opinion dynamics models have been proposed to characterize how a user iteratively updates her expressed opinion based on her innate opinion and the opinion of her neighbors. The extent to how much a user is influenced by her neighboring opinions, as opposed to her own innate opinion, is governed by a measure of her "conformity" parameter. Characterizing this degree of conformity for users of a social network is critical for several applications such as debiasing online surveys and finding social influencers. In this paper, we address the problem of estimating these conformity values for users, using only the expressed opinions and the social graph. We pose this problem in a constrained optimization framework and design efficient algorithms, which we validate on both synthetic and real-world Twitter data. Using these estimated conformity values, we then address the problem of identifying the smallest subset of users in a social graph that, when seeded initially with some non-neutral opinions, can accurately explain the current opinion values of users in the entire social graph. We call this problem seed recovery. Using ideas from compressed sensing, we analyze and design algorithms for both conformity estimation and seed recovery, and validate them on real and synthetic data. Copyright {\textcopyright} 2014 ACM.},
author = {Das, Abhimanyu and Gollapudi, Sreenivas and Khan, Arindam and Leme, Renato Paes},
doi = {10.1145/2660460.2660479},
isbn = {9781450331982},
journal = {Proc. Intl. Conference on Social Networks (COSN)},
pages = {13},
title = {{Role of Conformity in Opinion Dynamics in Social Networks}},
year = {2014}
}
@inproceedings{Xu2017,
abstract = {Although semi-supervised variational autoencoder (SemiVAE) works in image classification task, it fails in text classification task if using vanilla LSTM as its decoder. From a perspective of reinforcement learning, it is verified that the decoder's ca- pability to distinguish between different categorical labels is essential. Therefore, Semi-supervised Sequential Variational Autoencoder (SSVAE) is proposed, which increases the capa- bility by feeding label into its decoder RNN at each time-step. Two specific decoder structures are investigated and both of them are verified to be effective. Besides, in order to reduce the computational complexity in training, a novel optimization method is proposed, which estimates the gradient of the unla- beled objective function by sampling, along with two variance reduction techniques. Experimental results on Large Movie Review Dataset (IMDB) and AG's News corpus show that the proposed approach significantly improves the classifica- tion accuracy compared with pure-supervised classifiers, and achieves competitive performance against previous advanced methods. State-of-the-art results can be obtained by integrating other pretraining-based methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.02514v3},
author = {Xu, Weidi and Sun, Haoze and Deng, Chao and Tan, Ying},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
doi = {10.1051/0004-6361/201527329},
eprint = {arXiv:1603.02514v3},
file = {::},
isbn = {9781493903801},
issn = {19406045},
keywords = {Natural Language Processing and Machine Learning},
pmid = {25246403},
title = {{Variational Autoencoder for Semi-Supervised Text Classification}},
year = {2017}
}
@inproceedings{Mikolov2013c,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
eprint = {1301.3781},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://ronan.collobert.com/senna/ http://arxiv.org/abs/1301.3781},
year = {2013}
}
@inproceedings{Xenos2016,
abstract = {This paper describes our submissions to the Aspect Based Sentiment Analysis task of SemEval-2016. For Aspect Category Detec-tion (Subtask1/Slot1), we used multiple en-sembles, based on Support Vector Machine classifiers. For Opinion Target Expression extraction (Subtask1/Slot2), we used a se-quence labeling approach with Conditional Random Fields. For Polarity Detection (Sub-task1/Slot3), we used an ensemble of two su-pervised classifiers, one based on hand crafted features and one based on word embeddings. Our systems were ranked in the top 6 positions in all the tasks we participated. The source code of our systems is publicly available.},
address = {San Diego, USA},
author = {Xenos, Dionysios and Theodorakakos, Panagiotis and Pavlopoulos, John and Malakasiotis, Prodromos and Androutsopoulos, Ion},
booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
doi = {10.18653/v1/s16-1050},
file = {:Users/felix/Downloads/S16-1050.pdf:pdf},
pages = {312--317},
title = {{AUEB-ABSA at SemEval-2016 Task 5: Ensembles of Classifiers and Embeddings for Aspect Based Sentiment Analysis}},
year = {2016}
}
@article{Bandorski2016,
abstract = {The top-performing Question–Answering (QA) systems have been of two types: consistent, solid, well-established and multi-faceted systems that do well year after year, and ones that come out of nowhere employ- ing totally innovative approaches and which out-perform almost every- body else. This article examines both types of system in depth. We establish what a “typical” QA-system looks like, and cover the com- monly used approaches by the component modules. Understanding this will enable any proficient system developer to build his own QA-system. Fortunately there are many components available for free from their developers to make this a reasonable expectation for a graduate-level project. We also look at particular systems that have performed well and which employ interesting and innovative approaches.},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Bandorski, Dirk and Kurniawan, Niehls and Baltes, Peter and Hoeltgen, Reinhard and Hecker, Matthias and Stunder, Dominik and Keuchel, Martin},
doi = {10.3748/wjg.v22.i45.9898},
eprint = {0112017},
file = {::},
isbn = {1601981503},
issn = {22192840},
journal = {World Journal of Gastroenterology},
keywords = {Aspiration,Contraindications,Magnetic,Pacemaker,Pregnancy,Stenosis,Video capsule endoscopy},
number = {45},
pages = {9898--9908},
pmid = {191},
primaryClass = {cs},
title = {{Contraindications for video capsule endoscopy}},
volume = {22},
year = {2016}
}
@inproceedings{Newman2006,
abstract = {Statistical lauguage models can learn relationships between topics discussed in a document collection and persons, organizations and places mentioned in each document. We present a novel combination of statistical topic models and named-entity recognizers to jointly analyze entities mentioned (persons, organizations and places) and topics discussed in a collection of 330,000 New York Times news articles. We demonstrate an analytic framework which automatically extracts from a large collection: topics; topic trends; and topics that relate entities.},
author = {Newman, David and Chemudugunta, Chaitanya and Smyth, Padhraic and Steyvers, Mark},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11760146_9},
isbn = {3540344780},
issn = {16113349},
pages = {93--104},
title = {{Analyzing entities and topics in news articles using statistical topic models}},
volume = {3975 LNCS},
year = {2006}
}
@article{Ringeval2017,
abstract = {The Audio/Visual Emotion Challenge and Workshop (AVEC 2017) "Real-life depression, and aaect" will be the seventh competition event aimed at comparison of multimedia processing and machine learning methods for automatic audiovisual depression and emotion analysis, with all participants competing under strictly the same conditions. .e goal of the Challenge is to provide a common benchmark test set for multimodal information processing and to bring together the depression and emotion recognition communities, as well as the audiovisual processing communities, to compare the relative merits of the various approaches to depression and emotion recognition from real-life data. .is paper presents the novelties introduced this year, the challenge guidelines, the data used, and the performance of the baseline system on the two proposed tasks: dimensional emotion recognition (time and value-continuous), and dimensional depression estimation (value-continuous).},
author = {Ringeval, Fabien and Schuller, Bj{\"{o}}rn and Valstar, Michel and Gratch, Jonathan and Cowie, Roddy and Scherer, Stefan and Mozgai, Sharon and Cummins, Nicholas and Schmii, Maximilian and Pantic, Maja},
doi = {10.1145/3133944.3133953},
file = {::},
isbn = {9781450355025},
keywords = {evaluator weighted estimator,ewe,statistics,•Computing methodologies  Biometrics,•General and reference  Performance},
mendeley-tags = {evaluator weighted estimator,ewe,statistics},
title = {{AVEC 2017-Real-life Depression, and AAect Recognition Workshop and Challenge}},
url = {http://delivery.acm.org/10.1145/3140000/3133953/p3-ringeval.pdf?ip=150.65.249.59{\&}id=3133953{\&}acc=ACTIVE SERVICE{\&}key=D2341B890AD12BFE.FC02734FB516017D.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1536202826{\_}65ce5331fb4c3a577a21529d3223b817},
year = {2017}
}
@inproceedings{Pontiki2014,
abstract = {Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen). SemEval- 2014 Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task pro- vided datasets containing manually anno- tated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams.},
address = {Dublin, Ireland},
author = {Pontiki, Maria and Galanis, Dimitris and Pavlopoulos, John and Papageorgiou, Harris and Androutsopoulos, Ion and Manandhar, Suresh},
booktitle = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
doi = {10.3115/v1/s14-2004},
file = {:Users/felix/Downloads/S14-2004.pdf:pdf},
number = {SemEval},
pages = {27--35},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2014 Task 4: Aspect Based Sentiment Analysis}},
year = {2014}
}
@book{Gwet2014a,
author = {Gwet, Kilem Li},
isbn = {9780970806284 0970806280},
language = {English},
title = {{Handbook of inter-rater reliability : the definitive guide to measuring the extent of agreement among raters}},
year = {2014}
}
@inproceedings{Wojatzki2017,
abstract = {This paper describes the GermEval 2017 shared task on Aspect-Based Sentiment Analysis that consists of four subtasks: relevance , document-level sentiment polarity, aspect-level polarity ad opinion target extraction. System performance is measured on two evaluation sets-one from the same time period as the training and development set, and a second one, which contains data from a later time period. We describe the subtasks and the data in detail and provide the shared task results. Overall, the shared task attracted over 50 system runs from 8 teams.},
address = {Berlin},
author = {Wojatzki, Michael and Ruppert, Eugen and Holschneider, Sarah and Zesch, Torsten and Biemann, Chris and CogSci, Appl},
booktitle = {Proceedings of the GSCL GermEval Shared Task on Aspect-based Sentiment in Social Media Customer Feedback},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Wojatzki et al. - Unknown - GermEval 2017 Shared Task on Aspect-based Sentiment in Social Media Customer Feedback(2).pdf:pdf},
pages = {1----12},
title = {{GermEval 2017: Shared Task on Aspect-based Sentiment in Social Media Customer Feedback}},
url = {http://www.ltl.uni-due.de/http://lt.informatik.uni-hamburg.de},
year = {2017}
}
@misc{Ofcom2017,
author = {Ofcom},
booktitle = {International Communications Market Report 2017},
file = {::},
title = {{Which, if any, is your main source of news ?}},
urldate = {2018-03-16},
year = {2017}
}
@article{Behrens1997,
abstract = {Exploratory data analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns to foster hypothesis development and refinement. These tools and attitudes complement the use of significance and hypothesis tests used in confirmatory data analysis (CDA). Although EDA complements rather than replaces CDA, use of CDA without EDA is seldom warranted. Even when well specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data. This article introduces the central heuristics and computational tools of EDA and contrasts it with CDA and exploratory statistics in general. EDA techniques are illustrated using previously published psychological data. Changes in statistical training and practice are recommended to incorporate these tools.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Behrens, John T.},
doi = {10.1037/1082-989X.2.2.131},
eprint = {9809069v1},
file = {::},
isbn = {1082-989X$\backslash$r1939-1463},
issn = {1082989X},
journal = {Psychological Methods},
number = {2},
pages = {131--160},
pmid = {24809974},
primaryClass = {arXiv:gr-qc},
title = {{Principles and Procedures of Exploratory Data Analysis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.362.8937{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {1997}
}
@inproceedings{Søgaard2016,
abstract = {In all previous work on deep multi-task learning we are aware of, all task super- visions are on the same (outermost) layer. We present a multi-task learning architec- ture with deep bi-directional RNNs, where different tasks supervision can happen at different layers. We present experiments in syntactic chunking and CCG supertag- ging, coupled with the additional task of POS-tagging. We show that it is consis- tently better to have POS supervision at the innermost rather than the outermost layer. We argue that this is because “low- level” tasks are better kept at the lower layers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks. Finally, we also show how this architecture can be used for domain adaptation.},
author = {S{\o}gaard, Anders and Goldberg, Yoav},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
doi = {10.18653/v1/p16-2038},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/S{\o}gaard, Goldberg - Unknown - Deep multi-task learning with low level tasks supervised at lower layers.pdf:pdf},
keywords = {nlp},
mendeley-tags = {nlp},
pages = {231--235},
title = {{Deep multi-task learning with low level tasks supervised at lower layers}},
url = {http://ronan.collobert.com/senna/},
year = {2016}
}
@article{Yang2016a,
abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
archivePrefix = {arXiv},
arxivId = {1606.02393},
author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
doi = {10.18653/v1/N16-1174},
eprint = {1606.02393},
file = {::},
isbn = {9781941643914},
issn = {1606.02393},
journal = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {1480--1489},
title = {{Hierarchical Attention Networks for Document Classification}},
url = {http://aclweb.org/anthology/N16-1174},
year = {2016}
}
@article{Kullback1951,
abstract = {The use of transformations to stabilize the variance of binomial or Poisson data is familiar(Anscombe 1, Bartlett 2, 3, Curtiss 4, Eisenhart 5). The comparison of transformed binomial or Poisson data with percentage points of the normal distribution to make approximate significance tests or to set approximate confidence intervals is less familiar. Mosteller and Tukey 6 have recently made a graphical application of a transformation related to the square-root transformation for such purposes, where the use of "binomial probability paper" avoids all computation. We report here on an empirical study of a number of approximations, some intended for significance and confidence work and others for variance stabilization. For significance testing and the setting of confidence limits, we should like to use the normal deviate K exceeded with the same probability as the number of successes x from n in a binomial distribution with expectation np, which is defined by frac12pi int{\^{}}K-infty e{\^{}}-frac12t{\^{}}2 dt = operatornameProb x leq k mid operatornamebinomial, n, p. The most useful approximations to K that we can propose here are N (very simple), N{\^{}}+ (accurate near the usual percentage points), and N{\^{}}astast (quite accurate generally), where N = 2 (sqrt(k + 1)q - sqrt(n - k)p). (This is the approximation used with binomial probability paper.) N{\^{}}+ = N + fracN + 2p - 112sqrtE,quad E = textlesser of np textand nq, N{\^{}}ast = N + frac(N - 2)(N + 2)12 big(frac1sqrtnp + 1 - frac1sqrtnq + 1big), N{\^{}}astast = N{\^{}}ast + fracN{\^{}}ast + 2p - 112 sqrtEcdotquad E = textlesser of np textand nq. For variance stabilization, the averaged angular transformation sin{\^{}}-1sqrtfracxn + 1 + sin{\^{}}-1 sqrtfracx + 1n+1 has variance within pm 6{\%} of frac1n + frac12 text(angles in radians), frac821n + frac12 text(angles in degrees), for almost all cases where np geq 1. In the Poisson case, this simplifies to using sqrtx + sqrtx + 1 as having variance 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kullback, S. and Leibler, R. A.},
doi = {10.1214/aoms/1177705148},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {1862391912},
issn = {00034851},
journal = {Ann. Math. Statist. Volume 22, Number 1 79-86.},
keywords = {KL距離 KL divergence},
number = {3},
pages = {688--697},
pmid = {15288874},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Mathematical Statistics. {\textregistered} www.jstor.org}},
volume = {37},
year = {1951}
}
@inproceedings{Hall2008,
abstract = {How can the development of ideas in a sci- entific field be studied over time? We ap- ply unsupervised topic modeling to the ACL Anthology to analyze historical trends in the field of Computational Linguistics from 1978 to 2006. We induce topic clusters using Latent Dirichlet Allocation, and examine the strength of each topic over time. Our methods find trends in the field including the rise of prob- abilistic methods starting in 1988, a steady in- crease in applications, and a sharp decline of research in semantics and understanding be- tween 1978 and 2001, possibly rising again after 2001. We also introduce a model of the diversity of ideas, topic entropy, using it to show that COLING is a more diverse confer- ence than ACL, but that both conferences as well as EMNLP are becoming broader over time. Finally, we apply Jensen-Shannon di- vergence of topic distributions to show that all three conferences are converging in the topics they cover.},
author = {Hall, David and Jurafsky, Daniel and Manning, Christopher D.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP '08},
doi = {10.3115/1613715.1613763},
file = {::},
pages = {363},
title = {{Studying the history of ideas using topic models}},
url = {http://portal.acm.org/citation.cfm?doid=1613715.1613763},
year = {2008}
}
@techreport{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4{\%} (7.6{\%} absolute improvement), MultiNLI accuracy to 86.7{\%} (5.6{\%} absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.},
annote = {BERT is designed to pre-train
deep bidirectional representations by jointly
conditioning on both left and right context in
all layers.


BERT's model architecture is a multi-layer bidi-rectional Transformer encoder based on the orig-inal implementation},
archivePrefix = {arXiv},
arxivId = {1810.04805v1},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Google, Kristina Toutanova and Language, A I},
eprint = {1810.04805v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
keywords = {Transformer},
mendeley-tags = {Transformer},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {https://github.com/tensorflow/tensor2tensor},
year = {2018}
}
@article{Chae2015,
abstract = {Abstract Recently, businesses and research communities have paid a lot of attention to social media and big data. However, the field of supply chain management (SCM) has been relatively slow in studying social media and big data for research and practice. In these contexts, this research contributes to the SCM community by proposing a novel, analytical framework (Twitter Analytics) for analyzing supply chain tweets, highlighting the current use of Twitter in supply chain contexts, and further developing insights into the potential role of Twitter for supply chain practice and research. The proposed framework combines three methodologies - descriptive analytics (DA), content analytics (CA) integrating text mining and sentiment analysis, and network analytics (NA) relying on network visualization and metrics - for extracting intelligence from 22,399 {\#}supplychain tweets. Some of the findings are: supply chain tweets are used by different groups of supply chain professionals and organizations (e.g., news services, IT companies, logistic providers, manufacturers) for information sharing, hiring professionals, and communicating with stakeholders, among others; diverse topics are being discussed, ranging from logistics and corporate social responsibility, to risk, manufacturing, SCM IT and even human rights; some tweets carry strong sentiments about companies' delivery services, sales performance, and environmental standards, and risk and disruption in supply chains. Based on these findings, this research presents insights into the use and potential role of Twitter for supply chain practices (e.g., professional networking, stakeholder engagement, demand shaping, new product/service development, supply chain risk management) and the implications for research. Finally, the limitations of the current study and suggestions for future research are presented.},
archivePrefix = {arXiv},
arxivId = {1506.02089},
author = {Chae, Bongsug},
doi = {10.1016/j.ijpe.2014.12.037},
eprint = {1506.02089},
isbn = {0925-5273},
issn = {09255273},
journal = {International Journal of Production Economics},
keywords = {Application Programming Interface (API),Big data,Content analytics,Data analytics,Network analytics,Social media analytics,Supply chain management,Twitter},
pages = {247--259},
pmid = {1684380358},
publisher = {Elsevier},
title = {{Insights from hashtag {\#}supplychain and Twitter analytics: Considering Twitter and Twitter data for supply chain practice and research}},
volume = {165},
year = {2015}
}
@article{Burscher2014a,
abstract = {We explore the application of supervised machine learning (SML) to frame coding. By automating the coding of frames in news, SML facilitates the incorporation of large-scale content analysis into framing research, even if financial resources are scarce. This furthers a more integrated investigation of framing processes conceptually as well as methodologically. We conduct several experiments in which we automate the coding of four generic frames that are operationalised as a set of indicator questions. In doing so, we compare two approaches to modelling the coherence between indicator questions and frames as an SML task. The results of our experiments show that SML is well suited to automate frame coding but that coding performance is dependent on the way SML is implemented.$\backslash$nWe explore the application of supervised machine learning (SML) to frame coding. By automating the coding of frames in news, SML facilitates the incorporation of large-scale content analysis into framing research, even if financial resources are scarce. This furthers a more integrated investigation of framing processes conceptually as well as methodologically. We conduct several experiments in which we automate the coding of four generic frames that are operationalised as a set of indicator questions. In doing so, we compare two approaches to modelling the coherence between indicator questions and frames as an SML task. The results of our experiments show that SML is well suited to automate frame coding but that coding performance is dependent on the way SML is implemented.},
author = {Burscher, Bj{\"{o}}rn and Odijk, Daan and Vliegenthart, Rens and de Rijke, Maarten and de Vreese, Claes H.},
doi = {10.1080/19312458.2014.937527},
isbn = {1931-2458},
issn = {19312466},
journal = {Communication Methods and Measures},
number = {3},
pages = {190--206},
title = {{Teaching the Computer to Code Frames in News: Comparing Two Supervised Machine Learning Approaches to Frame Analysis}},
volume = {8},
year = {2014}
}
@misc{IfDAllensbach2016,
author = {{IfD Allensbach}},
booktitle = {Allensbacher Computer- und Technik-Analyse - ACTA 2016},
file = {::},
title = {{Number of internet users who posted comments in blogs of other users or wrote contributions in discussion forums in Germany from 2013 to 2016, by frequency ( in millions )}},
year = {2016}
}
@phdthesis{Widmer2018,
abstract = {Comment sections of editorial news sites, blogs, and forums are becoming a increasingly popular place for users and consumers to exchange their opinions. In this thesis we perform topic modeling with Latent Dirichlet Allocation (LDA) and Non-negative Matrix factorization to identify the main topics of German and U.S. discussions regarding organic food. With the help of a domain expert the identified topics were compared to consumer responses from qualitative surveys. Our results show that the issues that are important to the consumers when making consumption decisions are also expressed in online discussions. Further, we show that topic modeling can be used to identify events and trends that had an significant impact on online discussions.},
author = {Widmer, Christian},
file = {::},
school = {Technical University of Munich},
title = {{Topic Modeling for Opinion Mining}},
year = {2018}
}
@inproceedings{Schutze1992,
abstract = {Representations for semantic information about words are necessary for many applications of neural networks in natural language processing. This paper describes an efficient, corpus-based method for inducing distributed semantic representations for a large number of words (50,000) from lexical coccurrence statistics by means of a large-scale linear regression. The representations are successfully applied to word sense disambiguation using a nearest neighbor method .},
author = {Sch{\"{u}}tze, Hinrich},
booktitle = {Advances in Neural Information Processing Systems 5 (NIPS 1992)},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{u}}tze - 1992 - Word Space(3).pdf:pdf},
title = {{Word Space}},
url = {https://papers.nips.cc/paper/603-word-space.pdf},
year = {1992}
}
@article{Conneau2016,
abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.},
archivePrefix = {arXiv},
arxivId = {1606.01781},
author = {Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"{i}}c and Lecun, Yann},
doi = {10.1007/s13218-012-0198-z},
eprint = {1606.01781},
file = {::},
isbn = {9789814618038},
issn = {0933-1875},
pmid = {10463930},
title = {{Very Deep Convolutional Networks for Text Classification}},
url = {http://arxiv.org/abs/1606.01781},
year = {2016}
}
@book{Bishop2007,
author = {Bishop, Christopher M.},
file = {::},
isbn = {9780387310732},
title = {{Machine Learning and Pattern Recoginiton}},
year = {2007}
}
@misc{Devarajan2008,
abstract = {In the last decade, advances in high-throughput technologies such as DNA microarrays have made it possible to simultaneously measure the expression levels of tens of thousands of genes and proteins. This has resulted in large amounts of biological data requiring analysis and interpretation. Nonnegative matrix factorization (NMF) was introduced as an unsupervised, parts-based learning paradigm involving the decomposition of a nonnegative matrix V into two nonnegative matrices, W and H, via a multiplicative updates algorithm. In the context of a pxn gene expression matrix V consisting of observations on p genes from n samples, each column of W defines a metagene, and each column of H represents the metagene expression pattern of the corresponding sample. NMF has been primarily applied in an unsupervised setting in image and natural language processing. More recently, it has been successfully utilized in a variety of applications in computational biology. Examples include molecular pattern discovery, class comparison and prediction, cross-platform and cross-species analysis, functional characterization of genes and biomedical informatics. In this paper, we review this method as a data analytical and interpretive tool in computational biology with an emphasis on these applications.},
author = {Devarajan, Karthik},
booktitle = {PLoS Computational Biology},
doi = {10.1371/journal.pcbi.1000029},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
number = {7},
pmid = {18654623},
title = {{Nonnegative matrix factorization: An analytical and interpretive tool in computational biology}},
volume = {4},
year = {2008}
}
@article{Blei2003a,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {::},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@misc{commonCrawl,
abstract = {The Common Crawl corpus contains petabytes of data collected over the last 7 years. It contains raw web page data, extracted metadata and text extractions.},
title = {{So you're ready to get started. – Common Crawl}},
url = {https://commoncrawl.org/the-data/get-started/},
urldate = {2019-04-18}
}
@article{Loshin2013,
abstract = {This chapter looks at business problems suited for graph analytics, what differentiates the problems from traditional approaches, and considerations for discovery vs. search analyses. We discuss the graph model (vertices and edges), as well as graph analytics. The representation of a graph model using triple notation makes it particularly suited to a big data application model. In addition, we discuss the difference between the use of a traditional data warehouse model for reporting and querying, versus the use of a graph model for discovery analytics. We look at some use cases for graph analytics (health care quality, cybersecurity, correlation analysis), and provide some solution approaches. We then look at the characteristics of a platform to be used for graph analytics, including seamless data intake, high-speed I/O, standards-based representations, and inferencing, multithreading, large memory, among others.},
author = {Loshin, David and Loshin, David},
doi = {10.1016/B978-0-12-417319-4.00010-7},
isbn = {978-0-12-417319-4},
journal = {Big Data Analytics},
month = {jan},
pages = {91--103},
publisher = {Morgan Kaufmann},
title = {{Using Graph Analytics for Big Data}},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000107},
year = {2013}
}
@article{Yang2018,
abstract = {The topic of aspect-based sentiment analysis (ABSA) has been explored for a variety of industries, but it still remains much unexplored in finance. The recent release of data for an open challenge (FiQA) from the companion proceedings of WWW '18 has provided valuable finance-specific annotations. FiQA contains high quality labels, but it still lacks data quantity to apply traditional ABSA deep learning architecture. In this paper, we employ high-level semantic representations and methods of inductive transfer learning for NLP. We experiment with extensions of recently developed domain adaptation methods and target task fine-tuning that significantly improve performance on a small dataset. Our results show an 8.7{\%} improvement in the F1 score for classification and an 11{\%} improvement over the MSE for regression on current state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1808.07931},
author = {Yang, Steve and Rosenfeld, Jason and Makutonin, Jacques},
eprint = {1808.07931},
file = {::},
title = {{Financial Aspect-Based Sentiment Analysis using Deep Representations}},
url = {http://arxiv.org/abs/1808.07931},
year = {2018}
}
@article{Bhatia2016,
abstract = {Topics generated by topic models are typically represented as list of terms. To reduce the cognitive overhead of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels.},
archivePrefix = {arXiv},
arxivId = {1612.05340},
author = {Bhatia, Shraey and Lau, Jey Han and Baldwin, Timothy},
eprint = {1612.05340},
file = {::},
number = {1},
pages = {953--963},
title = {{Automatic Labelling of Topics with Neural Embeddings}},
url = {http://arxiv.org/abs/1612.05340},
year = {2016}
}
@book{Gwet2014a,
author = {Gwet, Kilem Li},
isbn = {9780970806284 0970806280},
language = {English},
title = {{Handbook of inter-rater reliability : the definitive guide to measuring the extent of agreement among raters}},
year = {2014}
}
@article{Cortes:1995:SN:218919.218929,
address = {Hingham, MA, USA},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1023/A:1022627411411},
issn = {0885-6125},
journal = {Mach. Learn.},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
month = {sep},
number = {3},
pages = {273--297},
publisher = {Kluwer Academic Publishers},
title = {{Support-Vector Networks}},
url = {https://doi.org/10.1023/A:1022627411411},
volume = {20},
year = {1995}
}
@article{Moon2017,
abstract = {Product reviews are becoming ubiquitous on the Web, representing a rich source of consumer information on a wide range of product categories (e.g., wines and hotels). Importantly, a product review reflects not only the perception and preference for a product, but also the acuity, bias, and writing style of the reviewer. This reviewer aspect has been overlooked in past studies that have drawn inferences about brands from online product reviews. Our framework combines ontology learning-based text mining and psychometric techniques to translate online product reviews into a product positioning map, while accounting for the idiosyncratic responses and writing styles of individual reviewers or a manageable number of reviewer groups (i.e., meta-reviewers). Our empirical illustrations using wine and hotel reviews demonstrate that a product review reveals information about the reviewer (for the wine example with a small number of expert reviewers) or the meta-reviewer (for the hotel example with an enormous number of reviewers reduced to a manageable number of meta-reviewers), as well as about the product under review. From a managerial perspective, product managers can use our framework focusing on meta-reviewers (e.g., traveler types and hotel reservation websites in our hotel example) as a way to obtain insights into their consumer segmentation strategy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Moon, Sangkil and Kamakura, Wagner A.},
doi = {10.1016/j.ijresmar.2016.05.007},
eprint = {arXiv:1011.1669v3},
isbn = {9780874216561},
issn = {01678116},
journal = {International Journal of Research in Marketing},
keywords = {Consumer segmentation,Experience products,Online product reviews,Product positioning map,Psychometrics,Text mining},
number = {1},
pages = {265--285},
pmid = {15991970},
publisher = {Elsevier B.V.},
title = {{A picture is worth a thousand words: Translating product reviews into a product positioning map}},
volume = {34},
year = {2017}
}
@article{McCann2017,
abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
archivePrefix = {arXiv},
arxivId = {1708.00107},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
eprint = {1708.00107},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/McCann et al. - 2017 - Learned in Translation Contextualized Word Vectors.pdf:pdf},
issn = {10495258},
keywords = {CoVe,GloVe,LSTM,Question classification,SNLI,SST,Sequence-to-sequence,TREC,attention,context vectors,encoder,entailment,sentiment analysis,transfer learning},
mendeley-tags = {CoVe,GloVe,LSTM,Question classification,SNLI,SST,Sequence-to-sequence,TREC,attention,context vectors,encoder,entailment,sentiment analysis,transfer learning},
number = {Nips},
pages = {1--12},
title = {{Learned in Translation: Contextualized Word Vectors}},
url = {http://arxiv.org/abs/1708.00107},
year = {2017}
}
@article{Goldberg2015,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
archivePrefix = {arXiv},
arxivId = {1510.00726},
author = {Goldberg, Yoav},
doi = {10.1613/jair.4992},
eprint = {1510.00726},
file = {::},
issn = {1076-9757},
pages = {1--75},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://arxiv.org/abs/1510.00726},
year = {2015}
}
@article{Uebersax1987,
author = {Uebersax, John S},
file = {::},
number = {1},
pages = {140--146},
title = {{Kapp{\_}and{\_}decision{\_}making{\_}models.pdf}},
volume = {101},
year = {1987}
}
@article{Leth2018,
author = {Leth, Rasmus and Gynther, Karsten},
file = {::},
keywords = {01,03,06,11,12,2018,26,3000 roskilde,accepted,actionable insights,corresponding author 1 email,data-informed decision-making,denmark,dk address,learning analytics,pha,published,ralj,submitted,trekroner forskerpark 4,university college absalon},
number = {3},
pages = {198--221},
title = {{What Constitutes an “ Actionable Insight ” in Learning Analytics ?}},
volume = {5},
year = {2018}
}
@article{Pham2015a,
abstract = {We propose a novel approach to learning dis-tributed representations of variable-length text sequences in multiple languages simultane-ously. Unlike previous work which often de-rive representations of multi-word sequences as weighted sums of individual word vec-tors, our model learns distributed representa-tions for phrases and sentences as a whole. Our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efficiently encode meaning-equivalent text sequences of multi-ple languages in the same semantic space. Our learned embeddings achieve state-of-the-art performance in the often used crosslingual document classification task (CLDC) with an accuracy of 92.7 for English to German and 91.5 for German to English. By learning text sequence representations as a whole, our model performs equally well in both classifi-cation directions in the CLDC task in which past work did not achieve.},
author = {Pham, Hieu and Luong, Minh-Thang and Manning, Christopher D.},
file = {::},
journal = {Workshop on Vector Modeling for NLP},
pages = {88--94},
title = {{Learning Distributed Representations for Multilingual Text Sequences}},
year = {2015}
}
@inproceedings{Cox2011,
abstract = {Many modern computer vision algorithms are built atop of a set of low-level feature operators (such as SIFT [1], [2]; HOG [3], [4]; or LBP [5], [6]) that transform raw pixel values into a representation better suited to subsequent processing and classification. While the choice of feature representation is often not central to the logic of a given algorithm, the quality of the feature representation can have critically important implications for performance. Here, we demonstrate a large-scale feature search approach to generating new, more powerful feature representations in which a multitude of complex, nonlinear, multilayer neuromorphic feature representations are randomly generated and screened to find those best suited for the task at hand. In particular, we show that a brute-force search can generate representations that, in combination with standard machine learning blending techniques, achieve state-of-the-art performance on the Labeled Faces in the Wild (LFW) [7] unconstrained face recognition challenge set. These representations outperform previous state-of-the-art approaches, in spite of requiring less training data and using a conceptually simpler machine learning backend. We argue that such large-scale-search-derived feature sets can play a synergistic role with other computer vision approaches by providing a richer base of features with which to work.},
address = {Santa Barbara, CA, USA},
author = {Cox, David and Pinto, Nicolas},
booktitle = {2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011},
doi = {10.1109/FG.2011.5771385},
editor = {IEEE},
file = {::},
isbn = {9781424491407},
pages = {8--15},
publisher = {IEEE},
title = {{Beyond simple features: A large-scale feature search approach to unconstrained face recognition}},
url = {https://ieeexplore-ieee-org.eaccess.ub.tum.de/abstract/document/5771385},
year = {2011}
}
@article{Liu2017,
abstract = {Distant-supervised relation extraction in-evitably suffers from wrong labeling prob-lems because it heuristically labels rela-tional facts with knowledge bases. Pre-vious sentence level denoise models don't achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable dur-ing training. To this end, we introduce an entity-pair level denoise method which ex-ploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms the state-of-the-art systems.},
author = {Liu, Tianyu and Wang, Kexiang and Chang, Baobao and Sui, Zhifang},
doi = {10.18653/v1/D17-1189},
file = {::},
journal = {Proceedings of the 2017 Conference on Empirical Methods in Natural
          Language Processing},
pages = {1790--1795},
title = {{A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction}},
url = {http://aclweb.org/anthology/D17-1189},
year = {2017}
}
@article{Mimno2012,
abstract = {More than a century of modern Classical scholarship has created a vast archive of journal publi-cations that is now becoming available online. Most of this work currently receives little, if any, attention. The collection is too large to be read by any single person and mostly not of sufficient interest to warrant traditional close reading. This paper presents computational methods for identifying patterns and testing hypotheses about Classics as a field. Such tools can help organize large collections, introduce younger scholars to the history of the field, and act as a " survey, " identifying anomalies that can be explored using more traditional methods.},
author = {Mimno, David and David},
doi = {10.1145/2160165.2160168},
issn = {15564673},
journal = {Journal on Computing and Cultural Heritage},
number = {1},
pages = {1--19},
title = {{Computational Historiography: Data Mining in a Century of Classics Journals}},
url = {http://www.perseus.tufts.edu/publications/02-jocch-mimno.pdf{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2160165.2160168},
volume = {5},
year = {2012}
}
@article{Lau2014a,
abstract = {Topic models based on latent Dirichlet allocation and related methods are used in a range of user-focused tasks including document navigation and trend analysis, but evaluation of the intrinsic quality of the topic model and topics remains an open research area. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation.},
archivePrefix = {arXiv},
arxivId = {1606.05908},
author = {Lau, Jey Han and Newman, David and Baldwin, Timothy},
eprint = {1606.05908},
file = {::},
isbn = {9781632663962},
journal = {Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014)},
number = {Eacl},
pages = {530--539},
title = {{Machine Reading Tea Leaves : Automatically Evaluating Topic Coherence and Topic Model Quality}},
url = {http://www.aclweb.org/anthology/E14-1056},
year = {2014}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
doi = {10.1145/2740908.2742760},
eprint = {1405.4053},
file = {::},
isbn = {9781634393973},
issn = {10495258},
pmid = {9377276},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Gwet2014,
abstract = {The notion of intrarater reliability will be of interest to researchers concerned about the reproducibility of clinical measurements. A rater in this context refers to any data- generating system, which includes individ- uals and laboratories; intrarater reliability is a metric for rater's self-consistency in the scoring of subjects. The importance of data reproducibility stems from the need for scientific inquiries to be based on solid evi- dence. Reproducible clinical measurements are recognized as representing a well-defined characteristic of interest. Reproducibility is a source of concern caused by the extensive manipulation of medical equipment in test laboratories and the complexity of the judg- mental processes involved in clinical data gathering. Grundy},
author = {Gwet, Kilem L.},
doi = {10.1002/9781118596333.ch19},
file = {::},
isbn = {9781118596333},
issn = {047146242X},
journal = {Methods and Applications of Statistics in Clinical Trials},
keywords = {Confidence interval,Intrarater reliability,Randomized block design,Rater factor,Statistical inference},
number = {1},
pages = {340--356},
title = {{Intrarater Reliability}},
volume = {2},
year = {2014}
}
@article{Leth2018,
author = {Leth, Rasmus and Gynther, Karsten},
file = {::},
keywords = {01,03,06,11,12,2018,26,3000 roskilde,accepted,actionable insights,corresponding author 1 email,data-informed decision-making,denmark,dk address,learning analytics,pha,published,ralj,submitted,trekroner forskerpark 4,university college absalon},
number = {3},
pages = {198--221},
title = {{What Constitutes an “ Actionable Insight ” in Learning Analytics ?}},
volume = {5},
year = {2018}
}
@article{Byrt1993,
abstract = {Since the introduction of Cohen's kappa as a chance-adjusted measure of agreement between two observers, several “paradoxes” in its interpretation have been pointed out. The difficulties occur because kappa not only measures agreement but is also affected in complex ways by the presence of bias between observers and by the distributions of data across the categories that are used (“prevalence”). In this paper, new indices that provide independent measures of bias and prevalence, as well as of observed agreement, are defined and a simple formula is derived that expresses kappa in terms of these three indices. When comparisons are made between agreement studies it can be misleading to report kappa values alone, and it is recommended that researchers also include quantitative indicators of bias and prevalence.},
author = {Byrt, Ted and Bishop, Janet and Carlin, John B.},
doi = {10.1016/0895-4356(93)90018-V},
issn = {0895-4356},
journal = {Journal of Clinical Epidemiology},
month = {may},
number = {5},
pages = {423--429},
publisher = {Pergamon},
title = {{Bias, prevalence and kappa}},
url = {https://www.sciencedirect.com/science/article/pii/089543569390018V},
volume = {46},
year = {1993}
}
@inproceedings{Kumar2016,
abstract = {Molecular dynamics simulations of aqueous mixtures of methanol and sorbitol were performed over a wide range of binary composition, density (pressure), and temperature to study the equation of state and solvation of small apolar solutes. Experimentally, methanol is a canonical solubilizing agent for apolar solutes and a protein denaturant in mixed-aqueous solvents; sorbitol represents a canonical ?salting-out? or protein-stabilizing cosolvent. The results reported here show increasing sorbitol concentration under isothermal, isobaric conditions results in monotonic increases in apolar solute excess chemical potential over the range of experimentally relevant temperatures. For methanol at elevated temperatures, increasing cosolvent composition results in monotonically decreasing . However, at lower temperatures exhibits a maximum versus cosolvent concentration, as seen experimentally for Ar in ethanol?water solutions. Both density anomalies and hydrophobic effectscharacterized by temperatures of density maxima and apolar solute solubility minima, respectivelyare suppressed upon addition of either sorbitol or methanol at all temperatures and compositions simulated here. Thus, the contrasting effects of sorbitol and methanol on solute chemical potential cannot be explained by qualitative differences in their ability to enhance or suppress hydrophobic effects. Rather, we find values across a broad range of temperatures and cosolvent composition can be quantitatively explained in terms of isobaric changes in solvent densityi.e., the equation of statealong with the corresponding packing fraction of the solvent. Analysis in terms of truncated preferential interaction parameters highlights that care must be taken in interpreting cosolvent effects on solvation in terms of local preferential hydration.},
address = {San Diego, USA},
author = {Kumar, Ayush and Kohail, Sarah and Kumar, Amit and Ekbal, Asif and Biemann, Chris},
booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
doi = {10.18653/v1/s16-1174},
file = {:Users/felix/Downloads/S16-1174.pdf:pdf},
pages = {1129--1135},
title = {{IIT-TUDA at SemEval-2016 Task 5: Beyond Sentiment Lexicon: Combining Domain Dependency and Distributional Semantics Features for Aspect Based Sentiment Analysis}},
year = {2016}
}
@article{Perone2018,
abstract = {Despite the fast developmental pace of new sentence embedding methods, it is still challenging to find comprehensive evaluations of these different techniques. In the past years, we saw significant improvements in the field of sentence embeddings and especially towards the development of universal sentence encoders that could provide inductive transfer to a wide variety of downstream tasks. In this work, we perform a comprehensive evaluation of recent methods using a wide variety of downstream and linguistic feature probing tasks. We show that a simple approach using bag-of-words with a recently introduced language model for deep context-dependent word embeddings proved to yield better results in many tasks when compared to sentence encoders trained on entailment datasets. We also show, however, that we are still far away from a universal encoder that can perform consistently across several downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1806.06259},
author = {Perone, Christian S. and Silveira, Roberto and Paula, Thomas S.},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1806.06259},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Perone, Silveira, Paula - Unknown - Evaluation of sentence embeddings in downstream and linguistic probing tasks(2).pdf:pdf},
isbn = {9781577357384},
issn = {10495258},
pmid = {18244602},
title = {{Evaluation of sentence embeddings in downstream and linguistic probing tasks}},
url = {http://arxiv.org/abs/1806.06259},
year = {2018}
}
@article{Chang2009,
abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
doi = {10.1.1.100.1089},
eprint = {arXiv:1011.1669v3},
isbn = {9781615679119},
issn = {1098-6596},
journal = {Advances in Neural Information Processing Systems 22},
pages = {288----296},
pmid = {25246403},
title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
url = {http://www.umiacs.umd.edu/{~}jbg/docs/nips2009-rtl.pdf},
year = {2009}
}
@article{Am2013,
abstract = {We propose a unified neural network architecture and learning algorithmthat can be applied to var- ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re- quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Am, Samples},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
file = {::},
isbn = {1532-4435},
issn = {0891-2017},
journal = {October},
keywords = {natural language processing,neural networks},
number = {April},
pages = {2493--2537},
pmid = {1000183096},
title = {{Sitara ™ AM335x ARM {\textregistered} Cortex ™ -A8 Microprocessors ( MPUs )}},
volume = {12},
year = {2013}
}
@article{Makatouni2002,
abstract = {What motivates consumers to buy organic food in the UK?: Results from a qualitative study},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1108/BIJ-10-2012-0068},
author = {Makatouni, Aikaterini},
doi = {10.1108/00070700210425769},
eprint = {/dx.doi.org/10.1108/BIJ-10-2012-0068},
isbn = {10.1108/00070700210425769},
issn = {0007-070X},
journal = {British Food Journal},
number = {3/4/5},
pages = {345--352},
pmid = {42012058},
primaryClass = {http:},
title = {{What motivates consumers to buy organic food in the UK?}},
url = {http://www.emeraldinsight.com/doi/10.1108/00070700210425769},
volume = {104},
year = {2002}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
pages = {785},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Blei2009,
abstract = {Scientists need newtools to explore and browse large collections of schol- arly literature. Thanks to organizations such as JSTOR, which scan and index the original bound archives of many journals, modern scientists can search digital libraries spanning hundreds of years. A scientist, suddenly faced with access to millions of articles in her field, is not satisfied with simple search. Effectively using such collections requires interacting with them in a more structured way: finding articles similar to those of interest, and exploring the collection through the underlying topics that run through it},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.1486v1},
author = {Blei, David M and Lafferty, John D},
doi = {10.1145/1143844.1143859},
eprint = {arXiv:0712.1486v1},
file = {::},
isbn = {1595933832},
issn = {15324435},
journal = {Text Mining: Classification, Clustering, and Applications},
pages = {71--89},
pmid = {9013932},
title = {{Topic Models}},
year = {2009}
}
@article{McCann2017,
abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
archivePrefix = {arXiv},
arxivId = {1708.00107},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
eprint = {1708.00107},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/McCann et al. - 2017 - Learned in Translation Contextualized Word Vectors.pdf:pdf},
issn = {10495258},
number = {Nips},
pages = {1--12},
title = {{Learned in Translation: Contextualized Word Vectors}},
url = {http://arxiv.org/abs/1708.00107},
year = {2017}
}
@article{Aghajanyan2018a,
abstract = {When a bilingual student learns to solve word problems in math, we expect the student to be able to solve these problem in both languages the student is fluent in,even if the math lessons were only taught in one language. However, current representations in machine learning are language dependent. In this work, we present a method to decouple the language from the problem by learning language agnostic representations and therefore allowing training a model in one language and applying to a different one in a zero shot fashion. We learn these representations by taking inspiration from linguistics and formalizing Universal Grammar as an optimization process (Chomsky, 2014; Montague, 1970). We demonstrate the capabilities of these representations by showing that the models trained on a single language using language agnostic representations achieve very similar accuracies in other languages.},
archivePrefix = {arXiv},
arxivId = {1809.08510},
author = {Aghajanyan, Armen and Song, Xia and Tiwary, Saurabh},
eprint = {1809.08510},
file = {::},
pages = {1--10},
title = {{Towards Language Agnostic Universal Representations}},
url = {http://arxiv.org/abs/1809.08510},
year = {2018}
}
@article{Epresentations2018,
abstract = {We present a novel multi-task training approach to learning multilingual dis-tributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. We construct sentence embeddings by processing word embeddings with an LSTM and by taking an average of the outputs. Our architec-ture can transparently use both monolingual and sentence aligned bilingual cor-pora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows com-petitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a low-resource scenario.},
author = {Epresentations, R},
file = {::},
journal = {Iclr2018},
pages = {1--9},
title = {{Multitask Learning of Multilingual Sentence Representations}},
year = {2018}
}
@article{Akhundov2018,
abstract = {We take a practical approach to solving sequence labeling problem assuming unavailability of domain expertise and scarcity of informational and computational resources. To this end, we utilize a universal end-to-end Bi-LSTM-based neural sequence labeling model applicable to a wide range of NLP tasks and languages. The model combines morphological, semantic, and structural cues extracted from data to arrive at informed predictions. The model's performance is evaluated on eight benchmark datasets (covering three tasks: POS-tagging, NER, and Chunking, and four languages: English, German, Dutch, and Spanish). We observe state-of-the-art results on four of them: CoNLL-2012 (English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus (German POS-tagging), and competitive performance on the rest.},
annote = {Summary

Sequence labeling with bi-lstms using byte- and word embeddings together},
archivePrefix = {arXiv},
arxivId = {1808.03926},
author = {Akhundov, Adnan and Trautmann, Dietrich and Groh, Georg},
eprint = {1808.03926},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Akhundov, Trautmann, Groh - 2018 - Sequence Labeling A Practical Approach.pdf:pdf},
keywords = {NER,POS,chunking,lstm,named entity recognition,part-of-speech,shallow parsing,supervised learning},
mendeley-tags = {NER,POS,chunking,lstm,named entity recognition,part-of-speech,shallow parsing,supervised learning},
title = {{Sequence Labeling: A Practical Approach}},
url = {http://arxiv.org/abs/1808.03926},
year = {2018}
}
@misc{BMEL2017,
author = {BMEL},
booktitle = {{\"{O}}kobarometer 2017},
file = {::},
title = {{Was sind die Gr{\"{u}}nde, die Sie dazu veranlassen , Bioprodukte zu kaufen ?}},
url = {https://de.statista.com/statistik/daten/studie/2419/umfrage/bioprodukte-gruende-fuer-den-kauf/},
urldate = {2018-03-16},
year = {2017}
}
@inproceedings{Pratt1991,
abstract = {A touted advantage of symbolic representations is the ease of transferring learned information from one intelligent agent to another. This paper investigates an analogous problem: how to use information from one neural network to help a second network learn a related task. Rather than translate such information into symbolic form (in which it may not be readily expressible), we investigate the direct transfer of information encoded as weights. Here, we focus on how transfer can be used to address the important problem of improving neural network learning speed. First we present an exploratory study of the somewhat surprising effects of pre-setting network weights on subsequent learning. Guided by hypotheses from this study, we sped up back-propagation learning for two speech recognition tasks. By transferring weights from smaller networks trained on sub-tasks, we achieved speedups of up to an order of magnitude compared with training starting with random weights, even taking into account the time to train the smaller networks. We include results on how transfer scales to a large phoneme recognition problem.},
author = {Pratt, Lorien and Mostow, Jack and Kamm, Candance},
booktitle = {AAAI-91 Proceedings},
file = {::},
pages = {584--589},
publisher = {AAAI},
title = {{Direct Transfer of Learned Information Among Neural Networks}},
url = {www.aaai.org},
year = {1991}
}
@article{Friedman2002,
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current “pseudo”-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
author = {Friedman, Jerome H},
doi = {https://doi.org/10.1016/S0167-9473(01)00065-2},
issn = {0167-9473},
journal = {Computational Statistics {\&} Data Analysis},
number = {4},
pages = {367--378},
title = {{Stochastic gradient boosting}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947301000652},
volume = {38},
year = {2002}
}
@misc{Zaiontz2015,
abstract = {There are a number of commonly used, powerful tools for carrying out statistical analyses. The most popular of these are Statistical Package for the Social Sciences (SPSS), Statistical Analysis System (SAS), Stata, Minitab and R. Many people choose to use Excel as their principal analysis tool or as a complement to one of these tools for any of the following reasons: ? It is widely available and so many people already know how to use it ? It is not necessary to incur the cost of yet another tool (as some of the popular tools are quite expensive) ? It is not necessary to learn new methods of manipulating data and drawing graphs ? It provides numerous built-in statistical functions and data analysis tools ? It is much easier to see what is going on since, unlike the more commonly used statistical analysis tools, very little is hidden from the user ? It provides the user with a lot of control and flexibility This},
author = {Zaiontz, Charles},
booktitle = {Succintly},
pages = {117},
title = {{Real Statistics Using Excel}},
url = {www.real-statistics.com},
volume = {1},
year = {2015}
}
@article{Hulpus2013,
abstract = {Automated topic labelling brings benefits for users aiming at analysing and understanding document collections, as well as for search engines targetting at the linkage between groups of words and their inherent topics. Current ap- proaches to achieve this suffer in quality, but we argue their performances might be improved by setting the focus on the structure in the data. Building upon research for con- cept disambiguation and linking to DBpedia, we are taking a novel approach to topic labelling by making use of structured data exposed by DBpedia. We start from the hypothesis that words co-occuring in text likely refer to concepts that belong closely together in the DBpedia graph. Using graph centrality measures, we show that we are able to identify the concepts that best represent the topics. We compara- tively evaluate our graph-based approach and the standard text-based approach, on topics extracted from three corpora, based on results gathered in a crowd-sourcing experiment. Our research shows that graph-based analysis of DBpedia can achieve better results for topic labelling in terms of both precision and topic coverage.},
author = {Hulpus, Ioana and Hayes, Conor and Karnstedt, Marcel and Greene, Derek},
doi = {10.1145/2433396.2433454},
file = {::},
isbn = {9781450318693},
issn = {1541034X},
journal = {Proceedings of the sixth ACM international conference on Web search and data mining - WSDM '13},
pages = {465},
pmid = {22902439},
title = {{Unsupervised graph-based topic labelling using dbpedia}},
url = {http://dl.acm.org/citation.cfm?doid=2433396.2433454},
year = {2013}
}
@article{Friedman00greedyfunction,
author = {Friedman, Jerome H},
journal = {Annals of Statistics},
pages = {1189--1232},
title = {{Greedy Function Approximation: A Gradient Boosting Machine}},
volume = {29},
year = {2000}
}
@article{Newman2010a,
abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on point- wise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
author = {Newman, David and Lau, Jh and Grieser, Karl and Baldwin, Timothy},
file = {::},
isbn = {1932432655},
journal = {{\ldots} Language Technologies: The {\ldots}},
number = {June},
pages = {100--108},
title = {{Automatic evaluation of topic coherence}},
url = {http://dl.acm.org/citation.cfm?id=1858011},
year = {2010}
}
@misc{Smith-Spangler2012,
abstract = {BACKGROUND: The health benefits of organic foods are unclear. PURPOSE: To review evidence comparing the health effects of organic and conventional foods. DATA SOURCES: MEDLINE (January 1966 to May 2011), EMBASE, CAB Direct, Agricola, TOXNET, Cochrane Library (January 1966 to May 2009), and bibliographies of retrieved articles. STUDY SELECTION: English-language reports of comparisons of organically and conventionally grown food or of populations consuming these foods. DATA EXTRACTION: 2 independent investigators extracted data on methods, health outcomes, and nutrient and contaminant levels. DATA SYNTHESIS: 17 studies in humans and 223 studies of nutrient and contaminant levels in foods met inclusion criteria. Only 3 of the human studies examined clinical outcomes, finding no significant differences between populations by food type for allergic outcomes (eczema, wheeze, atopic sensitization) or symptomatic Campylobacter infection. Two studies reported significantly lower urinary pesticide levels among children consuming organic versus conventional diets, but studies of biomarker and nutrient levels in serum, urine, breast milk, and semen in adults did not identify clinically meaningful differences. All estimates of differences in nutrient and contaminant levels in foods were highly heterogeneous except for the estimate for phosphorus; phosphorus levels were significantly higher than in conventional produce, although this difference is not clinically significant. The risk for contamination with detectable pesticide residues was lower among organic than conventional produce (risk difference, 30{\%} [CI, -37{\%} to -23{\%}]), but differences in risk for exceeding maximum allowed limits were small. Escherichia coli contamination risk did not differ between organic and conventional produce. Bacterial contamination of retail chicken and pork was common but unrelated to farming method. However, the risk for isolating bacteria resistant to 3 or more antibiotics was higher in conventional than in organic chicken and pork (risk difference, 33{\%} [CI, 21{\%} to 45{\%}]). LIMITATION: Studies were heterogeneous and limited in number, and publication bias may be present. CONCLUSION: The published literature lacks strong evidence that organic foods are significantly more nutritious than conventional foods. Consumption of organic foods may reduce exposure to pesticide residues and antibiotic-resistant bacteria. PRIMARY FUNDING SOURCE: None.},
author = {Smith-Spangler, Crystal and Brandeau, Margaret L. and Hunter, Grace E. and {Clay Bavinger}, J. and Pearson, Maren and Eschbach, Paul J. and Sundaram, Vandana and Liu, Hau and Schirmer, Patricia and Stave, Christopher and Olkin, Ingram and Bravata, Dena M.},
booktitle = {Annals of Internal Medicine},
doi = {10.7326/0003-4819-157-5-201209040-00007},
isbn = {00034819},
issn = {00034819},
number = {5},
pages = {348--366},
pmid = {22944875},
title = {{Are organic foods safer or healthier than conventional alternatives?: A systematic review}},
volume = {157},
year = {2012}
}
@article{Baxter1997,
abstract = {A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks.},
author = {Baxter, Jonathan},
doi = {https://doi.org/10.1023/A:1007327622663},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Baxter, Pratt, Thrun - 1997 - A BayesianInformation Theoretic Model of Learning to Learn via Multiple Task Sampling.pdf:pdf},
journal = {Machine Learning},
keywords = {Bias learning,Feature Learning,Hierarchichal Bayesian Inference,Information Theory,Neural Networks},
number = {1},
pages = {7--39},
publisher = {Kluwer Academic Publishers},
title = {{A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling *}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FA{\%}3A1007327622663.pdf},
volume = {28},
year = {1997}
}
@inproceedings{Caruana1995,
abstract = {Hinton [6] proposed that generalization in artificial neural nets should improve if nets learn to represent the domain's underlying regularities . Abu-Mustafa's hints work [1] shows that the outputs of a backprop net can be used as inputs through which domain-specific information can be given to the net . We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multi task backprop generalizes better in real domains.},
author = {Caruana, Rich},
booktitle = {Advances in Neural Information Processing Systems 7},
file = {::},
pages = {657--664},
publisher = {MIT Press},
title = {{Learning Many Related Tasks at the Same Time With Backpropagation}},
url = {https://papers.nips.cc/paper/959-learning-many-related-tasks-at-the-same-time-with-backpropagation.pdf https://pdfs.semanticscholar.org/6c3c/5b27e3788a815f67b21eafe8f7f47dc6153d.pdf},
year = {1995}
}
@incollection{Steyvers2007a,
abstract = {Many chapters in this book illustrate that applying a statistical method such as latent semantic analysis (LSA; Landauer {\&} Dumais, 1997; Landauer, Foltz, {\&} Laham, 1998) to large databases can yield insight into human cognition. The LSA approach makes three claims: that semantic information can be derived from a word-document co-occurrence matrix; that dimensionality reduction is an essential part of this derivation; and that words and documents can be represented as points in Euclidean space. This chapter pursues ...},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Steyvers, Mark and Griffiths, Tom},
booktitle = {Handbook of latent semantic analysis},
doi = {10.4324/9780203936399.ch21},
eprint = {1111.6189v1},
file = {::},
isbn = {1135603286},
issn = {1386-4564},
keywords = {a road to meaning,and w,d mcnamara,dennis,eds,in t,kintsch,landauer,latent semantic analysis,laurence erlbaum,probabilistic topic models,s},
number = {7},
pages = {424--440},
pmid = {21362469},
title = {{Probabilistic topic models}},
url = {http://173.236.226.255/tom/papers/SteyversGriffiths.pdf},
volume = {427},
year = {2007}
}
@article{He2017,
abstract = {Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.},
annote = {Summary

unsuppervised aspect extraction. goal is to learn a set of aspect embeddings where each aspect can be interpreted by looking at the nearest word},
author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
doi = {10.18653/v1/P17-1036},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2017 - An Unsupervised Neural Attention Model for Aspect Extraction.pdf:pdf},
isbn = {9781945626753},
journal = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
keywords = {LDA,attention,unsuppervised},
mendeley-tags = {LDA,attention,unsuppervised},
pages = {388--397},
title = {{An Unsupervised Neural Attention Model for Aspect Extraction}},
url = {http://aclweb.org/anthology/P17-1036},
year = {2017}
}
@article{Deerwester1990,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9788578110796},
issn = {10974571},
journal = {Journal of the American Society for Information Science},
number = {6},
pages = {391--407},
pmid = {25246403},
title = {{Indexing by latent semantic analysis}},
volume = {41},
year = {1990}
}
@book{katz2006multivariable,
author = {Katz, M H},
isbn = {9780521549851},
publisher = {Cambridge University Press},
title = {{Multivariable Analysis: A Practical Guide for Clinicians}},
url = {https://books.google.de/books?id=febxciaS83IC},
year = {2006}
}
@article{Rei2017,
abstract = {We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.},
annote = {used multiple tasks and word prediction to improve performance.

LSTM},
archivePrefix = {arXiv},
arxivId = {1704.07156},
author = {Rei, Marek},
eprint = {1704.07156},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Rei - 2017 - Semi-supervised Multitask Learning for Sequence Labeling.pdf:pdf},
month = {apr},
title = {{Semi-supervised Multitask Learning for Sequence Labeling}},
url = {http://arxiv.org/abs/1704.07156},
year = {2017}
}
@article{Pritchard2000,
abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g., seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home.html.},
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Pritchard, J. K. and Stephens, M. and Donnelly, P.},
doi = {10.1111/j.1471-8286.2007.01758.x},
eprint = {0208024},
isbn = {0016-6731},
issn = {0016-6731},
journal = {Genetics},
pages = {945--959},
pmid = {10835412},
primaryClass = {gr-qc},
title = {{Inference of population structure using multilocus genotype data}},
volume = {155},
year = {2000}
}
@article{Teh2006,
abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.6738v2},
author = {Teh, Yee Whye and Jordan, Michael I. and Beal, Matthew J. and Blei, David M.},
doi = {10.1198/016214506000000302},
eprint = {arXiv:1210.6738v2},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Clustering,Hierarchical model,Markov chain Monte Carlo,Mixture model,Nonparametric Bayesian statistics},
number = {476},
pages = {1566--1581},
pmid = {242869700023},
title = {{Hierarchical Dirichlet processes}},
volume = {101},
year = {2006}
}
@article{James2002,
abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state- of-the-art performance.},
author = {James, Eldon R.},
doi = {10.1086/212157},
file = {:Users/felix/Downloads/p160-collobert.pdf:pdf},
issn = {0002-9602},
journal = {American Journal of Sociology},
number = {6},
pages = {769--783},
title = {{Some Implications of Remedial and Preventive Legislation in the United States}},
volume = {18},
year = {2002}
}
@inproceedings{Pontiki2015a,
abstract = {SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster re- search beyond sentence- or text-level senti- ment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews in three domains (restaurants, laptops and ho- tels), and a common evaluation procedure. It attracted 93 submissions from 16 teams},
address = {Denver, USA},
author = {Pontiki, Maria and Galanis, Dimitrios and Papageorgiou, Haris},
booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)},
doi = {10.18653/v1/S15-2082},
file = {:Users/felix/Downloads/S15-2082.pdf:pdf},
pages = {486--495},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2015 Task 12 : Aspect Based Sentiment Analysis}},
year = {2015}
}
@article{Titov2008a,
abstract = {Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals.},
author = {Titov, Ivan and McDonald, Ryan},
doi = {10.1039/b003067h},
isbn = {9781932432046},
issn = {14704358},
journal = {Proceedings of ACL08 HLT},
number = {June},
pages = {308--316},
title = {{A joint model of text and aspect ratings for sentiment summarization}},
url = {http://www.aclweb.org/anthology/P/P08/P08-1036.pdf},
volume = {51},
year = {2008}
}
@inproceedings{Abadi:2016:TSL:3026877.3026899,
address = {Berkeley, CA, USA},
author = {Abadi, Mart$\backslash$'$\backslash$in and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
isbn = {978-1-931971-33-1},
pages = {265--283},
publisher = {USENIX Association},
series = {OSDI'16},
title = {{TensorFlow: A System for Large-scale Machine Learning}},
url = {http://dl.acm.org/citation.cfm?id=3026877.3026899},
year = {2016}
}
@inproceedings{Mimno2011b,
abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Un-fortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional sub- spaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
file = {::},
isbn = {9781937284114},
issn = {1937284115},
keywords = {topic coherence,topic models,topics evaluation},
number = {2},
pages = {262--272},
title = {{Optimizing semantic coherence in topic models}},
year = {2011}
}
@inproceedings{Hong2010a,
abstract = {Social networks such as Facebook, LinkedIn, and Twitter have been a crucial source of information for a wide spectrum of users. In Twitter, popular information that is deemed important by the community propagates through the network. Studying the characteristics of content in the messages becomes important for a number of tasks, such as breaking news detection, personalized message recommendation, friends recommendation, sentiment analysis and others. While many researchers wish to use standard text mining tools to understand messages on Twitter, the restricted length of those messages prevents them from being employed to their full potential. We address the problem of using standard topic models in microblogging environments by studying how the models can be trained on the dataset. We propose several schemes to train a standard topic model and compare their quality and effectiveness through a set of carefully designed experiments from both qualitative and quantitative perspectives. We show that by training a topic model on aggregated messages we can obtain a higher quality of learned model which results in significantly better performance in two real- world classification problems. We also discuss how the state-of- the-artAuthor-Topicmodel fails to model hierarchical relationships between entities in SocialMedia.},
author = {Hong, Liangjie and Davison, Brian D.},
booktitle = {Proceedings of the First Workshop on Social Media Analytics - SOMA '10},
doi = {10.1145/1964858.1964870},
isbn = {9781450302173},
issn = {15278999},
pages = {80--88},
pmid = {282},
title = {{Empirical study of topic modeling in Twitter}},
url = {http://portal.acm.org/citation.cfm?doid=1964858.1964870},
year = {2010}
}
@article{Tang2014,
abstract = {We present a method that learns word em- bedding for Twitter sentiment classifica- tion in this paper. Most existing algorithm- s for learning continuous word represen- tations typically only model the syntactic context of words but ignore the sentimen- t of text. This is problematic for senti- ment analysis as they usually map word- s with similar syntactic context but oppo- site sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment- specific word embedding (SSWE), which encodes sentiment information in the con- tinuous representation of words. Specif- ically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sen- tences or tweets) in their loss function- s. To obtain large scale training corpora, we learn the sentiment-specific word em- bedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS- WE to a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the perfor- mance is further improved by concatenat- ing SSWE with existing feature set.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
doi = {10.3115/1220575.1220648},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9781937284725},
issn = {03029743},
journal = {Acl},
pages = {1555--1565},
pmid = {18487783},
title = {{Learning Sentiment-Specific Word Embedding}},
year = {2014}
}
@article{Falotico2015,
abstract = {The Fleiss' kappa statistic is a well-known index for assessing the reliability of agreement between raters. It is used both in the psychological and in the psychiatric field. Unfortunately, the kappa statistic may behave inconsistently in case of strong agreement between raters, since this index assumes lower values than it would have been expected. The aim of this paper is to propose a new method to avoid this paradox through permutation techniques. Furthermore, we study the problem of kappa confidence intervals and, in particular, we suggest to use Bootstrap confidence intervals free of paradoxes.},
author = {Falotico, Rosa and Quatto, Piero},
doi = {10.1007/s11135-014-0003-1},
file = {::},
issn = {15737845},
journal = {Quality and Quantity},
keywords = {Bootstrap confidence intervals,Fleiss' kappa,Inter-rater agreement,Kappa paradoxes,Monte Carlo simulations},
number = {2},
pages = {463--470},
title = {{Fleiss' kappa statistic without paradoxes}},
url = {https://doi.org/10.1007/s11135-014-0003-1},
volume = {49},
year = {2015}
}
@book{croxton1968applied,
author = {Croxton, F E and Cowden, D J and Klein, S},
isbn = {9780273403159},
publisher = {Pitman},
title = {{Applied General Statistics}},
url = {https://books.google.de/books?id=qaDTAQAACAAJ},
year = {1968}
}
@misc{StatistaSurvey2017,
author = {{Statista Survey}},
file = {::},
title = {{How often do you make or post comments on the following types of digital media ?}},
urldate = {2018-03-16},
year = {2017}
}
@techreport{Conneau2018,
abstract = {Many modern NLP systems rely on word embeddings, previously trained in an un-supervised manner on large corpora, as base features. Efforts to obtain embed-dings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsu-pervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsu-pervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features , which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available 1 .},
archivePrefix = {arXiv},
arxivId = {1705.02364v5},
author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and {Lo¨ıc Barrault}, Lo¨ıc and Bordes, Antoine},
eprint = {1705.02364v5},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Conneau et al. - 2018 - Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.pdf:pdf},
keywords = {Infersent,LSTM,NLI,Natural language inference,facebook,sentence embeddings,sentence encoding,supervised learning},
mendeley-tags = {Infersent,LSTM,NLI,Natural language inference,facebook,sentence embeddings,sentence encoding,supervised learning},
title = {{Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}},
url = {https://www.github.com/facebookresearch/InferSent},
year = {2018}
}
@article{Cer2018,
abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
archivePrefix = {arXiv},
arxivId = {1803.11175},
author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St. and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
doi = {10.1007/s11423-014-9358-1},
eprint = {1803.11175},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Cer et al. - Unknown - Universal Sentence Encoder(2).pdf:pdf},
issn = {1042-1629},
title = {{Universal Sentence Encoder}},
year = {2018}
}
@article{Ayyad2018a,
author = {Ayyad, Ahmed},
file = {::},
title = {{Aspect-based Sentiment Analysis}},
year = {2018}
}
@phdthesis{Ayyad2018,
author = {Ayyad, Ahmed},
file = {::},
pages = {12},
school = {Techische Universit{\"{a}}t M{\"{u}}nchen},
title = {{Aspect-based Sentiment Analysis}},
type = {Guided Research},
year = {2018}
}
@misc{Dykeman2016,
abstract = {When I started working on understanding generative models, I didn't find any resources that gave a good, high level, intuitive overview of variational autoencoders. Some great resources exist for understanding them in detail and seeing the math behind them. In particular “Tutorial on Variational Autoencoders” by Carl Doersch covers the same topics as this post, but as the author notes, there is some abuse of notation in that article, and the treatment is more abstract then what I'll go for here. Here, I'll carry the example of a variational autoencoder for the MNIST digits dataset throughout, using concrete examples for each concept. Hopefully by reading this article you can get a general idea of how Variational Autoencoders work before tackling them in detail.},
author = {Dykeman, Isaac},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Dykeman - 2016 - Conditional Variational Autoencoders.pdf:pdf},
keywords = {VAR,Variational Autoencoder,unsuppervised},
mendeley-tags = {VAR,Variational Autoencoder,unsuppervised},
title = {{Conditional Variational Autoencoders}},
url = {http://ijdykeman.github.io/ml/2016/12/21/cvae.html},
urldate = {2018-11-08},
year = {2016}
}
@article{Jurafsky2009,
abstract = {This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora. Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.},
author = {Jurafsky, Daniel and Martin, James H},
doi = {10.1162/089120100750105975},
file = {::},
isbn = {0130950696},
issn = {08912017},
journal = {Speech and Language Processing An Introduction to Natural Language Processing Computational Linguistics and Speech Recognition},
pages = {0--934},
title = {{Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/089120100750105975},
volume = {21},
year = {2009}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0262661160},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{LeCun;1990,
abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1{\%} error rate and about a 9{\%} reject rate on zipcode digits provided by the U.S. Postal Service.},
archivePrefix = {arXiv},
arxivId = {1004.3732},
author = {LeCun;, Y and Boser, B and Denker, John S and Howard, R E and Habbard, W and Jackel, L D},
doi = {10.1111/dsu.12130},
eprint = {1004.3732},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/LeCun et al. - 1990 - Handwritten Digit Recognition with a Back-Propagation Network.pdf:pdf},
isbn = {1-55860-100-7},
issn = {1524-4725},
journal = {Advances in Neural Information Processing Systems},
pages = {396--404},
pmid = {23301817},
title = {{Handwritten Digit Recognition with a Back-Propagation Network}},
url = {http://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network.pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.5076{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.5076{\&}rep=rep1{\&}type=pdf},
year = {1990}
}
@incollection{Manning2009,
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar},
booktitle = {Online},
doi = {10.1109/LPT.2009.2020494},
eprint = {0521865719 9780521865715},
file = {::},
isbn = {0521865719},
issn = {13864564},
keywords = {keyword},
pages = {1},
pmid = {10575050},
title = {{An Introduction to Information Retrieval}},
url = {http://dspace.cusat.ac.in/dspace/handle/123456789/2538},
year = {2009}
}
@article{Bandorski2016,
abstract = {The top-performing Question–Answering (QA) systems have been of two types: consistent, solid, well-established and multi-faceted systems that do well year after year, and ones that come out of nowhere employ- ing totally innovative approaches and which out-perform almost every- body else. This article examines both types of system in depth. We establish what a “typical” QA-system looks like, and cover the com- monly used approaches by the component modules. Understanding this will enable any proficient system developer to build his own QA-system. Fortunately there are many components available for free from their developers to make this a reasonable expectation for a graduate-level project. We also look at particular systems that have performed well and which employ interesting and innovative approaches.},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Bandorski, Dirk and Kurniawan, Niehls and Baltes, Peter and Hoeltgen, Reinhard and Hecker, Matthias and Stunder, Dominik and Keuchel, Martin},
doi = {10.3748/wjg.v22.i45.9898},
eprint = {0112017},
file = {::},
isbn = {1601981503},
issn = {22192840},
journal = {World Journal of Gastroenterology},
keywords = {Aspiration,Contraindications,Magnetic,Pacemaker,Pregnancy,Stenosis,Video capsule endoscopy},
number = {45},
pages = {9898--9908},
pmid = {191},
primaryClass = {cs},
title = {{Contraindications for video capsule endoscopy}},
volume = {22},
year = {2016}
}
@article{DBLP:journals/corr/abs-1708-00055,
archivePrefix = {arXiv},
arxivId = {1708.00055},
author = {Cer, Daniel M and Diab, Mona T and Agirre, Eneko and Lopez-Gazpio, I{\~{n}}igo and Specia, Lucia},
eprint = {1708.00055},
journal = {CoRR},
title = {{SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation}},
url = {http://arxiv.org/abs/1708.00055},
volume = {abs/1708.0},
year = {2017}
}
@article{Ruder2017,
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
eprint = {1706.05098},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural Networks.pdf:pdf},
month = {jun},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
url = {http://arxiv.org/abs/1706.05098},
year = {2017}
}
@article{JyotiBora2014,
abstract = {K-means algorithm is a very popular clustering algorithm which is famous for its simplicity. Distance measure plays a very important rule on the performance of this algorithm. We have different distance measure techniques available. But choosing a proper technique for distance calculation is totally dependent on the type of the data that we are going to cluster. In this paper an experimental study is done in Matlab to cluster the iris and wine data sets with different distance measures and thereby observing the variation of the performances shown.},
author = {{Jyoti Bora}, Dibya and {Kumar Gupta}, Anil},
file = {::},
journal = {International Journal of Computer Science and Information Technologies},
keywords = {-Clustering,Iris,K Means,Matlab,Wine},
number = {2},
pages = {2501--2506},
title = {{Effect of Different Distance Measures on the Performance of K-Means Algorithm: An Experimental Study in Matlab}},
url = {https://arxiv.org/ftp/arxiv/papers/1405/1405.7471.pdf},
volume = {5},
year = {2014}
}
@inproceedings{Mei2007,
abstract = {In this paper, we define the problem of topic-sentiment analysis on Weblogs and propose a novel probabilistic model to capture the mixture of topics and sentiments simultaneously. The proposed Topic-Sentiment Mixture (TSM) model can reveal the latent topical facets in a Weblog collection, the subtopics in the results of an ad hoc query, and their associated sentiments. It could also provide general sentiment models that are applicable to any ad hoc topics. With a specifically designed HMM structure, the sentiment models and topic models estimated with TSM can be utilized to extract topic life cycles and sentiment dynamics. Empirical experiments on different Weblog datasets show that this approach is effective for modeling the topic facets and sentiments and extracting their dynamics from Weblog collections. The TSM model is quite general; it can be applied to any text collections with a mixture of topics and sentiments, thus has many potential applications, such as search result summarization, opinion tracking, and user behavior prediction.},
annote = {Presents a joined model that learns topics and sentiments from webblogs.},
author = {Mei, Qiaozhu and Ling, Xu and Wondra, Matthew and Su, Hang and Zhai, ChengXiang},
booktitle = {Proceedings of the 16th international conference on World Wide Web - WWW '07},
doi = {10.1145/1242572.1242596},
file = {::},
isbn = {9781595936547},
issn = {1595936548},
pages = {171},
title = {{Topic sentiment mixture}},
url = {http://portal.acm.org/citation.cfm?doid=1242572.1242596},
year = {2007}
}
@article{Zhang2015,
abstract = {This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.},
archivePrefix = {arXiv},
arxivId = {1502.01710},
author = {Zhang, Xiang and LeCun, Yann},
doi = {10.1063/1.4906785},
eprint = {1502.01710},
file = {::},
isbn = {0123456789},
issn = {2166-532X},
pmid = {25246403},
title = {{Text Understanding from Scratch}},
url = {http://arxiv.org/abs/1502.01710},
year = {2015}
}
@misc{Grosse1984,
abstract = {Under the common duct stones of cases with cholecystectomy, operated three and more years before, pigment-stones are significantly more frequent than under the cases, which are not operated or operated just before. Cholesterol stones in the common duct after cholecystectomy are almost overlooked stones, but pigment stones often develop after cholecystectomy.},
author = {Grosse, H},
booktitle = {Deutsche Zeitschrift fur Verdauungs- und Stoffwechselkrankheiten},
doi = {https://www.tensorflow.org/tutorials/representation/word2vec},
issn = {0012-1053},
number = {4},
pages = {161--6},
pmid = {6479090},
title = {{[Structural differences between biliary calculi overlooked at cholecystectomy or developing later].}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6479090},
urldate = {2019-02-17},
volume = {44},
year = {1984}
}
@article{Peters2019,
abstract = {While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.},
archivePrefix = {arXiv},
arxivId = {1903.05987},
author = {Peters, Matthew and Ruder, Sebastian and Smith, Noah A},
eprint = {1903.05987},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Peters, Ruder, Smith - Unknown - To Tune or Not to Tune Adapting Pretrained Representations to Diverse Tasks.pdf:pdf},
title = {{To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks}},
url = {https://arxiv.org/pdf/1903.05987.pdf http://arxiv.org/abs/1903.05987},
year = {2019}
}
@article{He2016,
abstract = {Building a successful recommender system depends on understanding both the dimensions of people's preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difficult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users' fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform state-of-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset.},
archivePrefix = {arXiv},
arxivId = {1602.01585},
author = {He, Ruining and McAuley, Julian},
doi = {10.1145/2872427.2883037},
eprint = {1602.01585},
file = {::},
isbn = {9781450341431},
keywords = {fashion evolution,personalized ranking,recommender systems},
title = {{Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering}},
url = {http://arxiv.org/abs/1602.01585{\%}0Ahttp://dx.doi.org/10.1145/2872427.2883037},
year = {2016}
}
@article{Jurafsky2009,
abstract = {Dave Bowman: Open the pod bay doors, HAL. HAL: Im sorry Dave, Im afraid I cant do that. Kubrick, Stanley Clarke, Arthur C},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jurafsky, Daniel and Martin, James H},
doi = {10.1162/089120100750105975},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0130950696},
issn = {08912017},
journal = {Speech and Language Processing An Introduction to Natural Language Processing Computational Linguistics and Speech Recognition},
pages = {0--934},
pmid = {19878769},
title = {{Speech and Language Processing}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/089120100750105975},
volume = {21},
year = {2009}
}
@inproceedings{Huettner2000,
abstract = {This prototype system demonstrates a novel method of document analysis and management, based on a combination of techniques from NLP and fuzzy logic. Since the central technique we use from NLP is semantic typing, we refer to this approach as fuzzy typing for document management. The fuzzy typing approach is general in scope and can be applied to many different kinds of analysis. In this prototype, we illustrate its use in analyzing affect. At a basic level, it involves: • Isolating a vocabulary of words belonging to a metalinguistic domain (here, affect or emotion) • Using multiple categorizations and scalar metrics to represent the meaning of each word in that domain • Computing profiles for texts based on the categorizations and scores of their component domain words • Manipulating the profiles to categorize, differentiate, cluster, match, or visualize the texts Our affect lexicon was generated from a list of roughly 4,000 English words denoting or connoting affect – from abhor and abject to yen and yucky. We created a small set of semantic categories, representing basic, core emotions (e.g., anger, happiness, fear) as well as some recurring abstract themes (e.g., superiority, violence, death). Each word in the lexicon is assigned to as many categories as necessary to capture all aspects of its meaning. Ambiguous words are not disambiguated, but simply assigned to all categories relevant to any of their meanings. Each association between a domain word and a category is assigned a numerical centrality ranging from 0 to 1, representing the degree of relatedness between the word and the category. It is also assigned a numerical intensity, representing the strength of the affect conveyed by the wor.},
author = {Huettner, A and Subasic, P},
booktitle = {CL 2000 Companion Volume: Tutorial Abstracts and Demonstration Notes},
file = {:Users/felix/Downloads/huettner.pdf:pdf},
pages = {26--27},
title = {{Fuzzy typing for document management}},
url = {http://www.cs.jhu.edu/{~}yarowsky/acl2000/tutdemo/huettner.pdf},
year = {2000}
}
@book{kurdi2018natural,
author = {Kurdi, M Z},
isbn = {9781848219212},
number = {Bd. 2},
publisher = {Wiley},
series = {Cognitive Science},
title = {{Natural Language Processing and Computational Linguistics 2: Semantics, Discourse and Applications}},
url = {https://books.google.de/books?id=jdl7jgEACAAJ},
year = {2018}
}
@article{Stanic2009,
abstract = {This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization. The latter are borne out by the recent results of the SUMMAC conference in the evaluation of summarization systems. However, the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection.},
author = {Stanic, Elena and Lipavsky, Corina},
doi = {10.1145/290941.291025},
isbn = {1581130155},
issn = {01635840 (ISSN)},
journal = {Education},
pages = {599},
title = {{Atlas of grapich designers}},
year = {2009}
}
@inproceedings{Claesen2015,
abstract = {We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.},
address = {Agadir},
archivePrefix = {arXiv},
arxivId = {1502.02127},
author = {Claesen, Marc and {De Moor}, Bart},
booktitle = {MIC 2015: The XI Metaheuristics International Conference},
eprint = {1502.02127},
file = {::},
keywords = {()},
pages = {14----19},
title = {{Hyperparameter Search in Machine Learning}},
url = {https://www.codalab.org/competitions/2321. http://arxiv.org/abs/1502.02127},
year = {2015}
}
@phdthesis{Rajat2017,
abstract = {Distant supervision is an efficient approach to automatically label data for relation extraction task. This work introduces a novel approach for the task of relation extraction using distant supervision. Building upon the success of deep learning, our approach utilizes bidirectional LSTMs, multi-level attention network and pre-trained word embeddings to solve the problem. By incorporating several best practices such as Xavier initialization, learning rate decay, dropout, etc. our model was able to achieve state-of-the-art performance on NYT 2010 relation extraction dataset. Moreover, the work discusses problems and possible solutions of applying relation extraction on real world dataset.},
author = {Rajat, Jain},
file = {::},
isbn = {1253978748590},
title = {{Relation Extraction and Classification using Machine Learning}},
url = {http://www.ii.uib.no/publikasjoner/texrap/pdf/2008-367.pdf},
year = {2017}
}
@article{Cer2018,
abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
archivePrefix = {arXiv},
arxivId = {1803.11175},
author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St. and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
doi = {10.1007/s11423-014-9358-1},
eprint = {1803.11175},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Cer et al. - Unknown - Universal Sentence Encoder(2).pdf:pdf},
issn = {1042-1629},
title = {{Universal Sentence Encoder}},
year = {2018}
}
@unpublished{Ramsundar2015,
abstract = {Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process.},
annote = {more tasks -{\textgreater} better performance compared to single},
archivePrefix = {arXiv},
arxivId = {1502.02072},
author = {Ramsundar, Bharath and Kearnes, Steven and Riley, Patrick and Webster, Dale and Konerding, David and Pande, Vijay},
eprint = {1502.02072},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Ramsundar et al. - 2015 - Massively Multitask Networks for Drug Discovery.pdf:pdf},
month = {feb},
title = {{Massively Multitask Networks for Drug Discovery}},
url = {http://arxiv.org/abs/1502.02072},
year = {2015}
}
@article{Popescul2000,
abstract = {Automatically labeling document clusters with words which indicate their topics is difficult to do well. The most commonly used method, labeling with the most frequent words in the clusters, ends up using many words that are virtually void of descriptive power even after traditional stop words are removed. Another method, labeling with the most predictive words, often includes rather obscure words. We present two methods of labeling document clusters motivated by the model that words are generated by a hierarchy of mixture components of varying generality. The first method assumes existence of a document hierarchy (manually constructed or resulting from a hierarchical clustering algorithm) and uses a X2 test of significance to detect different word usage across categories in the hierarchy. The second method selects words which both occur frequently in a cluster and effectively discriminate the given cluster from the other clusters. We compare these methods on abstracts of documents selected from a subset of the hierarchy of the Cora search engine for computer science research papers. Labels produced by our methods showed superior results to the commonly employed methods.},
author = {Popescul, Alexandrin and Ungar, Lyle H},
journal = {Technical Report},
pages = {1--16},
title = {{Automatic Labeling of Document Clusters}},
year = {2000}
}
@article{Cohen1960,
annote = {doi: 10.1177/001316446002000104},
author = {Cohen, Jacob},
doi = {10.1177/001316446002000104},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
month = {apr},
number = {1},
pages = {37--46},
publisher = {SAGE Publications Inc},
title = {{A Coefficient of Agreement for Nominal Scales}},
url = {https://doi.org/10.1177/001316446002000104},
volume = {20},
year = {1960}
}
@article{Liu2018,
abstract = {We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
archivePrefix = {arXiv},
arxivId = {1801.10198},
author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
doi = {arXiv:1801.10198v1},
eprint = {1801.10198},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - Unknown - GENERATING WIKIPEDIA BY SUMMARIZING LONG SEQUENCES.pdf:pdf},
issn = {1006-7248},
pages = {1--18},
pmid = {23852056},
title = {{Generating Wikipedia by Summarizing Long Sequences}},
url = {http://arxiv.org/abs/1801.10198},
year = {2018}
}
@article{Uebersax1987,
author = {Uebersax, John S},
file = {::},
number = {1},
pages = {140--146},
title = {{Kapp{\_}and{\_}decision{\_}making{\_}models.pdf}},
volume = {101},
year = {1987}
}
@inproceedings{Ruder2016,
abstract = {Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources.},
address = {Austin, Texas},
archivePrefix = {arXiv},
arxivId = {1609.02745},
author = {Ruder, Sebastian and Ghaffari, Parsa and Breslin, John G},
booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
eprint = {1609.02745},
file = {:Users/felix/Downloads/1609.02745.pdf:pdf},
pages = {999--1005},
publisher = {Association for Computational Linguistics},
title = {{A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis}},
url = {http://arxiv.org/abs/1609.02745},
year = {2016}
}
@techreport{Perone,
abstract = {Despite the fast developmental pace of new sentence embedding methods, it is still challenging to find comprehensive evaluations of these different techniques. In the past years, we saw significant improvements in the field of sentence embeddings and especially towards the development of universal sentence encoders that could provide inductive transfer to a wide variety of downstream tasks. In this work, we perform a comprehensive evaluation of recent methods using a wide variety of downstream and linguistic feature probing tasks. We show that a simple approach using bag-of-words with a recently introduced language model for deep context-dependent word embeddings proved to yield better results in many tasks when compared to sentence encoders trained on entailment datasets. We also show, however, that we are still far away from a universal encoder that can perform consistently across several downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1806.06259v1},
author = {Perone, Christian S and Silveira, Roberto and Paula, Thomas S},
eprint = {1806.06259v1},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Perone, Silveira, Paula - Unknown - Evaluation of sentence embeddings in downstream and linguistic probing tasks(2).pdf:pdf},
title = {{Evaluation of sentence embeddings in downstream and linguistic probing tasks}},
url = {https://arxiv.org/pdf/1806.06259.pdf}
}
@phdthesis{Rajat2017,
abstract = {Distant supervision is an efficient approach to automatically label data for relation extraction task. This work introduces a novel approach for the task of relation extraction using distant supervision. Building upon the success of deep learning, our approach utilizes bidirectional LSTMs, multi-level attention network and pre-trained word embeddings to solve the problem. By incorporating several best practices such as Xavier initialization, learning rate decay, dropout, etc. our model was able to achieve state-of-the-art performance on NYT 2010 relation extraction dataset. Moreover, the work discusses problems and possible solutions of applying relation extraction on real world dataset.},
author = {Rajat, Jain},
file = {::},
isbn = {1253978748590},
title = {{Relation Extraction and Classification using Machine Learning}},
url = {http://www.ii.uib.no/publikasjoner/texrap/pdf/2008-367.pdf},
year = {2017}
}
@inproceedings{Caruana1995a,
abstract = {Hinton [6] proposed that generalization in artificial neural nets should improve if nets learn to represent the domain's underlying regularities. Abu-Mustafa's hints work [1] shows that the outputs of a backprop net can be used as inputs through which domain-specific information can be given to the net. We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multi task backprop generalizes better in real domains.},
author = {Caruana, Rich},
booktitle = {Advances in Neural Information Processing Systems 7},
editor = {G, Tesauro and Touretzky, D. S. and Leen, T. K.},
file = {::},
pages = {657----664},
publisher = {MIT Press},
title = {{Learning Many Related Tasks at the Same Time With Backpropagation}},
url = {https://papers.nips.cc/paper/959-learning-many-related-tasks-at-the-same-time-with-backpropagation.pdf},
year = {1995}
}
@article{Pham2015,
abstract = {We propose a novel approach to learning dis-tributed representations of variable-length text sequences in multiple languages simultane-ously. Unlike previous work which often de-rive representations of multi-word sequences as weighted sums of individual word vec-tors, our model learns distributed representa-tions for phrases and sentences as a whole. Our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efficiently encode meaning-equivalent text sequences of multi-ple languages in the same semantic space. Our learned embeddings achieve state-of-the-art performance in the often used crosslingual document classification task (CLDC) with an accuracy of 92.7 for English to German and 91.5 for German to English. By learning text sequence representations as a whole, our model performs equally well in both classifi-cation directions in the CLDC task in which past work did not achieve.},
author = {Pham, Hieu and Luong, Minh-Thang and Manning, Christopher D.},
file = {::},
journal = {Workshop on Vector Modeling for NLP},
pages = {88--94},
title = {{Learning Distributed Representations for Multilingual Text Sequences}},
year = {2015}
}
@article{Bergmanis2018,
abstract = {The main motivation for developing context-sensitive lemmatizers is to improve perfor-mance on unseen and ambiguous words. Yet previous systems have not carefully evaluated whether the use of context actually helps in these cases. We introduce Lematus, a lemma-tizer based on a standard encoder-decoder ar-chitecture, which incorporates character-level sentence context. We evaluate its lemmatiza-tion accuracy across 20 languages in both a full data setting and a lower-resource setting with 10k training examples in each language. In both settings, we show that including con-text significantly improves results against a context-free version of the model. Context helps more for ambiguous words than for un-seen words, though the latter have a greater effect on overall performance differences be-tween languages. We also compare to three previous context-sensitive lemmatization sys-tems, which all use pre-extracted edit trees as well as hand-selected features and/or addi-tional sources of information such as tagged training data. Without using any of these, our context-sensitive model outperforms the best competitor system (Lemming) in the full-data setting, and performs on par in the lower-resource setting.},
author = {Bergmanis, Toms and Goldwater, Sharon},
doi = {10.18653/v1/n18-1126},
file = {::},
pages = {1391--1400},
title = {{Context Sensitive Neural Lemmatization with Lematus}},
year = {2018}
}
@inproceedings{Blitzer2007,
abstract = {Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30{\%} over the original SCL algorithm and 46{\%} over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.},
address = {Prague, Czech Republic},
annote = {introduction
domain adaptation},
author = {Blitzer, John and Dredze, Mark and Pereira, Fernando},
booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Blitzer, Dredze, Pereira - Unknown - Biographies, Bollywood, Boom-boxes and Blenders Domain Adaptation for Sentiment Classification(2).pdf:pdf},
pages = {440--447},
publisher = {Association for Computational Linguistics},
title = {{Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification}},
url = {http://ida.},
year = {2007}
}
@inproceedings{Xu2003,
abstract = {In this paper, we propose a novel document clustering method based on the non-negative factorization of the term- document matrix of the given document corpus. In the la- tent semantic space derived by the non-negative matrix fac- torization (NMF), each axis captures the base topic of a par- ticular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value. Our experimental evalua- tions show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clus- tering methods not only in the easy and reliable derivation of document clustering results, but also in document clus- tering accuracies.},
archivePrefix = {arXiv},
arxivId = {1410.0993},
author = {Xu, Wei and Liu, Xin and Gong, Yihong},
booktitle = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval  - SIGIR '03},
doi = {10.1145/860484.860485},
eprint = {1410.0993},
isbn = {1581136463},
issn = {01635840},
pages = {267},
title = {{Document clustering based on non-negative matrix factorization}},
url = {http://portal.acm.org/citation.cfm?doid=860435.860485},
year = {2003}
}
@inproceedings{McAuley2013a,
abstract = {Recommending products to consumers means not only understanding their tastes, but also understanding their level of experience. For example, it would be a mistake to recommend the iconic film Seven Samurai simply because a user enjoys other action movies; rather, we might conclude that they will eventually enjoy it -- once they are ready. The same is true for beers, wines, gourmet foods -- or any products where users have acquired tastes: the `best' products may not be the most `accessible'. Thus our goal in this paper is to recommend products that a user will enjoy now, while acknowledging that their tastes may have changed over time, and may change again in the future. We model how tastes change due to the very act of consuming more products -- in other words, as users become more experienced. We develop a latent factor recommendation system that explicitly accounts for each user's level of experience. We find that such a model not only leads to better recommendations, but also allows us to study the role of user experience and expertise on a novel dataset of fifteen million beer, wine, food, and movie reviews.},
address = {Rio de Janeiro, Brazil},
archivePrefix = {arXiv},
arxivId = {1303.4402},
author = {McAuley, Julian and Leskovec, Jure},
booktitle = {WWW '13 Proceedings of the 22Nd International Conference on World Wide Web},
doi = {10.1145/2488388.2488466},
eprint = {1303.4402},
file = {::},
keywords = {expertise,recommender systems,user modeling},
month = {mar},
pages = {897----908},
publisher = {ACM},
title = {{From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews}},
url = {http://arxiv.org/abs/1303.4402},
year = {2013}
}
@inproceedings{Tong2001,
author = {Tong, Richard M.},
booktitle = {Working Notes of the ACM SIGIR 2001 Workshop on Operational Text Classification},
keywords = {analysis mining opinion sentiment},
title = {{An operational system for detecting and tracking opinions in on-line discussion}},
year = {2001}
}
@article{Boutsidis2008,
abstract = {We describe Nonnegative Double Singular Value Decomposition (NNDSVD), a new method designed to enhance the initialization stage of nonnegative matrix factorization (NMF). NNDSVD can readily be combined with existing NMF algorithms. The basic algorithm contains no randomization and is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. Simple practical variants for NMF with dense factors are described. NNDSVD is also well suited to initialize NMF algorithms with sparse factors. Many numerical examples suggest that NNDSVD leads to rapid reduction of the approximation error of many NMF algorithms. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Boutsidis, C. and Gallopoulos, E.},
doi = {10.1016/j.patcog.2007.09.010},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Low rank,NMF,Nonnegative matrix factorization,Perron-Frobenius,SVD,Singular value decomposition,Sparse NMF,Sparse factorization,Structured initialization},
number = {4},
pages = {1350--1362},
title = {{SVD based initialization: A head start for nonnegative matrix factorization}},
volume = {41},
year = {2008}
}
@article{Loshin2013,
abstract = {This chapter looks at business problems suited for graph analytics, what differentiates the problems from traditional approaches, and considerations for discovery vs. search analyses. We discuss the graph model (vertices and edges), as well as graph analytics. The representation of a graph model using triple notation makes it particularly suited to a big data application model. In addition, we discuss the difference between the use of a traditional data warehouse model for reporting and querying, versus the use of a graph model for discovery analytics. We look at some use cases for graph analytics (health care quality, cybersecurity, correlation analysis), and provide some solution approaches. We then look at the characteristics of a platform to be used for graph analytics, including seamless data intake, high-speed I/O, standards-based representations, and inferencing, multithreading, large memory, among others.},
author = {Loshin, David and Loshin, David},
doi = {10.1016/B978-0-12-417319-4.00010-7},
isbn = {978-0-12-417319-4},
journal = {Big Data Analytics},
month = {jan},
pages = {91--103},
publisher = {Morgan Kaufmann},
title = {{Using Graph Analytics for Big Data}},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000107},
year = {2013}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, Jj},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:Users/felix/Downloads/2604.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{LONG SHORT-TERM MEMORY}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit{\%}0Ahttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@article{Berry2005a,
abstract = {In this study,we apply a non-negative matrix factorization approach for the extraction and detection of concepts or topics from electronic mail messages. For the publicly released Enron electronic mail collection, we encode sparse term-by-message matrices and use a lowrank non-negative matrix factorization algorithm to preserve natural data non-negativity and avoid subtractive basis vector and encoding interactions present in techniques such as principal component analysis. Results in topic detection and message clustering are discussed in the context of published Enron business practices and activities, and benchmarks addressing the computational complexity of our approach are provided. The resulting basis vectors and matrix projections of this approach can be used to identify and monitor underlying semantic features (topics) and message clusters in a general or high-level way without the need to read individual electronic mail messages. Keywords: electronic mail, Enron collection, non-negative matrix factorization, surveillance, topic detection, constrained},
author = {Berry, Micheal W. and Browne, Murray},
doi = {10.1007/s10588-005-5380-5},
file = {::},
issn = {1381298X},
journal = {Computational {\&} Mathematical Organization Theory},
keywords = {constrained least squares,electronic mail,enron collection,non-negative matrix factorization,surveillance,topic detection},
number = {December 1979},
pages = {249--264},
title = {{Email surveillance using non-negative matrix factorization}},
url = {http://link.springer.com/article/10.1007/s10588-005-5380-5},
volume = {11},
year = {2005}
}
@article{Joulin2018,
abstract = {Continuous word representations, learned on different languages, can be aligned with remarkable precision. Using a small bilingual lexicon as training data, learning the linear transformation is often formulated as a regression problem using the square loss. The obtained mapping is known to suffer from the hubness problem, when used for retrieval tasks (e.g. for word translation). To address this issue, we propose to use a retrieval criterion instead of the square loss for learning the mapping. We evaluate our method on word translation, showing that our loss function leads to state-of-the-art results, with the biggest improvements observed for distant language pairs such as English-Chinese.},
archivePrefix = {arXiv},
arxivId = {1804.07745},
author = {Joulin, Armand and Bojanowski, Piotr and Mikolov, Tomas and Grave, Edouard},
eprint = {1804.07745},
file = {::},
title = {{Improving Supervised Bilingual Mapping of Word Embeddings}},
url = {http://arxiv.org/abs/1804.07745},
year = {2018}
}
@article{Iyer1999,
abstract = {Conventional neural network training algorithms often get stuck in local minima. To find the global optimum, training is conventionally repeated with 10, or so, random starting values for the weights. Here we develop an analytical procedure to determine how many times a neural network needs to be trained, with random starting weights, to ensure that the best of those is within a desirable lower percentile of all possible trainings, with a certain level of confidence. The theoretical developments are validated by experimental results. While applied to neural network training, the method is generally applicable to nonlinear optimization.},
author = {Iyer, Mahesh S. and Rhinehart, R. Russell},
doi = {10.1109/72.750573},
file = {::},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Neural-network training,Steady-state identification,Weakest-link-in-a-chain},
number = {2},
pages = {427--432},
title = {{A method to determine the required number of neural-network training repetitions}},
volume = {10},
year = {1999}
}
@article{Sievert2014,
abstract = {We present LDAvis, a web-based interactive visualization of topics estimated using Latent Dirichlet Allocation that is built using a combination of R and D3. Our visualization provides a global view of the topics (and how they differ from each other), while at the same time allowing for a deep inspection of the terms most highly associated with each individual topic. First, we propose a novel method for choosing which terms to present to a user to aid in the task of topic interpretation, in which we define the relevance of a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, we describe LDAvis, our visualization system that allows users to flexibly explore topic-term relationships using relevance to better understand a fitted LDA model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sievert, Carson and Shirley, Kenneth},
doi = {10.1.1.100.1089},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {1932432655},
issn = {10495258},
journal = {Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces},
pages = {63--70},
pmid = {25246403},
title = {{LDAvis: A method for visualizing and interpreting topics}},
url = {http://www.aclweb.org/anthology/W/W14/W14-3110},
year = {2014}
}
@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {::},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/felix/Library/Application Support/Mendeley Desktop/Downloaded/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge(2).pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://dx.doi.org/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@article{Vibration2002a,
author = {Vibration, Journal O F},
issn = {1566-5283},
keywords = {blasting,d roadway excavation,energy,hht method,seismic wave,time and frequency},
number = {August 2011},
title = {{1 Hilbert {\_} Huang}},
year = {2002}
}
@article{PowersDavid2011,
abstract = {Commonly used evaluation measures including Recall, Precision, F-Measure and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case.},
author = {{Powers, David}, M W},
doi = {10.1.1.214.9232},
file = {::},
isbn = {2229-3981},
issn = {2229-3981},
journal = {Journal of Machine Learning Technologies},
keywords = {Correlation,DeltaP,F-Measure,Informedness and Markedness,Kappa,Rand Accuracy,Recall and Precision,Significance},
number = {1},
pages = {37--63},
pmid = {12345678},
title = {{Evaluation: From Precision, Recall and F-Measure To Roc, Informedness, Markedness {\&} Correlation}},
url = {http://www.bioinfopublication.org/files/articles/2{\_}1{\_}1{\_}JMLT.pdf},
volume = {2},
year = {2011}
}
@misc{Statista2014a,
author = {AGOF},
file = {::},
number = {September},
pages = {2018},
title = {{Nettoreichweite der Top 15 Nachrichtenseiten (ab 14 Jahre) im November 2014 in Unique Usern (in Millionen)}},
url = {http://de.statista.com/statistik/daten/studie/165258/umfrage/reichweite-der-meistbesuchten-nachrichtenwebsites/},
volume = {2018},
year = {2014}
}
@article{Chang2009b,
abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
doi = {10.1.1.100.1089},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9781615679119},
issn = {1098-6596},
journal = {Advances in Neural Information Processing Systems 22},
pages = {288----296},
pmid = {25246403},
title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
url = {http://www.umiacs.umd.edu/{~}jbg/docs/nips2009-rtl.pdf},
year = {2009}
}
@article{Goldstone2014,
abstract = {We use quantitative methods to analyze a collection of 21,367 scholarly articles in literary studies from 1889–2013. Our approach reveals aspects of our disciplinary history that have been occluded by existing histories' emphasis on generational and methodological conflict. We demonstrate gradual, unnoted shifts in the themes and vocabularies of scholarship - including the long rise of new subjects (like violence); we show the surprising novelty of central theoretical concepts; and we explore transformations in the shared rationales for literary scholarship that exceed the boundaries of conventional labels like 'New Criticism' and 'New Historicism.' Though our method uses computational tools, we not claim to provide a definitive or objective perspective on disciplinary history; instead, our approach, like the related methods of content analysis in the social sciences, allows us to pursue nuanced interpretations of the language of many texts at once.},
author = {Goldstone, Andrew and Underwood, Ted},
doi = {10.1353/nlh.2014.0025},
issn = {0028-6087},
journal = {New Literary History: A Journal of Theory and Interpretation},
keywords = {1889-2013,and literary studies,literary historical approach,literary theory and criticism,quantitative approach},
number = {3},
pages = {359--384},
title = {{The Quiet Transformations of Literary Studies: What Thirteen Thousand Scholars Could Tell Us}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=mzh{\&}AN=2014396761{\&}site=ehost-live{\%}5CnComment: http://muse.jhu.edu/journals/new{\_}literary{\_}history/v045/45.3.goldstone.html},
volume = {45},
year = {2014}
}
@article{Titov2008,
abstract = {In this paper we present a novel framework for extracting the ratable aspects of objects from online user reviews. Extracting such aspects is an important challenge in automatically mining product opinions from the web and in generating opinion-based summaries of user reviews [18, 19, 7, 12, 27, 36, 21]. Our models are based on extensions to standard topic modeling methods such as LDA and PLSA to induce multi-grain topics. We argue that multi-grain models are more appropriate for our task since standard models tend to produce topics that correspond to global properties of objects (e.g., the brand of a product type) rather than the aspects of an object that tend to be rated by a user. The models we present not only extract ratable aspects, but also cluster them into coherent topics, e.g., waitress and bartender are part of the same topic staff for restaurants. This differentiates it from much of the previous work which extracts aspects through term frequency analysis with minimal clustering. We evaluate the multi-grain models both qualitatively and quantitatively to show that they improve significantly upon standard topic models.},
annote = {Might not be completly related it focuses on online reviews instead of webblogs or discussion boards},
archivePrefix = {arXiv},
arxivId = {0801.1063},
author = {Titov, Ivan and McDonald, Ryan},
doi = {10.1145/1367497.1367513},
eprint = {0801.1063},
isbn = {9781605580852},
journal = {Proceeding of the 17th international conference on World Wide Web (WWW)},
keywords = {LDA,Multi-grained Topic Models,Ratable aspects,Review data,Topic model,aspects,sliding window},
pages = {111},
title = {{Modeling online reviews with multi-grain topic models}},
url = {http://portal.acm.org/citation.cfm?doid=1367497.1367513},
year = {2008}
}
@article{Balducci2018,
abstract = {The rise of unstructured data (UD), propelled by novel technologies, is reshaping markets and the management of marketing activities. Yet these increased data remain mostly untapped by many firms, suggesting the potential for further research developments. The integrative framework proposed in this study addresses the nature of UD and pursues theoretical richness and computational advancements by integrating insights from other disciplines. This article makes three main contributions to the literature by (1) offering a unifying definition and conceptualization of UD in marketing; (2) bridging disjoint literature with an organizing framework that synthesizes various subsets of UD relevant for marketing management through an integrative review; and (3) identifying substantive, computational, and theoretical gaps in extant literature and ways to leverage interdisciplinary knowledge to advance marketing research by applying UD analyses to underdeveloped areas.

},
author = {Balducci, Bitty and Marinova, Detelina},
doi = {10.1007/s11747-018-0581-x},
issn = {00920703},
journal = {Journal of the Academy of Marketing Science},
keywords = {Acoustic,Artificial intelligence,Big data,Deep learning,Image,Linguistics,Machine learning,Nonverbal,Text,Text mining,Unstructured data,Video,Voice},
pages = {1--34},
publisher = {Journal of the Academy of Marketing Science},
title = {{Unstructured data in marketing}},
year = {2018}
}
@book{böck2017social,
abstract = {Die Autoren legen beispielhafte Analysemethoden von Social-Media-Daten dar: deskriptive und Data-Mining-Methoden. Mit deren Hilfe werden kundenorientierte Gesch{\"{a}}ftsma{\ss}nahmen eingeleitet und ein stetiges Abw{\"{a}}gen zwischen vollautomatisierten und manuellen, kostenintensiven Reports gesteuert. Das Werk liefert eine {\"{U}}bersicht zu aktuell diskutierten Themen wie begleitende Emotionen, Vernetzung der interagierenden User oder Verbindung von Themen. Als Gewinn f{\"{u}}r ein Unternehmen m{\"{u}}ssen die Analysen durch eine strategische Prozedur geleitet werden, um Erkenntnisse in konkrete Handlungsempfehlungen zu {\"{u}}berf{\"{u}}hren. Neben den Potenzialen durch die Anwendung komplexerer Analysemethoden gibt es auch konzeptionelle, technische und ethische Herausforderungen, wie die Autoren veranschaulichen. Der Inhalt Methoden - Metriken, Wordclouds, Trends, Sentiment, Assoziationsregeln, Netzwerkstrukturen Mehrwert durch strategische Prozedur{\~{}} Social-Media-Analysen der Zukunft Die Zielgruppen Social-Media-Verantwortliche und -Analysten, Budgetverantwortliche der Online-Marketing-Kan{\"{a}}le Die Autoren{\~{}} Dr. Matthias B{\"{o}}ck promovierte im Bereich Data Mining an der Technischen Universit{\"{a}}t M{\"{u}}nchen. Dr. Felix K{\"{o}}bler promovierte in der Wirtschaftsinformatik an der Technischen Universit{\"{a}}t M{\"{u}}nchen. Dr. Eva Anderl promovierte an der Universit{\"{a}}t Passau im Themengebiet Online-Kundenverhalten. Linda Le studierte Statistik an der Ludwig-Maximilians-Universit{\"{a}}t M{\"{u}}nchen},
author = {B{\"{o}}ck, Matthias and K{\"{o}}bler, Felix and Anderl, Eva and Le, Linda},
doi = {10.1007/978-3-658-19802-2},
isbn = {978-3-658-19802-2},
pages = {7--8},
publisher = {Springer Fachmedien Wiesbaden},
series = {essentials},
title = {{Social-Media-Analyse - mehr als nur eine Wordcloud: HMD Best Paper Award 2016}},
url = {http://dx.doi.org/10.1007/978-3-658-19802-2},
year = {2017}
}
@article{Newman2011,
abstract = {Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. When learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval. However, when dealing with small collections or noisy text (e.g. web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful. To overcome this, we propose two methods to regularize the learning of topic models. Our regularizers work by creating a structured prior over words that reflect broad patterns in the external data. Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. Overall, this work makes topic models more useful across a broader range of text data.},
author = {Newman, D and Bonilla, Edwin and Buntine, Wray},
file = {::},
isbn = {9781618395993},
journal = {Proceedings NIPS 2011},
keywords = {LDA,regularisation,topic modelling},
pages = {1--9},
title = {{Improving Topic Coherence with Regularized Topic Models}},
url = {http://eprints.pascal-network.org/archive/00009047/},
year = {2011}
}
@book{ignatow2017introduction,
author = {Ignatow, G and Mihalcea, R},
isbn = {9781506337029},
publisher = {SAGE Publications},
title = {{An Introduction to Text Mining: Research Design, Data Collection, and Analysis}},
url = {https://books.google.de/books?id=ubQkDwAAQBAJ},
year = {2017}
}
